{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0298ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:09:32: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from time import time\n",
    "from indic_transliteration import detect\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.detect import Scheme\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e2524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary linking the start date of each week with a list containing the dates contained within\n",
    "#each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3b5de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(\"2020-01-22\", format = \"%Y-%m-%d\")\n",
    "week_starts = [start_date + pd.Timedelta(i, \"W\") for i in range(19)]\n",
    "week_days = {}\n",
    "for week in week_starts:\n",
    "    days = []\n",
    "    if week_starts.index(week) == 18:\n",
    "        for i in range(4):\n",
    "            days.append(str((week + pd.Timedelta(i, \"D\")).date()))\n",
    "        week_days[str(week.date())] = days\n",
    "    else:\n",
    "        for i in range(7):\n",
    "            days.append(str((week + pd.Timedelta(i, \"D\")).date()))\n",
    "        week_days[str(week.date())] = days\n",
    "start_of_weeks = [str(week.date()) for week in week_starts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7243234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2020-01-22 00:00:00'),\n",
       " Timestamp('2020-01-29 00:00:00'),\n",
       " Timestamp('2020-02-05 00:00:00'),\n",
       " Timestamp('2020-02-12 00:00:00'),\n",
       " Timestamp('2020-02-19 00:00:00'),\n",
       " Timestamp('2020-02-26 00:00:00'),\n",
       " Timestamp('2020-03-04 00:00:00'),\n",
       " Timestamp('2020-03-11 00:00:00'),\n",
       " Timestamp('2020-03-18 00:00:00'),\n",
       " Timestamp('2020-03-25 00:00:00'),\n",
       " Timestamp('2020-04-01 00:00:00'),\n",
       " Timestamp('2020-04-08 00:00:00'),\n",
       " Timestamp('2020-04-15 00:00:00'),\n",
       " Timestamp('2020-04-22 00:00:00'),\n",
       " Timestamp('2020-04-29 00:00:00'),\n",
       " Timestamp('2020-05-06 00:00:00'),\n",
       " Timestamp('2020-05-13 00:00:00'),\n",
       " Timestamp('2020-05-20 00:00:00'),\n",
       " Timestamp('2020-05-27 00:00:00')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e180dfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2020-01-22': ['2020-01-22',\n",
       "  '2020-01-23',\n",
       "  '2020-01-24',\n",
       "  '2020-01-25',\n",
       "  '2020-01-26',\n",
       "  '2020-01-27',\n",
       "  '2020-01-28'],\n",
       " '2020-01-29': ['2020-01-29',\n",
       "  '2020-01-30',\n",
       "  '2020-01-31',\n",
       "  '2020-02-01',\n",
       "  '2020-02-02',\n",
       "  '2020-02-03',\n",
       "  '2020-02-04'],\n",
       " '2020-02-05': ['2020-02-05',\n",
       "  '2020-02-06',\n",
       "  '2020-02-07',\n",
       "  '2020-02-08',\n",
       "  '2020-02-09',\n",
       "  '2020-02-10',\n",
       "  '2020-02-11'],\n",
       " '2020-02-12': ['2020-02-12',\n",
       "  '2020-02-13',\n",
       "  '2020-02-14',\n",
       "  '2020-02-15',\n",
       "  '2020-02-16',\n",
       "  '2020-02-17',\n",
       "  '2020-02-18'],\n",
       " '2020-02-19': ['2020-02-19',\n",
       "  '2020-02-20',\n",
       "  '2020-02-21',\n",
       "  '2020-02-22',\n",
       "  '2020-02-23',\n",
       "  '2020-02-24',\n",
       "  '2020-02-25'],\n",
       " '2020-02-26': ['2020-02-26',\n",
       "  '2020-02-27',\n",
       "  '2020-02-28',\n",
       "  '2020-02-29',\n",
       "  '2020-03-01',\n",
       "  '2020-03-02',\n",
       "  '2020-03-03'],\n",
       " '2020-03-04': ['2020-03-04',\n",
       "  '2020-03-05',\n",
       "  '2020-03-06',\n",
       "  '2020-03-07',\n",
       "  '2020-03-08',\n",
       "  '2020-03-09',\n",
       "  '2020-03-10'],\n",
       " '2020-03-11': ['2020-03-11',\n",
       "  '2020-03-12',\n",
       "  '2020-03-13',\n",
       "  '2020-03-14',\n",
       "  '2020-03-15',\n",
       "  '2020-03-16',\n",
       "  '2020-03-17'],\n",
       " '2020-03-18': ['2020-03-18',\n",
       "  '2020-03-19',\n",
       "  '2020-03-20',\n",
       "  '2020-03-21',\n",
       "  '2020-03-22',\n",
       "  '2020-03-23',\n",
       "  '2020-03-24'],\n",
       " '2020-03-25': ['2020-03-25',\n",
       "  '2020-03-26',\n",
       "  '2020-03-27',\n",
       "  '2020-03-28',\n",
       "  '2020-03-29',\n",
       "  '2020-03-30',\n",
       "  '2020-03-31'],\n",
       " '2020-04-01': ['2020-04-01',\n",
       "  '2020-04-02',\n",
       "  '2020-04-03',\n",
       "  '2020-04-04',\n",
       "  '2020-04-05',\n",
       "  '2020-04-06',\n",
       "  '2020-04-07'],\n",
       " '2020-04-08': ['2020-04-08',\n",
       "  '2020-04-09',\n",
       "  '2020-04-10',\n",
       "  '2020-04-11',\n",
       "  '2020-04-12',\n",
       "  '2020-04-13',\n",
       "  '2020-04-14'],\n",
       " '2020-04-15': ['2020-04-15',\n",
       "  '2020-04-16',\n",
       "  '2020-04-17',\n",
       "  '2020-04-18',\n",
       "  '2020-04-19',\n",
       "  '2020-04-20',\n",
       "  '2020-04-21'],\n",
       " '2020-04-22': ['2020-04-22',\n",
       "  '2020-04-23',\n",
       "  '2020-04-24',\n",
       "  '2020-04-25',\n",
       "  '2020-04-26',\n",
       "  '2020-04-27',\n",
       "  '2020-04-28'],\n",
       " '2020-04-29': ['2020-04-29',\n",
       "  '2020-04-30',\n",
       "  '2020-05-01',\n",
       "  '2020-05-02',\n",
       "  '2020-05-03',\n",
       "  '2020-05-04',\n",
       "  '2020-05-05'],\n",
       " '2020-05-06': ['2020-05-06',\n",
       "  '2020-05-07',\n",
       "  '2020-05-08',\n",
       "  '2020-05-09',\n",
       "  '2020-05-10',\n",
       "  '2020-05-11',\n",
       "  '2020-05-12'],\n",
       " '2020-05-13': ['2020-05-13',\n",
       "  '2020-05-14',\n",
       "  '2020-05-15',\n",
       "  '2020-05-16',\n",
       "  '2020-05-17',\n",
       "  '2020-05-18',\n",
       "  '2020-05-19'],\n",
       " '2020-05-20': ['2020-05-20',\n",
       "  '2020-05-21',\n",
       "  '2020-05-22',\n",
       "  '2020-05-23',\n",
       "  '2020-05-24',\n",
       "  '2020-05-25',\n",
       "  '2020-05-26'],\n",
       " '2020-05-27': ['2020-05-27', '2020-05-28', '2020-05-29', '2020-05-30']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25db44bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-01-22',\n",
       " '2020-01-29',\n",
       " '2020-02-05',\n",
       " '2020-02-12',\n",
       " '2020-02-19',\n",
       " '2020-02-26',\n",
       " '2020-03-04',\n",
       " '2020-03-11',\n",
       " '2020-03-18',\n",
       " '2020-03-25',\n",
       " '2020-04-01',\n",
       " '2020-04-08',\n",
       " '2020-04-15',\n",
       " '2020-04-22',\n",
       " '2020-04-29',\n",
       " '2020-05-06',\n",
       " '2020-05-13',\n",
       " '2020-05-20',\n",
       " '2020-05-27']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_of_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc97551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function that detects the language scheme of every word in a tweet. \n",
    "#if the scheme is not one of (\"itrans\", \"hk\", \"slp1\", \"iast\", \"velthuis\", \"kolkata_v2\" - these are all \n",
    "#Devanagari script to Roman script transliteration schemes), then the word is transliterated into Roman script\n",
    "#using the Harvard-Kyoto(\"hk\") scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d7f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_hindi(word):\n",
    "    lang = detect.detect(str(word))\n",
    "    if lang not in [Scheme.itrans, Scheme.hk, Scheme.slp1, Scheme.iast, Scheme.velthuis, Scheme.kolkata_v2]:\n",
    "        word = transliterate(word, lang, sanscript.HK).lower()\n",
    "    else:\n",
    "        word = word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c13565f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"क्या kar rahe ho\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c738b4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kya', 'kar', 'rahe', 'ho']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [transliterate_hindi(word) for word in text.lower().split()]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6fb89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#going through every week_start key in the week_days dictionary created above\n",
    "#going through every day in the list associated with the week_start key\n",
    "#looping through every state in for each day of the previous step and loading the file associated with that date\n",
    "#transliterating every tweet and then saving all the tweets in a txt file which is common to that week\n",
    "#saving the txt files in the \"weekly training text for word2vec\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3fc5c94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-22 completed\n",
      "time taken: 0.0 mins\n",
      "2020-01-23 completed\n",
      "time taken: 0.0 mins\n",
      "2020-01-24 completed\n",
      "time taken: 0.0 mins\n",
      "2020-01-25 completed\n",
      "time taken: 0.0 mins\n",
      "2020-01-26 completed\n",
      "time taken: 0.0 mins\n",
      "2020-01-27 completed\n",
      "time taken: 0.0 mins\n",
      "2020-01-28 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-01-22 completed\n",
      "time taken: 0.02 mins\n",
      "2020-01-29 completed\n",
      "time taken: 0.0 mins\n",
      "2020-01-30 completed\n",
      "time taken: 0.0 mins\n",
      "2020-01-31 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-01 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-02 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-03 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-04 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-01-29 completed\n",
      "time taken: 0.02 mins\n",
      "2020-02-05 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-06 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-07 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-08 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-09 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-10 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-11 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-02-05 completed\n",
      "time taken: 0.02 mins\n",
      "2020-02-12 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-13 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-14 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-15 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-16 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-17 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-18 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-02-12 completed\n",
      "time taken: 0.02 mins\n",
      "2020-02-19 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-20 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-21 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-22 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-23 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-24 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-25 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-02-19 completed\n",
      "time taken: 0.02 mins\n",
      "2020-02-26 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-27 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-28 completed\n",
      "time taken: 0.0 mins\n",
      "2020-02-29 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-01 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-02 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-03 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-02-26 completed\n",
      "time taken: 0.02 mins\n",
      "2020-03-04 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-05 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-06 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-07 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-08 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-09 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-10 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-03-04 completed\n",
      "time taken: 0.02 mins\n",
      "2020-03-11 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-12 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-13 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-14 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-15 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-16 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-17 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-03-11 completed\n",
      "time taken: 0.01 mins\n",
      "2020-03-18 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-19 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-20 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-21 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-22 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-23 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-24 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-03-18 completed\n",
      "time taken: 0.02 mins\n",
      "2020-03-25 completed\n",
      "time taken: 0.01 mins\n",
      "2020-03-26 completed\n",
      "time taken: 0.01 mins\n",
      "2020-03-27 completed\n",
      "time taken: 0.01 mins\n",
      "2020-03-28 completed\n",
      "time taken: 0.01 mins\n",
      "2020-03-29 completed\n",
      "time taken: 0.01 mins\n",
      "2020-03-30 completed\n",
      "time taken: 0.0 mins\n",
      "2020-03-31 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-03-25 completed\n",
      "time taken: 0.04 mins\n",
      "2020-04-01 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-02 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-03 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-04 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-05 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-06 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-07 completed\n",
      "time taken: 0.01 mins\n",
      "week starting 2020-04-01 completed\n",
      "time taken: 0.03 mins\n",
      "2020-04-08 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-09 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-10 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-11 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-12 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-13 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-14 completed\n",
      "time taken: 0.01 mins\n",
      "week starting 2020-04-08 completed\n",
      "time taken: 0.03 mins\n",
      "2020-04-15 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-16 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-17 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-18 completed\n",
      "time taken: 0.0 mins\n",
      "2020-04-19 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-20 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-21 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-04-15 completed\n",
      "time taken: 0.03 mins\n",
      "2020-04-22 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-23 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-24 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-25 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-26 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-27 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-28 completed\n",
      "time taken: 0.01 mins\n",
      "week starting 2020-04-22 completed\n",
      "time taken: 0.04 mins\n",
      "2020-04-29 completed\n",
      "time taken: 0.01 mins\n",
      "2020-04-30 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-01 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-02 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-03 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-04 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-05 completed\n",
      "time taken: 0.01 mins\n",
      "week starting 2020-04-29 completed\n",
      "time taken: 0.04 mins\n",
      "2020-05-06 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-07 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-08 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-09 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-10 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-11 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-12 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-05-06 completed\n",
      "time taken: 0.04 mins\n",
      "2020-05-13 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-14 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-15 completed\n",
      "time taken: 0.02 mins\n",
      "2020-05-16 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-17 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-18 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-19 completed\n",
      "time taken: 0.01 mins\n",
      "week starting 2020-05-13 completed\n",
      "time taken: 0.06 mins\n",
      "2020-05-20 completed\n",
      "time taken: 0.01 mins\n",
      "2020-05-21 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-22 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-23 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-24 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-25 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-26 completed\n",
      "time taken: 0.0 mins\n",
      "week starting 2020-05-20 completed\n",
      "time taken: 0.03 mins\n",
      "2020-05-27 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-28 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-29 completed\n",
      "time taken: 0.0 mins\n",
      "2020-05-30 completed\n",
      "time taken: 0.01 mins\n",
      "week starting 2020-05-27 completed\n",
      "time taken: 0.02 mins\n",
      "time taken: 0.54 mins\n"
     ]
    }
   ],
   "source": [
    "state_list = os.listdir(\"Cleaned Data\")\n",
    "t = time()\n",
    "for start_ in start_of_weeks:\n",
    "    t_1 = time()\n",
    "    for day in week_days[start_]:\n",
    "        t_2 = time()\n",
    "        try:\n",
    "            for state in state_list:\n",
    "                _path = os.path.join(\"Cleaned Data\", state)\n",
    "                df = pickle.load(open(os.path.join(_path, \"cleaned {}.csv.pkl\".format(day)), \"rb\")) \n",
    "                for i in range(df.shape[0]):\n",
    "                    tweet = df.at[i, \"cleaned_tweet\"].lower().split()\n",
    "                    tweet = [transliterate_hindi(word) for word in tweet]\n",
    "                    with open(os.path.join(\"weekly training text for word2vec\\\\{} week tweets.txt\".format(start_)), \"a\", encoding = \"utf-8\") as f:\n",
    "                        f.write(\" \".join(tweet))\n",
    "                        f.write(\"\\n\")\n",
    "                        f.close()\n",
    "            print(\"{} completed\".format(day))\n",
    "            print(\"time taken: {} mins\".format(round((time() - t_2)/60, 2)))\n",
    "        except:\n",
    "            pass\n",
    "    print(\"week starting {} completed\".format(start_))\n",
    "    print(\"time taken: {} mins\".format(round((time() - t_1)/60, 2)))\n",
    "print(\"time taken: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c49293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you are going through the entire thesis workflow, you must also save these txt files in a folder named the\n",
    "#same within the \"Contextual Analysis\" main folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08bb3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function that will iterate over the weekly text file, reading it line by line and feeding it into \n",
    "#word2vec model. allows us to not load the whole txt file in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71a9cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self.filename = file\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.filename, encoding = \"utf-8\") as f:\n",
    "            for row in f:\n",
    "                _row = row.lower().split()\n",
    "                yield _row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce35bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating functions that will create bigrams and trigrams for that week's tweet corpus, once again by\n",
    "#reading the file line by line. the bigrams are created on the original tweets, and the trigrams are created on\n",
    "#the bigrammed tweets. we consider only those bigrams and trigrams that occur at least 5 times a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6955760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9db34cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBigramCorpus:\n",
    "    \n",
    "    def __init__(self, file, bigram_mod):\n",
    "        self.filename = file\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.filename, encoding = \"utf-8\") as f:\n",
    "            for row in f:\n",
    "                _row = row.lower().split()\n",
    "                yield bigram_mod[_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65c6d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrigramCorpus:\n",
    "    \n",
    "    def __init__(self, file, bigram_mod, trigram_mod):\n",
    "        self.filename = file\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.filename, encoding = \"utf-8\") as f:\n",
    "            for row in f:\n",
    "                _row = row.lower().split()\n",
    "                yield trigram_mod[bigram_mod[_row]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01a4420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating over each of these weekly txt files and training a separate word2vec model for each week\n",
    "#saving these word2vec models in a folder called \"wv models\". the word2vec models are trained with \n",
    "#window size 5, vector size 100, CBOW architecture, and heirarchical softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa43e480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:07:54: collecting all words and their counts\n",
      "INFO - 23:07:54: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:07:54: PROGRESS: at sentence #10000, processed 132288 words and 61618 word types\n",
      "INFO - 23:07:54: collected 61618 token types (unigram + bigrams) from a corpus of 157742 words and 11832 sentences\n",
      "INFO - 23:07:54: merged Phrases<61618 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:07:54: Phrases lifecycle event {'msg': 'built Phrases<61618 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.26s', 'datetime': '2023-04-01T23:07:54.531252', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:07:54: exporting phrases from Phrases<61618 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:07:54: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<889 phrases, min_count=5, threshold=7> from Phrases<61618 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.15s', 'datetime': '2023-04-01T23:07:54.682885', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:07:54: collecting all words and their counts\n",
      "INFO - 23:07:54: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:07:55: PROGRESS: at sentence #10000, processed 123022 words and 63270 word types\n",
      "INFO - 23:07:55: collected 63270 token types (unigram + bigrams) from a corpus of 146668 words and 11832 sentences\n",
      "INFO - 23:07:55: merged Phrases<63270 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:07:55: Phrases lifecycle event {'msg': 'built Phrases<63270 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.44s', 'datetime': '2023-04-01T23:07:55.127704', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:07:55: exporting phrases from Phrases<63270 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:07:55: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<896 phrases, min_count=5, threshold=7> from Phrases<63270 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.20s', 'datetime': '2023-04-01T23:07:55.326860', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:07:55: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:07:55.330851', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:07:55: collecting all words and their counts\n",
      "INFO - 23:07:55: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:07:55: PROGRESS: at sentence #10000, processed 120241 words, keeping 12768 word types\n",
      "INFO - 23:07:55: collected 12768 word types from a corpus of 143334 raw words and 11832 sentences\n",
      "INFO - 23:07:55: Creating a fresh vocabulary\n",
      "INFO - 23:07:55: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1018 unique words (7.97% of original 12768, drops 11750)', 'datetime': '2023-04-01T23:07:55.786670', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:07:55: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 93832 word corpus (65.46% of original 143334, drops 49502)', 'datetime': '2023-04-01T23:07:55.786670', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:07:55: deleting the raw counts dictionary of 12768 items\n",
      "INFO - 23:07:55: sample=1e-05 downsamples 1018 most-common words\n",
      "INFO - 23:07:55: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 8532.706679350422 word corpus (9.1%% of prior 93832)', 'datetime': '2023-04-01T23:07:55.800594', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:07:55: constructing a huffman tree from 1018 words\n",
      "INFO - 23:07:55: built huffman tree with maximum node depth 12\n",
      "INFO - 23:07:55: estimated required memory for 1018 words and 100 dimensions: 1934200 bytes\n",
      "INFO - 23:07:55: resetting layer weights\n",
      "INFO - 23:07:55: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:07:55.860789', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:07:55: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1018 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:07:55.861787', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:07:56: EPOCH 0: training on 143334 raw words (8561 effective words) took 0.5s, 17780 effective words/s\n",
      "INFO - 23:07:56: EPOCH 1: training on 143334 raw words (8583 effective words) took 0.5s, 18901 effective words/s\n",
      "INFO - 23:07:57: EPOCH 2: training on 143334 raw words (8615 effective words) took 0.5s, 16264 effective words/s\n",
      "INFO - 23:07:57: EPOCH 3: training on 143334 raw words (8519 effective words) took 0.4s, 19449 effective words/s\n",
      "INFO - 23:07:58: EPOCH 4: training on 143334 raw words (8298 effective words) took 0.4s, 18929 effective words/s\n",
      "INFO - 23:07:58: EPOCH 5: training on 143334 raw words (8666 effective words) took 0.5s, 18412 effective words/s\n",
      "INFO - 23:07:59: EPOCH 6: training on 143334 raw words (8596 effective words) took 0.4s, 19476 effective words/s\n",
      "INFO - 23:07:59: EPOCH 7: training on 143334 raw words (8393 effective words) took 0.4s, 19230 effective words/s\n",
      "INFO - 23:08:00: EPOCH 8: training on 143334 raw words (8647 effective words) took 0.4s, 19900 effective words/s\n",
      "INFO - 23:08:00: EPOCH 9: training on 143334 raw words (8631 effective words) took 0.4s, 19188 effective words/s\n",
      "INFO - 23:08:00: EPOCH 10: training on 143334 raw words (8318 effective words) took 0.4s, 18694 effective words/s\n",
      "INFO - 23:08:01: EPOCH 11: training on 143334 raw words (8486 effective words) took 0.4s, 19921 effective words/s\n",
      "INFO - 23:08:01: EPOCH 12: training on 143334 raw words (8580 effective words) took 0.4s, 19880 effective words/s\n",
      "INFO - 23:08:02: EPOCH 13: training on 143334 raw words (8501 effective words) took 0.4s, 19554 effective words/s\n",
      "INFO - 23:08:02: EPOCH 14: training on 143334 raw words (8492 effective words) took 0.5s, 16094 effective words/s\n",
      "INFO - 23:08:03: EPOCH 15: training on 143334 raw words (8531 effective words) took 0.5s, 18551 effective words/s\n",
      "INFO - 23:08:03: EPOCH 16: training on 143334 raw words (8546 effective words) took 0.5s, 18563 effective words/s\n",
      "INFO - 23:08:04: EPOCH 17: training on 143334 raw words (8436 effective words) took 0.4s, 18995 effective words/s\n",
      "INFO - 23:08:04: EPOCH 18: training on 143334 raw words (8497 effective words) took 0.5s, 18723 effective words/s\n",
      "INFO - 23:08:05: EPOCH 19: training on 143334 raw words (8543 effective words) took 0.4s, 19626 effective words/s\n",
      "INFO - 23:08:05: EPOCH 20: training on 143334 raw words (8590 effective words) took 0.4s, 20112 effective words/s\n",
      "INFO - 23:08:05: EPOCH 21: training on 143334 raw words (8634 effective words) took 0.5s, 17462 effective words/s\n",
      "INFO - 23:08:06: EPOCH 22: training on 143334 raw words (8470 effective words) took 0.4s, 19691 effective words/s\n",
      "INFO - 23:08:06: EPOCH 23: training on 143334 raw words (8545 effective words) took 0.4s, 19812 effective words/s\n",
      "INFO - 23:08:07: EPOCH 24: training on 143334 raw words (8485 effective words) took 0.4s, 19543 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:08:07: EPOCH 25: training on 143334 raw words (8583 effective words) took 0.5s, 18949 effective words/s\n",
      "INFO - 23:08:08: EPOCH 26: training on 143334 raw words (8556 effective words) took 0.5s, 17007 effective words/s\n",
      "INFO - 23:08:08: EPOCH 27: training on 143334 raw words (8496 effective words) took 0.4s, 19041 effective words/s\n",
      "INFO - 23:08:09: EPOCH 28: training on 143334 raw words (8475 effective words) took 0.4s, 19498 effective words/s\n",
      "INFO - 23:08:09: EPOCH 29: training on 143334 raw words (8443 effective words) took 0.4s, 19621 effective words/s\n",
      "INFO - 23:08:10: EPOCH 30: training on 143334 raw words (8598 effective words) took 0.4s, 19744 effective words/s\n",
      "INFO - 23:08:10: EPOCH 31: training on 143334 raw words (8511 effective words) took 0.4s, 19224 effective words/s\n",
      "INFO - 23:08:10: EPOCH 32: training on 143334 raw words (8478 effective words) took 0.4s, 19955 effective words/s\n",
      "INFO - 23:08:11: EPOCH 33: training on 143334 raw words (8539 effective words) took 0.4s, 19674 effective words/s\n",
      "INFO - 23:08:11: EPOCH 34: training on 143334 raw words (8555 effective words) took 0.4s, 19734 effective words/s\n",
      "INFO - 23:08:12: EPOCH 35: training on 143334 raw words (8520 effective words) took 0.4s, 20026 effective words/s\n",
      "INFO - 23:08:12: EPOCH 36: training on 143334 raw words (8459 effective words) took 0.4s, 19676 effective words/s\n",
      "INFO - 23:08:13: EPOCH 37: training on 143334 raw words (8394 effective words) took 0.5s, 17004 effective words/s\n",
      "INFO - 23:08:13: EPOCH 38: training on 143334 raw words (8375 effective words) took 0.4s, 18898 effective words/s\n",
      "INFO - 23:08:14: EPOCH 39: training on 143334 raw words (8515 effective words) took 0.4s, 18930 effective words/s\n",
      "INFO - 23:08:14: EPOCH 40: training on 143334 raw words (8462 effective words) took 0.4s, 19707 effective words/s\n",
      "INFO - 23:08:14: EPOCH 41: training on 143334 raw words (8473 effective words) took 0.4s, 19418 effective words/s\n",
      "INFO - 23:08:15: EPOCH 42: training on 143334 raw words (8436 effective words) took 0.4s, 19459 effective words/s\n",
      "INFO - 23:08:15: EPOCH 43: training on 143334 raw words (8505 effective words) took 0.4s, 19381 effective words/s\n",
      "INFO - 23:08:16: EPOCH 44: training on 143334 raw words (8635 effective words) took 0.4s, 20771 effective words/s\n",
      "INFO - 23:08:16: EPOCH 45: training on 143334 raw words (8600 effective words) took 0.4s, 20108 effective words/s\n",
      "INFO - 23:08:17: EPOCH 46: training on 143334 raw words (8516 effective words) took 0.4s, 19666 effective words/s\n",
      "INFO - 23:08:17: EPOCH 47: training on 143334 raw words (8502 effective words) took 0.4s, 20000 effective words/s\n",
      "INFO - 23:08:17: EPOCH 48: training on 143334 raw words (8534 effective words) took 0.4s, 19720 effective words/s\n",
      "INFO - 23:08:18: EPOCH 49: training on 143334 raw words (8473 effective words) took 0.4s, 19359 effective words/s\n",
      "INFO - 23:08:18: Word2Vec lifecycle event {'msg': 'training on 7166700 raw words (425796 effective words) took 22.5s, 18897 effective words/s', 'datetime': '2023-04-01T23:08:18.395675', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:08:18: collecting all words and their counts\n",
      "INFO - 23:08:18: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:08:18: PROGRESS: at sentence #10000, processed 132209 words and 56206 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-01-22 completed\n",
      "time taken: 0.4 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:08:18: collected 56206 token types (unigram + bigrams) from a corpus of 140230 words and 10584 sentences\n",
      "INFO - 23:08:18: merged Phrases<56206 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:08:18: Phrases lifecycle event {'msg': 'built Phrases<56206 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.25s', 'datetime': '2023-04-01T23:08:18.661486', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:18: exporting phrases from Phrases<56206 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:08:18: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<695 phrases, min_count=5, threshold=7> from Phrases<56206 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.13s', 'datetime': '2023-04-01T23:08:18.790183', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:18: collecting all words and their counts\n",
      "INFO - 23:08:18: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:08:19: PROGRESS: at sentence #10000, processed 124217 words and 57483 word types\n",
      "INFO - 23:08:19: collected 57483 token types (unigram + bigrams) from a corpus of 131716 words and 10584 sentences\n",
      "INFO - 23:08:19: merged Phrases<57483 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:08:19: Phrases lifecycle event {'msg': 'built Phrases<57483 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.38s', 'datetime': '2023-04-01T23:08:19.167134', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:19: exporting phrases from Phrases<57483 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:08:19: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<678 phrases, min_count=5, threshold=7> from Phrases<57483 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.13s', 'datetime': '2023-04-01T23:08:19.302771', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:19: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:08:19.303816', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:19: collecting all words and their counts\n",
      "INFO - 23:08:19: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:08:19: PROGRESS: at sentence #10000, processed 121977 words, keeping 11516 word types\n",
      "INFO - 23:08:19: collected 11516 word types from a corpus of 129324 raw words and 10584 sentences\n",
      "INFO - 23:08:19: Creating a fresh vocabulary\n",
      "INFO - 23:08:19: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 939 unique words (8.15% of original 11516, drops 10577)', 'datetime': '2023-04-01T23:08:19.697717', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:08:19: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 84418 word corpus (65.28% of original 129324, drops 44906)', 'datetime': '2023-04-01T23:08:19.698712', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:08:19: deleting the raw counts dictionary of 11516 items\n",
      "INFO - 23:08:19: sample=1e-05 downsamples 939 most-common words\n",
      "INFO - 23:08:19: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7373.672995928491 word corpus (8.7%% of prior 84418)', 'datetime': '2023-04-01T23:08:19.710682', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:08:19: constructing a huffman tree from 939 words\n",
      "INFO - 23:08:19: built huffman tree with maximum node depth 12\n",
      "INFO - 23:08:19: estimated required memory for 939 words and 100 dimensions: 1784100 bytes\n",
      "INFO - 23:08:19: resetting layer weights\n",
      "INFO - 23:08:19: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:08:19.762579', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:08:19: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 939 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:08:19.763578', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:08:20: EPOCH 0: training on 129324 raw words (7254 effective words) took 0.4s, 18244 effective words/s\n",
      "INFO - 23:08:20: EPOCH 1: training on 129324 raw words (7220 effective words) took 0.4s, 18937 effective words/s\n",
      "INFO - 23:08:20: EPOCH 2: training on 129324 raw words (7382 effective words) took 0.4s, 16998 effective words/s\n",
      "INFO - 23:08:21: EPOCH 3: training on 129324 raw words (7408 effective words) took 0.4s, 16773 effective words/s\n",
      "INFO - 23:08:21: EPOCH 4: training on 129324 raw words (7398 effective words) took 0.4s, 19174 effective words/s\n",
      "INFO - 23:08:22: EPOCH 5: training on 129324 raw words (7273 effective words) took 0.4s, 19118 effective words/s\n",
      "INFO - 23:08:22: EPOCH 6: training on 129324 raw words (7275 effective words) took 0.4s, 18356 effective words/s\n",
      "INFO - 23:08:23: EPOCH 7: training on 129324 raw words (7268 effective words) took 0.4s, 18417 effective words/s\n",
      "INFO - 23:08:23: EPOCH 8: training on 129324 raw words (7472 effective words) took 0.4s, 18898 effective words/s\n",
      "INFO - 23:08:23: EPOCH 9: training on 129324 raw words (7306 effective words) took 0.4s, 18700 effective words/s\n",
      "INFO - 23:08:24: EPOCH 10: training on 129324 raw words (7388 effective words) took 0.4s, 19226 effective words/s\n",
      "INFO - 23:08:24: EPOCH 11: training on 129324 raw words (7399 effective words) took 0.4s, 19143 effective words/s\n",
      "INFO - 23:08:24: EPOCH 12: training on 129324 raw words (7407 effective words) took 0.4s, 19251 effective words/s\n",
      "INFO - 23:08:25: EPOCH 13: training on 129324 raw words (7459 effective words) took 0.4s, 19650 effective words/s\n",
      "INFO - 23:08:25: EPOCH 14: training on 129324 raw words (7402 effective words) took 0.4s, 19306 effective words/s\n",
      "INFO - 23:08:26: EPOCH 15: training on 129324 raw words (7414 effective words) took 0.4s, 19593 effective words/s\n",
      "INFO - 23:08:26: EPOCH 16: training on 129324 raw words (7391 effective words) took 0.4s, 19129 effective words/s\n",
      "INFO - 23:08:26: EPOCH 17: training on 129324 raw words (7407 effective words) took 0.4s, 19537 effective words/s\n",
      "INFO - 23:08:27: EPOCH 18: training on 129324 raw words (7375 effective words) took 0.4s, 19129 effective words/s\n",
      "INFO - 23:08:27: EPOCH 19: training on 129324 raw words (7412 effective words) took 0.4s, 19331 effective words/s\n",
      "INFO - 23:08:28: EPOCH 20: training on 129324 raw words (7485 effective words) took 0.4s, 19185 effective words/s\n",
      "INFO - 23:08:28: EPOCH 21: training on 129324 raw words (7326 effective words) took 0.4s, 18203 effective words/s\n",
      "INFO - 23:08:28: EPOCH 22: training on 129324 raw words (7290 effective words) took 0.4s, 18613 effective words/s\n",
      "INFO - 23:08:29: EPOCH 23: training on 129324 raw words (7310 effective words) took 0.4s, 18063 effective words/s\n",
      "INFO - 23:08:29: EPOCH 24: training on 129324 raw words (7411 effective words) took 0.4s, 16667 effective words/s\n",
      "INFO - 23:08:30: EPOCH 25: training on 129324 raw words (7316 effective words) took 0.4s, 19187 effective words/s\n",
      "INFO - 23:08:30: EPOCH 26: training on 129324 raw words (7277 effective words) took 0.4s, 18960 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:08:30: EPOCH 27: training on 129324 raw words (7321 effective words) took 0.4s, 19282 effective words/s\n",
      "INFO - 23:08:31: EPOCH 28: training on 129324 raw words (7292 effective words) took 0.4s, 19361 effective words/s\n",
      "INFO - 23:08:31: EPOCH 29: training on 129324 raw words (7479 effective words) took 0.4s, 19555 effective words/s\n",
      "INFO - 23:08:32: EPOCH 30: training on 129324 raw words (7416 effective words) took 0.4s, 19268 effective words/s\n",
      "INFO - 23:08:32: EPOCH 31: training on 129324 raw words (7419 effective words) took 0.4s, 19366 effective words/s\n",
      "INFO - 23:08:32: EPOCH 32: training on 129324 raw words (7418 effective words) took 0.4s, 18857 effective words/s\n",
      "INFO - 23:08:33: EPOCH 33: training on 129324 raw words (7323 effective words) took 0.4s, 18796 effective words/s\n",
      "INFO - 23:08:33: EPOCH 34: training on 129324 raw words (7402 effective words) took 0.4s, 18515 effective words/s\n",
      "INFO - 23:08:34: EPOCH 35: training on 129324 raw words (7345 effective words) took 0.4s, 18446 effective words/s\n",
      "INFO - 23:08:34: EPOCH 36: training on 129324 raw words (7346 effective words) took 0.4s, 18656 effective words/s\n",
      "INFO - 23:08:34: EPOCH 37: training on 129324 raw words (7464 effective words) took 0.4s, 19659 effective words/s\n",
      "INFO - 23:08:35: EPOCH 38: training on 129324 raw words (7523 effective words) took 0.4s, 19807 effective words/s\n",
      "INFO - 23:08:35: EPOCH 39: training on 129324 raw words (7345 effective words) took 0.4s, 18795 effective words/s\n",
      "INFO - 23:08:36: EPOCH 40: training on 129324 raw words (7351 effective words) took 0.4s, 16548 effective words/s\n",
      "INFO - 23:08:36: EPOCH 41: training on 129324 raw words (7298 effective words) took 0.4s, 16301 effective words/s\n",
      "INFO - 23:08:36: EPOCH 42: training on 129324 raw words (7325 effective words) took 0.4s, 19102 effective words/s\n",
      "INFO - 23:08:37: EPOCH 43: training on 129324 raw words (7451 effective words) took 0.4s, 19493 effective words/s\n",
      "INFO - 23:08:37: EPOCH 44: training on 129324 raw words (7347 effective words) took 0.4s, 18100 effective words/s\n",
      "INFO - 23:08:38: EPOCH 45: training on 129324 raw words (7369 effective words) took 0.4s, 18647 effective words/s\n",
      "INFO - 23:08:38: EPOCH 46: training on 129324 raw words (7475 effective words) took 0.4s, 19022 effective words/s\n",
      "INFO - 23:08:38: EPOCH 47: training on 129324 raw words (7207 effective words) took 0.4s, 17904 effective words/s\n",
      "INFO - 23:08:39: EPOCH 48: training on 129324 raw words (7357 effective words) took 0.4s, 18537 effective words/s\n",
      "INFO - 23:08:39: EPOCH 49: training on 129324 raw words (7456 effective words) took 0.4s, 19031 effective words/s\n",
      "INFO - 23:08:39: Word2Vec lifecycle event {'msg': 'training on 6466200 raw words (368454 effective words) took 19.9s, 18480 effective words/s', 'datetime': '2023-04-01T23:08:39.702740', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:08:39: collecting all words and their counts\n",
      "INFO - 23:08:39: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:08:39: PROGRESS: at sentence #10000, processed 134101 words and 57346 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-01-29 completed\n",
      "time taken: 0.36 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:08:39: collected 57346 token types (unigram + bigrams) from a corpus of 141718 words and 10548 sentences\n",
      "INFO - 23:08:39: merged Phrases<57346 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:08:39: Phrases lifecycle event {'msg': 'built Phrases<57346 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.24s', 'datetime': '2023-04-01T23:08:39.952922', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:39: exporting phrases from Phrases<57346 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:08:40: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<725 phrases, min_count=5, threshold=7> from Phrases<57346 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.12s', 'datetime': '2023-04-01T23:08:40.079202', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:40: collecting all words and their counts\n",
      "INFO - 23:08:40: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:08:40: PROGRESS: at sentence #10000, processed 126124 words and 58597 word types\n",
      "INFO - 23:08:40: collected 58597 token types (unigram + bigrams) from a corpus of 133322 words and 10548 sentences\n",
      "INFO - 23:08:40: merged Phrases<58597 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:08:40: Phrases lifecycle event {'msg': 'built Phrases<58597 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.37s', 'datetime': '2023-04-01T23:08:40.454200', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:40: exporting phrases from Phrases<58597 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:08:40: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<713 phrases, min_count=5, threshold=7> from Phrases<58597 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.13s', 'datetime': '2023-04-01T23:08:40.587843', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:40: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:08:40.588840', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:08:40: collecting all words and their counts\n",
      "INFO - 23:08:40: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:08:40: PROGRESS: at sentence #10000, processed 123869 words, keeping 11831 word types\n",
      "INFO - 23:08:40: collected 11831 word types from a corpus of 130966 raw words and 10548 sentences\n",
      "INFO - 23:08:40: Creating a fresh vocabulary\n",
      "INFO - 23:08:40: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 925 unique words (7.82% of original 11831, drops 10906)', 'datetime': '2023-04-01T23:08:40.973921', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:08:40: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 85376 word corpus (65.19% of original 130966, drops 45590)', 'datetime': '2023-04-01T23:08:40.974920', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:08:40: deleting the raw counts dictionary of 11831 items\n",
      "INFO - 23:08:40: sample=1e-05 downsamples 925 most-common words\n",
      "INFO - 23:08:40: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7398.947954660495 word corpus (8.7%% of prior 85376)', 'datetime': '2023-04-01T23:08:40.992878', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:08:40: constructing a huffman tree from 925 words\n",
      "INFO - 23:08:41: built huffman tree with maximum node depth 12\n",
      "INFO - 23:08:41: estimated required memory for 925 words and 100 dimensions: 1757500 bytes\n",
      "INFO - 23:08:41: resetting layer weights\n",
      "INFO - 23:08:41: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:08:41.047685', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:08:41: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 925 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:08:41.048683', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:08:41: EPOCH 0: training on 130966 raw words (7322 effective words) took 0.4s, 18973 effective words/s\n",
      "INFO - 23:08:41: EPOCH 1: training on 130966 raw words (7456 effective words) took 0.4s, 19208 effective words/s\n",
      "INFO - 23:08:42: EPOCH 2: training on 130966 raw words (7366 effective words) took 0.4s, 19098 effective words/s\n",
      "INFO - 23:08:42: EPOCH 3: training on 130966 raw words (7276 effective words) took 0.4s, 18621 effective words/s\n",
      "INFO - 23:08:43: EPOCH 4: training on 130966 raw words (7474 effective words) took 0.4s, 19151 effective words/s\n",
      "INFO - 23:08:43: EPOCH 5: training on 130966 raw words (7457 effective words) took 0.4s, 18876 effective words/s\n",
      "INFO - 23:08:43: EPOCH 6: training on 130966 raw words (7474 effective words) took 0.5s, 15995 effective words/s\n",
      "INFO - 23:08:44: EPOCH 7: training on 130966 raw words (7397 effective words) took 0.4s, 18515 effective words/s\n",
      "INFO - 23:08:44: EPOCH 8: training on 130966 raw words (7384 effective words) took 0.4s, 18993 effective words/s\n",
      "INFO - 23:08:45: EPOCH 9: training on 130966 raw words (7366 effective words) took 0.4s, 19136 effective words/s\n",
      "INFO - 23:08:45: EPOCH 10: training on 130966 raw words (7389 effective words) took 0.4s, 18824 effective words/s\n",
      "INFO - 23:08:45: EPOCH 11: training on 130966 raw words (7504 effective words) took 0.4s, 19281 effective words/s\n",
      "INFO - 23:08:46: EPOCH 12: training on 130966 raw words (7349 effective words) took 0.4s, 19206 effective words/s\n",
      "INFO - 23:08:46: EPOCH 13: training on 130966 raw words (7306 effective words) took 0.4s, 18697 effective words/s\n",
      "INFO - 23:08:47: EPOCH 14: training on 130966 raw words (7352 effective words) took 0.4s, 19034 effective words/s\n",
      "INFO - 23:08:47: EPOCH 15: training on 130966 raw words (7469 effective words) took 0.4s, 19634 effective words/s\n",
      "INFO - 23:08:47: EPOCH 16: training on 130966 raw words (7360 effective words) took 0.4s, 18860 effective words/s\n",
      "INFO - 23:08:48: EPOCH 17: training on 130966 raw words (7539 effective words) took 0.4s, 19082 effective words/s\n",
      "INFO - 23:08:48: EPOCH 18: training on 130966 raw words (7319 effective words) took 0.4s, 17946 effective words/s\n",
      "INFO - 23:08:49: EPOCH 19: training on 130966 raw words (7395 effective words) took 0.4s, 18781 effective words/s\n",
      "INFO - 23:08:49: EPOCH 20: training on 130966 raw words (7399 effective words) took 0.4s, 18955 effective words/s\n",
      "INFO - 23:08:49: EPOCH 21: training on 130966 raw words (7378 effective words) took 0.4s, 18586 effective words/s\n",
      "INFO - 23:08:50: EPOCH 22: training on 130966 raw words (7336 effective words) took 0.4s, 16474 effective words/s\n",
      "INFO - 23:08:50: EPOCH 23: training on 130966 raw words (7392 effective words) took 0.4s, 19052 effective words/s\n",
      "INFO - 23:08:51: EPOCH 24: training on 130966 raw words (7379 effective words) took 0.4s, 19337 effective words/s\n",
      "INFO - 23:08:51: EPOCH 25: training on 130966 raw words (7317 effective words) took 0.4s, 16504 effective words/s\n",
      "INFO - 23:08:51: EPOCH 26: training on 130966 raw words (7419 effective words) took 0.4s, 19267 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:08:52: EPOCH 27: training on 130966 raw words (7351 effective words) took 0.4s, 18992 effective words/s\n",
      "INFO - 23:08:52: EPOCH 28: training on 130966 raw words (7385 effective words) took 0.4s, 18558 effective words/s\n",
      "INFO - 23:08:53: EPOCH 29: training on 130966 raw words (7400 effective words) took 0.4s, 18648 effective words/s\n",
      "INFO - 23:08:53: EPOCH 30: training on 130966 raw words (7327 effective words) took 0.4s, 18088 effective words/s\n",
      "INFO - 23:08:53: EPOCH 31: training on 130966 raw words (7601 effective words) took 0.4s, 18978 effective words/s\n",
      "INFO - 23:08:54: EPOCH 32: training on 130966 raw words (7325 effective words) took 0.4s, 18379 effective words/s\n",
      "INFO - 23:08:54: EPOCH 33: training on 130966 raw words (7349 effective words) took 0.5s, 16265 effective words/s\n",
      "INFO - 23:08:55: EPOCH 34: training on 130966 raw words (7440 effective words) took 0.4s, 19152 effective words/s\n",
      "INFO - 23:08:55: EPOCH 35: training on 130966 raw words (7329 effective words) took 0.4s, 19234 effective words/s\n",
      "INFO - 23:08:55: EPOCH 36: training on 130966 raw words (7311 effective words) took 0.4s, 18966 effective words/s\n",
      "INFO - 23:08:56: EPOCH 37: training on 130966 raw words (7407 effective words) took 0.4s, 19405 effective words/s\n",
      "INFO - 23:08:56: EPOCH 38: training on 130966 raw words (7387 effective words) took 0.4s, 18870 effective words/s\n",
      "INFO - 23:08:57: EPOCH 39: training on 130966 raw words (7328 effective words) took 0.4s, 19080 effective words/s\n",
      "INFO - 23:08:57: EPOCH 40: training on 130966 raw words (7422 effective words) took 0.4s, 19402 effective words/s\n",
      "INFO - 23:08:57: EPOCH 41: training on 130966 raw words (7307 effective words) took 0.4s, 18362 effective words/s\n",
      "INFO - 23:08:58: EPOCH 42: training on 130966 raw words (7432 effective words) took 0.4s, 18924 effective words/s\n",
      "INFO - 23:08:58: EPOCH 43: training on 130966 raw words (7506 effective words) took 0.4s, 18849 effective words/s\n",
      "INFO - 23:08:59: EPOCH 44: training on 130966 raw words (7415 effective words) took 0.4s, 18814 effective words/s\n",
      "INFO - 23:08:59: EPOCH 45: training on 130966 raw words (7357 effective words) took 0.4s, 18752 effective words/s\n",
      "INFO - 23:08:59: EPOCH 46: training on 130966 raw words (7435 effective words) took 0.4s, 18265 effective words/s\n",
      "INFO - 23:09:00: EPOCH 47: training on 130966 raw words (7391 effective words) took 0.4s, 19202 effective words/s\n",
      "INFO - 23:09:00: EPOCH 48: training on 130966 raw words (7477 effective words) took 0.4s, 19212 effective words/s\n",
      "INFO - 23:09:01: EPOCH 49: training on 130966 raw words (7427 effective words) took 0.4s, 19293 effective words/s\n",
      "INFO - 23:09:01: Word2Vec lifecycle event {'msg': 'training on 6548300 raw words (369683 effective words) took 20.0s, 18468 effective words/s', 'datetime': '2023-04-01T23:09:01.067539', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:09:01: collecting all words and their counts\n",
      "INFO - 23:09:01: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:09:01: PROGRESS: at sentence #10000, processed 138397 words and 60525 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-02-05 completed\n",
      "time taken: 0.36 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:09:01: collected 60525 token types (unigram + bigrams) from a corpus of 152864 words and 11040 sentences\n",
      "INFO - 23:09:01: merged Phrases<60525 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:01: Phrases lifecycle event {'msg': 'built Phrases<60525 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.35s', 'datetime': '2023-04-01T23:09:01.425930', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:01: exporting phrases from Phrases<60525 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:01: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<802 phrases, min_count=5, threshold=7> from Phrases<60525 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.14s', 'datetime': '2023-04-01T23:09:01.567587', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:01: collecting all words and their counts\n",
      "INFO - 23:09:01: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:09:01: PROGRESS: at sentence #10000, processed 128977 words and 62172 word types\n",
      "INFO - 23:09:01: collected 62172 token types (unigram + bigrams) from a corpus of 142482 words and 11040 sentences\n",
      "INFO - 23:09:01: merged Phrases<62172 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:01: Phrases lifecycle event {'msg': 'built Phrases<62172 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.41s', 'datetime': '2023-04-01T23:09:01.979449', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:01: exporting phrases from Phrases<62172 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:02: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<853 phrases, min_count=5, threshold=7> from Phrases<62172 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.14s', 'datetime': '2023-04-01T23:09:02.119114', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:02: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:09:02.121070', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:02: collecting all words and their counts\n",
      "INFO - 23:09:02: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:09:02: PROGRESS: at sentence #10000, processed 126323 words, keeping 12337 word types\n",
      "INFO - 23:09:02: collected 12337 word types from a corpus of 139576 raw words and 11040 sentences\n",
      "INFO - 23:09:02: Creating a fresh vocabulary\n",
      "INFO - 23:09:02: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 990 unique words (8.02% of original 12337, drops 11347)', 'datetime': '2023-04-01T23:09:02.546234', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:09:02: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 91278 word corpus (65.40% of original 139576, drops 48298)', 'datetime': '2023-04-01T23:09:02.547230', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:09:02: deleting the raw counts dictionary of 12337 items\n",
      "INFO - 23:09:02: sample=1e-05 downsamples 990 most-common words\n",
      "INFO - 23:09:02: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 8186.536589129052 word corpus (9.0%% of prior 91278)', 'datetime': '2023-04-01T23:09:02.558202', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:09:02: constructing a huffman tree from 990 words\n",
      "INFO - 23:09:02: built huffman tree with maximum node depth 12\n",
      "INFO - 23:09:02: estimated required memory for 990 words and 100 dimensions: 1881000 bytes\n",
      "INFO - 23:09:02: resetting layer weights\n",
      "INFO - 23:09:02: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:09:02.614052', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:09:02: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 990 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:09:02.615049', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:09:03: EPOCH 0: training on 139576 raw words (8199 effective words) took 0.5s, 16574 effective words/s\n",
      "INFO - 23:09:03: EPOCH 1: training on 139576 raw words (8173 effective words) took 0.4s, 19357 effective words/s\n",
      "INFO - 23:09:03: EPOCH 2: training on 139576 raw words (8370 effective words) took 0.4s, 20148 effective words/s\n",
      "INFO - 23:09:04: EPOCH 3: training on 139576 raw words (8025 effective words) took 0.4s, 18833 effective words/s\n",
      "INFO - 23:09:04: EPOCH 4: training on 139576 raw words (8108 effective words) took 0.4s, 19256 effective words/s\n",
      "INFO - 23:09:05: EPOCH 5: training on 139576 raw words (8134 effective words) took 0.4s, 19989 effective words/s\n",
      "INFO - 23:09:05: EPOCH 6: training on 139576 raw words (8234 effective words) took 0.4s, 19725 effective words/s\n",
      "INFO - 23:09:06: EPOCH 7: training on 139576 raw words (8216 effective words) took 0.4s, 19906 effective words/s\n",
      "INFO - 23:09:06: EPOCH 8: training on 139576 raw words (8072 effective words) took 0.4s, 19716 effective words/s\n",
      "INFO - 23:09:06: EPOCH 9: training on 139576 raw words (8125 effective words) took 0.5s, 17130 effective words/s\n",
      "INFO - 23:09:07: EPOCH 10: training on 139576 raw words (8284 effective words) took 0.4s, 20417 effective words/s\n",
      "INFO - 23:09:07: EPOCH 11: training on 139576 raw words (8147 effective words) took 0.4s, 19128 effective words/s\n",
      "INFO - 23:09:08: EPOCH 12: training on 139576 raw words (8165 effective words) took 0.5s, 16724 effective words/s\n",
      "INFO - 23:09:08: EPOCH 13: training on 139576 raw words (8077 effective words) took 0.4s, 18837 effective words/s\n",
      "INFO - 23:09:09: EPOCH 14: training on 139576 raw words (8260 effective words) took 0.4s, 19766 effective words/s\n",
      "INFO - 23:09:09: EPOCH 15: training on 139576 raw words (8149 effective words) took 0.4s, 19627 effective words/s\n",
      "INFO - 23:09:09: EPOCH 16: training on 139576 raw words (8326 effective words) took 0.4s, 19999 effective words/s\n",
      "INFO - 23:09:10: EPOCH 17: training on 139576 raw words (8339 effective words) took 0.4s, 20450 effective words/s\n",
      "INFO - 23:09:10: EPOCH 18: training on 139576 raw words (8304 effective words) took 0.4s, 20557 effective words/s\n",
      "INFO - 23:09:11: EPOCH 19: training on 139576 raw words (8215 effective words) took 0.4s, 20181 effective words/s\n",
      "INFO - 23:09:11: EPOCH 20: training on 139576 raw words (8300 effective words) took 0.4s, 20269 effective words/s\n",
      "INFO - 23:09:12: EPOCH 21: training on 139576 raw words (8201 effective words) took 0.4s, 19935 effective words/s\n",
      "INFO - 23:09:12: EPOCH 22: training on 139576 raw words (8212 effective words) took 0.4s, 20081 effective words/s\n",
      "INFO - 23:09:12: EPOCH 23: training on 139576 raw words (8356 effective words) took 0.4s, 19907 effective words/s\n",
      "INFO - 23:09:13: EPOCH 24: training on 139576 raw words (8057 effective words) took 0.4s, 19305 effective words/s\n",
      "INFO - 23:09:13: EPOCH 25: training on 139576 raw words (8071 effective words) took 0.4s, 19040 effective words/s\n",
      "INFO - 23:09:14: EPOCH 26: training on 139576 raw words (8128 effective words) took 0.4s, 19377 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:09:14: EPOCH 27: training on 139576 raw words (8223 effective words) took 0.5s, 17324 effective words/s\n",
      "INFO - 23:09:15: EPOCH 28: training on 139576 raw words (8235 effective words) took 0.4s, 19165 effective words/s\n",
      "INFO - 23:09:15: EPOCH 29: training on 139576 raw words (8253 effective words) took 0.4s, 20032 effective words/s\n",
      "INFO - 23:09:15: EPOCH 30: training on 139576 raw words (8157 effective words) took 0.4s, 19362 effective words/s\n",
      "INFO - 23:09:16: EPOCH 31: training on 139576 raw words (8225 effective words) took 0.4s, 20075 effective words/s\n",
      "INFO - 23:09:16: EPOCH 32: training on 139576 raw words (8089 effective words) took 0.4s, 19510 effective words/s\n",
      "INFO - 23:09:17: EPOCH 33: training on 139576 raw words (8302 effective words) took 0.4s, 19926 effective words/s\n",
      "INFO - 23:09:17: EPOCH 34: training on 139576 raw words (8285 effective words) took 0.4s, 19837 effective words/s\n",
      "INFO - 23:09:18: EPOCH 35: training on 139576 raw words (8239 effective words) took 0.4s, 19521 effective words/s\n",
      "INFO - 23:09:18: EPOCH 36: training on 139576 raw words (8086 effective words) took 0.4s, 19137 effective words/s\n",
      "INFO - 23:09:18: EPOCH 37: training on 139576 raw words (8022 effective words) took 0.4s, 18834 effective words/s\n",
      "INFO - 23:09:19: EPOCH 38: training on 139576 raw words (8101 effective words) took 0.4s, 19494 effective words/s\n",
      "INFO - 23:09:19: EPOCH 39: training on 139576 raw words (8249 effective words) took 0.4s, 19817 effective words/s\n",
      "INFO - 23:09:20: EPOCH 40: training on 139576 raw words (8174 effective words) took 0.4s, 19984 effective words/s\n",
      "INFO - 23:09:20: EPOCH 41: training on 139576 raw words (8137 effective words) took 0.4s, 19887 effective words/s\n",
      "INFO - 23:09:20: EPOCH 42: training on 139576 raw words (8173 effective words) took 0.4s, 19976 effective words/s\n",
      "INFO - 23:09:21: EPOCH 43: training on 139576 raw words (8276 effective words) took 0.4s, 19789 effective words/s\n",
      "INFO - 23:09:21: EPOCH 44: training on 139576 raw words (8279 effective words) took 0.4s, 20042 effective words/s\n",
      "INFO - 23:09:22: EPOCH 45: training on 139576 raw words (8092 effective words) took 0.5s, 17499 effective words/s\n",
      "INFO - 23:09:22: EPOCH 46: training on 139576 raw words (8059 effective words) took 0.4s, 19611 effective words/s\n",
      "INFO - 23:09:23: EPOCH 47: training on 139576 raw words (8139 effective words) took 0.4s, 19518 effective words/s\n",
      "INFO - 23:09:23: EPOCH 48: training on 139576 raw words (8137 effective words) took 0.4s, 18908 effective words/s\n",
      "INFO - 23:09:24: EPOCH 49: training on 139576 raw words (8078 effective words) took 0.5s, 16658 effective words/s\n",
      "INFO - 23:09:24: Word2Vec lifecycle event {'msg': 'training on 6978800 raw words (409257 effective words) took 21.4s, 19099 effective words/s', 'datetime': '2023-04-01T23:09:24.044547', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:09:24: collecting all words and their counts\n",
      "INFO - 23:09:24: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:09:24: PROGRESS: at sentence #10000, processed 137468 words and 59089 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-02-12 completed\n",
      "time taken: 0.38 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:09:24: collected 59089 token types (unigram + bigrams) from a corpus of 146988 words and 10682 sentences\n",
      "INFO - 23:09:24: merged Phrases<59089 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:24: Phrases lifecycle event {'msg': 'built Phrases<59089 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.23s', 'datetime': '2023-04-01T23:09:24.281152', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:24: exporting phrases from Phrases<59089 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:24: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<736 phrases, min_count=5, threshold=7> from Phrases<59089 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.13s', 'datetime': '2023-04-01T23:09:24.412839', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:24: collecting all words and their counts\n",
      "INFO - 23:09:24: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:09:24: PROGRESS: at sentence #10000, processed 128815 words and 60491 word types\n",
      "INFO - 23:09:24: collected 60491 token types (unigram + bigrams) from a corpus of 137772 words and 10682 sentences\n",
      "INFO - 23:09:24: merged Phrases<60491 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:24: Phrases lifecycle event {'msg': 'built Phrases<60491 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.39s', 'datetime': '2023-04-01T23:09:24.804786', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:24: exporting phrases from Phrases<60491 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:24: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<753 phrases, min_count=5, threshold=7> from Phrases<60491 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.14s', 'datetime': '2023-04-01T23:09:24.944436', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:24: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:09:24.945421', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:24: collecting all words and their counts\n",
      "INFO - 23:09:24: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:09:25: PROGRESS: at sentence #10000, processed 126065 words, keeping 12162 word types\n",
      "INFO - 23:09:25: collected 12162 word types from a corpus of 134846 raw words and 10682 sentences\n",
      "INFO - 23:09:25: Creating a fresh vocabulary\n",
      "INFO - 23:09:25: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 955 unique words (7.85% of original 12162, drops 11207)', 'datetime': '2023-04-01T23:09:25.342314', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:09:25: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 87660 word corpus (65.01% of original 134846, drops 47186)', 'datetime': '2023-04-01T23:09:25.343311', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:09:25: deleting the raw counts dictionary of 12162 items\n",
      "INFO - 23:09:25: sample=1e-05 downsamples 955 most-common words\n",
      "INFO - 23:09:25: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7697.543134981301 word corpus (8.8%% of prior 87660)', 'datetime': '2023-04-01T23:09:25.355280', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:09:25: constructing a huffman tree from 955 words\n",
      "INFO - 23:09:25: built huffman tree with maximum node depth 12\n",
      "INFO - 23:09:25: estimated required memory for 955 words and 100 dimensions: 1814500 bytes\n",
      "INFO - 23:09:25: resetting layer weights\n",
      "INFO - 23:09:25: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:09:25.404150', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:09:25: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 955 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:09:25.405151', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:09:25: EPOCH 0: training on 134846 raw words (7605 effective words) took 0.4s, 18972 effective words/s\n",
      "INFO - 23:09:26: EPOCH 1: training on 134846 raw words (7769 effective words) took 0.4s, 19703 effective words/s\n",
      "INFO - 23:09:26: EPOCH 2: training on 134846 raw words (7728 effective words) took 0.4s, 18989 effective words/s\n",
      "INFO - 23:09:27: EPOCH 3: training on 134846 raw words (7620 effective words) took 0.4s, 18096 effective words/s\n",
      "INFO - 23:09:27: EPOCH 4: training on 134846 raw words (7721 effective words) took 0.4s, 19186 effective words/s\n",
      "INFO - 23:09:27: EPOCH 5: training on 134846 raw words (7918 effective words) took 0.4s, 19125 effective words/s\n",
      "INFO - 23:09:28: EPOCH 6: training on 134846 raw words (7770 effective words) took 0.4s, 19055 effective words/s\n",
      "INFO - 23:09:28: EPOCH 7: training on 134846 raw words (7551 effective words) took 0.4s, 18114 effective words/s\n",
      "INFO - 23:09:29: EPOCH 8: training on 134846 raw words (7595 effective words) took 0.4s, 18690 effective words/s\n",
      "INFO - 23:09:29: EPOCH 9: training on 134846 raw words (7816 effective words) took 0.4s, 19456 effective words/s\n",
      "INFO - 23:09:29: EPOCH 10: training on 134846 raw words (7761 effective words) took 0.4s, 19192 effective words/s\n",
      "INFO - 23:09:30: EPOCH 11: training on 134846 raw words (7637 effective words) took 0.5s, 16937 effective words/s\n",
      "INFO - 23:09:30: EPOCH 12: training on 134846 raw words (7769 effective words) took 0.4s, 19683 effective words/s\n",
      "INFO - 23:09:31: EPOCH 13: training on 134846 raw words (7722 effective words) took 0.4s, 18972 effective words/s\n",
      "INFO - 23:09:31: EPOCH 14: training on 134846 raw words (7812 effective words) took 0.4s, 19583 effective words/s\n",
      "INFO - 23:09:32: EPOCH 15: training on 134846 raw words (7750 effective words) took 0.4s, 19501 effective words/s\n",
      "INFO - 23:09:32: EPOCH 16: training on 134846 raw words (7684 effective words) took 0.4s, 19192 effective words/s\n",
      "INFO - 23:09:32: EPOCH 17: training on 134846 raw words (7645 effective words) took 0.4s, 18864 effective words/s\n",
      "INFO - 23:09:33: EPOCH 18: training on 134846 raw words (7650 effective words) took 0.4s, 18456 effective words/s\n",
      "INFO - 23:09:33: EPOCH 19: training on 134846 raw words (7622 effective words) took 0.4s, 18285 effective words/s\n",
      "INFO - 23:09:34: EPOCH 20: training on 134846 raw words (7703 effective words) took 0.4s, 19057 effective words/s\n",
      "INFO - 23:09:34: EPOCH 21: training on 134846 raw words (7804 effective words) took 0.4s, 19384 effective words/s\n",
      "INFO - 23:09:34: EPOCH 22: training on 134846 raw words (7594 effective words) took 0.4s, 19249 effective words/s\n",
      "INFO - 23:09:35: EPOCH 23: training on 134846 raw words (7666 effective words) took 0.5s, 16688 effective words/s\n",
      "INFO - 23:09:35: EPOCH 24: training on 134846 raw words (7697 effective words) took 0.4s, 19403 effective words/s\n",
      "INFO - 23:09:36: EPOCH 25: training on 134846 raw words (7624 effective words) took 0.4s, 19101 effective words/s\n",
      "INFO - 23:09:36: EPOCH 26: training on 134846 raw words (7640 effective words) took 0.4s, 19146 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:09:36: EPOCH 27: training on 134846 raw words (7499 effective words) took 0.4s, 18608 effective words/s\n",
      "INFO - 23:09:37: EPOCH 28: training on 134846 raw words (7636 effective words) took 0.4s, 17198 effective words/s\n",
      "INFO - 23:09:37: EPOCH 29: training on 134846 raw words (7728 effective words) took 0.4s, 18918 effective words/s\n",
      "INFO - 23:09:38: EPOCH 30: training on 134846 raw words (7528 effective words) took 0.4s, 18611 effective words/s\n",
      "INFO - 23:09:38: EPOCH 31: training on 134846 raw words (7597 effective words) took 0.4s, 18473 effective words/s\n",
      "INFO - 23:09:39: EPOCH 32: training on 134846 raw words (7662 effective words) took 0.4s, 18598 effective words/s\n",
      "INFO - 23:09:39: EPOCH 33: training on 134846 raw words (7626 effective words) took 0.4s, 18768 effective words/s\n",
      "INFO - 23:09:39: EPOCH 34: training on 134846 raw words (7680 effective words) took 0.5s, 16676 effective words/s\n",
      "INFO - 23:09:40: EPOCH 35: training on 134846 raw words (7621 effective words) took 0.4s, 19351 effective words/s\n",
      "INFO - 23:09:40: EPOCH 36: training on 134846 raw words (7815 effective words) took 0.4s, 19616 effective words/s\n",
      "INFO - 23:09:41: EPOCH 37: training on 134846 raw words (7734 effective words) took 0.4s, 19558 effective words/s\n",
      "INFO - 23:09:41: EPOCH 38: training on 134846 raw words (7711 effective words) took 0.4s, 19399 effective words/s\n",
      "INFO - 23:09:41: EPOCH 39: training on 134846 raw words (7721 effective words) took 0.4s, 19318 effective words/s\n",
      "INFO - 23:09:42: EPOCH 40: training on 134846 raw words (7716 effective words) took 0.4s, 19187 effective words/s\n",
      "INFO - 23:09:42: EPOCH 41: training on 134846 raw words (7642 effective words) took 0.4s, 18815 effective words/s\n",
      "INFO - 23:09:43: EPOCH 42: training on 134846 raw words (7551 effective words) took 0.4s, 18599 effective words/s\n",
      "INFO - 23:09:43: EPOCH 43: training on 134846 raw words (7833 effective words) took 0.4s, 19150 effective words/s\n",
      "INFO - 23:09:43: EPOCH 44: training on 134846 raw words (7621 effective words) took 0.4s, 18641 effective words/s\n",
      "INFO - 23:09:44: EPOCH 45: training on 134846 raw words (7659 effective words) took 0.4s, 18654 effective words/s\n",
      "INFO - 23:09:44: EPOCH 46: training on 134846 raw words (7754 effective words) took 0.5s, 17043 effective words/s\n",
      "INFO - 23:09:45: EPOCH 47: training on 134846 raw words (7729 effective words) took 0.4s, 19612 effective words/s\n",
      "INFO - 23:09:45: EPOCH 48: training on 134846 raw words (7685 effective words) took 0.4s, 19066 effective words/s\n",
      "INFO - 23:09:46: EPOCH 49: training on 134846 raw words (7702 effective words) took 0.4s, 18949 effective words/s\n",
      "INFO - 23:09:46: Word2Vec lifecycle event {'msg': 'training on 6742300 raw words (384323 effective words) took 20.7s, 18584 effective words/s', 'datetime': '2023-04-01T23:09:46.086656', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:09:46: collecting all words and their counts\n",
      "INFO - 23:09:46: PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-02-19 completed\n",
      "time taken: 0.37 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:09:46: PROGRESS: at sentence #10000, processed 140157 words and 67072 word types\n",
      "INFO - 23:09:46: collected 67072 token types (unigram + bigrams) from a corpus of 173248 words and 12630 sentences\n",
      "INFO - 23:09:46: merged Phrases<67072 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:46: Phrases lifecycle event {'msg': 'built Phrases<67072 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.27s', 'datetime': '2023-04-01T23:09:46.369925', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:46: exporting phrases from Phrases<67072 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:46: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<921 phrases, min_count=5, threshold=7> from Phrases<67072 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.16s', 'datetime': '2023-04-01T23:09:46.538475', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:46: collecting all words and their counts\n",
      "INFO - 23:09:46: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:09:46: PROGRESS: at sentence #10000, processed 130914 words and 68944 word types\n",
      "INFO - 23:09:46: collected 68944 token types (unigram + bigrams) from a corpus of 161672 words and 12630 sentences\n",
      "INFO - 23:09:46: merged Phrases<68944 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:46: Phrases lifecycle event {'msg': 'built Phrases<68944 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.45s', 'datetime': '2023-04-01T23:09:46.993259', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:46: exporting phrases from Phrases<68944 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:09:47: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<911 phrases, min_count=5, threshold=7> from Phrases<68944 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.16s', 'datetime': '2023-04-01T23:09:47.157855', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:47: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:09:47.158854', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:09:47: collecting all words and their counts\n",
      "INFO - 23:09:47: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:09:47: PROGRESS: at sentence #10000, processed 128574 words, keeping 13103 word types\n",
      "INFO - 23:09:47: collected 13103 word types from a corpus of 158706 raw words and 12630 sentences\n",
      "INFO - 23:09:47: Creating a fresh vocabulary\n",
      "INFO - 23:09:47: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1149 unique words (8.77% of original 13103, drops 11954)', 'datetime': '2023-04-01T23:09:47.627565', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:09:47: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 107426 word corpus (67.69% of original 158706, drops 51280)', 'datetime': '2023-04-01T23:09:47.628560', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:09:47: deleting the raw counts dictionary of 13103 items\n",
      "INFO - 23:09:47: sample=1e-05 downsamples 1149 most-common words\n",
      "INFO - 23:09:47: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 10405.37143331339 word corpus (9.7%% of prior 107426)', 'datetime': '2023-04-01T23:09:47.642522', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:09:47: constructing a huffman tree from 1149 words\n",
      "INFO - 23:09:47: built huffman tree with maximum node depth 12\n",
      "INFO - 23:09:47: estimated required memory for 1149 words and 100 dimensions: 2183100 bytes\n",
      "INFO - 23:09:47: resetting layer weights\n",
      "INFO - 23:09:47: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:09:47.707352', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:09:47: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1149 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:09:47.708349', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:09:48: EPOCH 0: training on 158706 raw words (10451 effective words) took 0.5s, 21175 effective words/s\n",
      "INFO - 23:09:48: EPOCH 1: training on 158706 raw words (10578 effective words) took 0.5s, 21057 effective words/s\n",
      "INFO - 23:09:49: EPOCH 2: training on 158706 raw words (10385 effective words) took 0.5s, 19241 effective words/s\n",
      "INFO - 23:09:49: EPOCH 3: training on 158706 raw words (10340 effective words) took 0.5s, 19544 effective words/s\n",
      "INFO - 23:09:50: EPOCH 4: training on 158706 raw words (10324 effective words) took 0.5s, 19150 effective words/s\n",
      "INFO - 23:09:50: EPOCH 5: training on 158706 raw words (10500 effective words) took 0.5s, 20861 effective words/s\n",
      "INFO - 23:09:51: EPOCH 6: training on 158706 raw words (10436 effective words) took 0.5s, 20706 effective words/s\n",
      "INFO - 23:09:51: EPOCH 7: training on 158706 raw words (10413 effective words) took 0.5s, 21213 effective words/s\n",
      "INFO - 23:09:52: EPOCH 8: training on 158706 raw words (10452 effective words) took 0.5s, 19850 effective words/s\n",
      "INFO - 23:09:52: EPOCH 9: training on 158706 raw words (10441 effective words) took 0.6s, 17373 effective words/s\n",
      "INFO - 23:09:53: EPOCH 10: training on 158706 raw words (10571 effective words) took 0.5s, 20752 effective words/s\n",
      "INFO - 23:09:54: EPOCH 11: training on 158706 raw words (10475 effective words) took 0.6s, 18951 effective words/s\n",
      "INFO - 23:09:54: EPOCH 12: training on 158706 raw words (10517 effective words) took 0.5s, 20954 effective words/s\n",
      "INFO - 23:09:55: EPOCH 13: training on 158706 raw words (10486 effective words) took 0.5s, 21425 effective words/s\n",
      "INFO - 23:09:55: EPOCH 14: training on 158706 raw words (10459 effective words) took 0.5s, 20748 effective words/s\n",
      "INFO - 23:09:56: EPOCH 15: training on 158706 raw words (10378 effective words) took 0.5s, 21252 effective words/s\n",
      "INFO - 23:09:56: EPOCH 16: training on 158706 raw words (10411 effective words) took 0.5s, 20523 effective words/s\n",
      "INFO - 23:09:57: EPOCH 17: training on 158706 raw words (10389 effective words) took 0.5s, 21153 effective words/s\n",
      "INFO - 23:09:57: EPOCH 18: training on 158706 raw words (10282 effective words) took 0.5s, 21292 effective words/s\n",
      "INFO - 23:09:58: EPOCH 19: training on 158706 raw words (10484 effective words) took 0.5s, 20952 effective words/s\n",
      "INFO - 23:09:58: EPOCH 20: training on 158706 raw words (10445 effective words) took 0.5s, 21329 effective words/s\n",
      "INFO - 23:09:59: EPOCH 21: training on 158706 raw words (10476 effective words) took 0.6s, 18963 effective words/s\n",
      "INFO - 23:09:59: EPOCH 22: training on 158706 raw words (10418 effective words) took 0.5s, 22127 effective words/s\n",
      "INFO - 23:10:00: EPOCH 23: training on 158706 raw words (10464 effective words) took 0.5s, 21946 effective words/s\n",
      "INFO - 23:10:00: EPOCH 24: training on 158706 raw words (10319 effective words) took 0.5s, 21809 effective words/s\n",
      "INFO - 23:10:01: EPOCH 25: training on 158706 raw words (10397 effective words) took 0.5s, 22265 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:10:01: EPOCH 26: training on 158706 raw words (10372 effective words) took 0.5s, 22010 effective words/s\n",
      "INFO - 23:10:01: EPOCH 27: training on 158706 raw words (10300 effective words) took 0.5s, 21703 effective words/s\n",
      "INFO - 23:10:02: EPOCH 28: training on 158706 raw words (10518 effective words) took 0.5s, 22117 effective words/s\n",
      "INFO - 23:10:02: EPOCH 29: training on 158706 raw words (10337 effective words) took 0.5s, 21530 effective words/s\n",
      "INFO - 23:10:03: EPOCH 30: training on 158706 raw words (10365 effective words) took 0.5s, 21332 effective words/s\n",
      "INFO - 23:10:03: EPOCH 31: training on 158706 raw words (10450 effective words) took 0.5s, 21378 effective words/s\n",
      "INFO - 23:10:04: EPOCH 32: training on 158706 raw words (10423 effective words) took 0.5s, 21813 effective words/s\n",
      "INFO - 23:10:04: EPOCH 33: training on 158706 raw words (10442 effective words) took 0.5s, 22468 effective words/s\n",
      "INFO - 23:10:05: EPOCH 34: training on 158706 raw words (10450 effective words) took 0.5s, 20162 effective words/s\n",
      "INFO - 23:10:05: EPOCH 35: training on 158706 raw words (10269 effective words) took 0.5s, 21566 effective words/s\n",
      "INFO - 23:10:06: EPOCH 36: training on 158706 raw words (10365 effective words) took 0.5s, 22120 effective words/s\n",
      "INFO - 23:10:06: EPOCH 37: training on 158706 raw words (10577 effective words) took 0.5s, 22348 effective words/s\n",
      "INFO - 23:10:07: EPOCH 38: training on 158706 raw words (10490 effective words) took 0.5s, 22360 effective words/s\n",
      "INFO - 23:10:07: EPOCH 39: training on 158706 raw words (10528 effective words) took 0.5s, 19751 effective words/s\n",
      "INFO - 23:10:08: EPOCH 40: training on 158706 raw words (10454 effective words) took 0.5s, 21312 effective words/s\n",
      "INFO - 23:10:08: EPOCH 41: training on 158706 raw words (10494 effective words) took 0.5s, 21525 effective words/s\n",
      "INFO - 23:10:09: EPOCH 42: training on 158706 raw words (10477 effective words) took 0.5s, 21713 effective words/s\n",
      "INFO - 23:10:09: EPOCH 43: training on 158706 raw words (10271 effective words) took 0.5s, 21847 effective words/s\n",
      "INFO - 23:10:10: EPOCH 44: training on 158706 raw words (10446 effective words) took 0.5s, 21989 effective words/s\n",
      "INFO - 23:10:10: EPOCH 45: training on 158706 raw words (10533 effective words) took 0.5s, 22343 effective words/s\n",
      "INFO - 23:10:11: EPOCH 46: training on 158706 raw words (10498 effective words) took 0.5s, 21978 effective words/s\n",
      "INFO - 23:10:11: EPOCH 47: training on 158706 raw words (10294 effective words) took 0.5s, 21998 effective words/s\n",
      "INFO - 23:10:12: EPOCH 48: training on 158706 raw words (10413 effective words) took 0.5s, 19582 effective words/s\n",
      "INFO - 23:10:12: EPOCH 49: training on 158706 raw words (10206 effective words) took 0.5s, 20967 effective words/s\n",
      "INFO - 23:10:12: Word2Vec lifecycle event {'msg': 'training on 7935300 raw words (521263 effective words) took 25.0s, 20832 effective words/s', 'datetime': '2023-04-01T23:10:12.731513', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:10:12: collecting all words and their counts\n",
      "INFO - 23:10:12: PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-02-26 completed\n",
      "time taken: 0.44 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:10:12: PROGRESS: at sentence #10000, processed 145641 words and 63238 word types\n",
      "INFO - 23:10:13: collected 63238 token types (unigram + bigrams) from a corpus of 160176 words and 11018 sentences\n",
      "INFO - 23:10:13: merged Phrases<63238 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:13: Phrases lifecycle event {'msg': 'built Phrases<63238 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.26s', 'datetime': '2023-04-01T23:10:13.005133', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:13: exporting phrases from Phrases<63238 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:13: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<753 phrases, min_count=5, threshold=7> from Phrases<63238 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.14s', 'datetime': '2023-04-01T23:10:13.156727', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:13: collecting all words and their counts\n",
      "INFO - 23:10:13: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:10:13: PROGRESS: at sentence #10000, processed 136635 words and 64778 word types\n",
      "INFO - 23:10:13: collected 64778 token types (unigram + bigrams) from a corpus of 150344 words and 11018 sentences\n",
      "INFO - 23:10:13: merged Phrases<64778 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:13: Phrases lifecycle event {'msg': 'built Phrases<64778 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.44s', 'datetime': '2023-04-01T23:10:13.597773', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:13: exporting phrases from Phrases<64778 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:13: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<812 phrases, min_count=5, threshold=7> from Phrases<64778 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.15s', 'datetime': '2023-04-01T23:10:13.750363', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:13: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:10:13.752357', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:13: collecting all words and their counts\n",
      "INFO - 23:10:13: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:10:14: PROGRESS: at sentence #10000, processed 133875 words, keeping 12709 word types\n",
      "INFO - 23:10:14: collected 12709 word types from a corpus of 147342 raw words and 11018 sentences\n",
      "INFO - 23:10:14: Creating a fresh vocabulary\n",
      "INFO - 23:10:14: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1049 unique words (8.25% of original 12709, drops 11660)', 'datetime': '2023-04-01T23:10:14.198167', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:10:14: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 98358 word corpus (66.75% of original 147342, drops 48984)', 'datetime': '2023-04-01T23:10:14.200162', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:10:14: deleting the raw counts dictionary of 12709 items\n",
      "INFO - 23:10:14: sample=1e-05 downsamples 1049 most-common words\n",
      "INFO - 23:10:14: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 9055.893657829065 word corpus (9.2%% of prior 98358)', 'datetime': '2023-04-01T23:10:14.212128', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:10:14: constructing a huffman tree from 1049 words\n",
      "INFO - 23:10:14: built huffman tree with maximum node depth 12\n",
      "INFO - 23:10:14: estimated required memory for 1049 words and 100 dimensions: 1993100 bytes\n",
      "INFO - 23:10:14: resetting layer weights\n",
      "INFO - 23:10:14: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:10:14.272967', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:10:14: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1049 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:10:14.274962', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:10:14: EPOCH 0: training on 147342 raw words (8942 effective words) took 0.4s, 20486 effective words/s\n",
      "INFO - 23:10:15: EPOCH 1: training on 147342 raw words (9046 effective words) took 0.4s, 21317 effective words/s\n",
      "INFO - 23:10:15: EPOCH 2: training on 147342 raw words (8850 effective words) took 0.4s, 19924 effective words/s\n",
      "INFO - 23:10:16: EPOCH 3: training on 147342 raw words (9208 effective words) took 0.4s, 21016 effective words/s\n",
      "INFO - 23:10:16: EPOCH 4: training on 147342 raw words (8998 effective words) took 0.4s, 20857 effective words/s\n",
      "INFO - 23:10:16: EPOCH 5: training on 147342 raw words (9139 effective words) took 0.4s, 20953 effective words/s\n",
      "INFO - 23:10:17: EPOCH 6: training on 147342 raw words (8948 effective words) took 0.4s, 20559 effective words/s\n",
      "INFO - 23:10:17: EPOCH 7: training on 147342 raw words (9167 effective words) took 0.4s, 20559 effective words/s\n",
      "INFO - 23:10:18: EPOCH 8: training on 147342 raw words (8953 effective words) took 0.4s, 20213 effective words/s\n",
      "INFO - 23:10:18: EPOCH 9: training on 147342 raw words (9102 effective words) took 0.4s, 20581 effective words/s\n",
      "INFO - 23:10:19: EPOCH 10: training on 147342 raw words (8989 effective words) took 0.5s, 18085 effective words/s\n",
      "INFO - 23:10:19: EPOCH 11: training on 147342 raw words (8923 effective words) took 0.4s, 20293 effective words/s\n",
      "INFO - 23:10:20: EPOCH 12: training on 147342 raw words (9004 effective words) took 0.4s, 21062 effective words/s\n",
      "INFO - 23:10:20: EPOCH 13: training on 147342 raw words (8938 effective words) took 0.4s, 20620 effective words/s\n",
      "INFO - 23:10:20: EPOCH 14: training on 147342 raw words (8995 effective words) took 0.5s, 19753 effective words/s\n",
      "INFO - 23:10:21: EPOCH 15: training on 147342 raw words (9129 effective words) took 0.4s, 20436 effective words/s\n",
      "INFO - 23:10:21: EPOCH 16: training on 147342 raw words (9096 effective words) took 0.4s, 20601 effective words/s\n",
      "INFO - 23:10:22: EPOCH 17: training on 147342 raw words (9184 effective words) took 0.4s, 20990 effective words/s\n",
      "INFO - 23:10:22: EPOCH 18: training on 147342 raw words (8956 effective words) took 0.6s, 15946 effective words/s\n",
      "INFO - 23:10:23: EPOCH 19: training on 147342 raw words (9007 effective words) took 0.5s, 19295 effective words/s\n",
      "INFO - 23:10:23: EPOCH 20: training on 147342 raw words (9074 effective words) took 0.5s, 19531 effective words/s\n",
      "INFO - 23:10:24: EPOCH 21: training on 147342 raw words (9030 effective words) took 0.5s, 19978 effective words/s\n",
      "INFO - 23:10:24: EPOCH 22: training on 147342 raw words (9006 effective words) took 0.4s, 20027 effective words/s\n",
      "INFO - 23:10:25: EPOCH 23: training on 147342 raw words (8978 effective words) took 0.5s, 18132 effective words/s\n",
      "INFO - 23:10:25: EPOCH 24: training on 147342 raw words (9035 effective words) took 0.5s, 17359 effective words/s\n",
      "INFO - 23:10:26: EPOCH 25: training on 147342 raw words (9167 effective words) took 0.6s, 15250 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:10:26: EPOCH 26: training on 147342 raw words (9034 effective words) took 0.5s, 19160 effective words/s\n",
      "INFO - 23:10:27: EPOCH 27: training on 147342 raw words (9089 effective words) took 0.5s, 18931 effective words/s\n",
      "INFO - 23:10:27: EPOCH 28: training on 147342 raw words (8947 effective words) took 0.6s, 14767 effective words/s\n",
      "INFO - 23:10:28: EPOCH 29: training on 147342 raw words (9052 effective words) took 0.5s, 18862 effective words/s\n",
      "INFO - 23:10:28: EPOCH 30: training on 147342 raw words (9067 effective words) took 0.5s, 19411 effective words/s\n",
      "INFO - 23:10:29: EPOCH 31: training on 147342 raw words (9010 effective words) took 0.5s, 19613 effective words/s\n",
      "INFO - 23:10:29: EPOCH 32: training on 147342 raw words (9022 effective words) took 0.5s, 16967 effective words/s\n",
      "INFO - 23:10:30: EPOCH 33: training on 147342 raw words (8970 effective words) took 0.4s, 20697 effective words/s\n",
      "INFO - 23:10:30: EPOCH 34: training on 147342 raw words (9050 effective words) took 0.5s, 19965 effective words/s\n",
      "INFO - 23:10:31: EPOCH 35: training on 147342 raw words (8938 effective words) took 0.5s, 18942 effective words/s\n",
      "INFO - 23:10:31: EPOCH 36: training on 147342 raw words (9014 effective words) took 0.5s, 18344 effective words/s\n",
      "INFO - 23:10:32: EPOCH 37: training on 147342 raw words (9069 effective words) took 0.8s, 11750 effective words/s\n",
      "INFO - 23:10:32: EPOCH 38: training on 147342 raw words (9250 effective words) took 0.5s, 20148 effective words/s\n",
      "INFO - 23:10:33: EPOCH 39: training on 147342 raw words (8879 effective words) took 0.5s, 17454 effective words/s\n",
      "INFO - 23:10:34: EPOCH 40: training on 147342 raw words (9032 effective words) took 0.5s, 18571 effective words/s\n",
      "INFO - 23:10:34: EPOCH 41: training on 147342 raw words (9226 effective words) took 0.6s, 15884 effective words/s\n",
      "INFO - 23:10:35: EPOCH 42: training on 147342 raw words (9186 effective words) took 0.7s, 12676 effective words/s\n",
      "INFO - 23:10:35: EPOCH 43: training on 147342 raw words (9021 effective words) took 0.5s, 16547 effective words/s\n",
      "INFO - 23:10:36: EPOCH 44: training on 147342 raw words (9079 effective words) took 0.4s, 21942 effective words/s\n",
      "INFO - 23:10:36: EPOCH 45: training on 147342 raw words (9177 effective words) took 0.5s, 19723 effective words/s\n",
      "INFO - 23:10:37: EPOCH 46: training on 147342 raw words (9005 effective words) took 0.4s, 21104 effective words/s\n",
      "INFO - 23:10:37: EPOCH 47: training on 147342 raw words (9129 effective words) took 0.5s, 19539 effective words/s\n",
      "INFO - 23:10:38: EPOCH 48: training on 147342 raw words (9057 effective words) took 0.4s, 21624 effective words/s\n",
      "INFO - 23:10:38: EPOCH 49: training on 147342 raw words (9021 effective words) took 0.4s, 20837 effective words/s\n",
      "INFO - 23:10:38: Word2Vec lifecycle event {'msg': 'training on 7367100 raw words (452188 effective words) took 24.3s, 18646 effective words/s', 'datetime': '2023-04-01T23:10:38.526708', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:10:38: collecting all words and their counts\n",
      "INFO - 23:10:38: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:10:38: collected 53459 token types (unigram + bigrams) from a corpus of 129918 words and 9170 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-03-04 completed\n",
      "time taken: 0.43 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:10:38: merged Phrases<53459 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:38: Phrases lifecycle event {'msg': 'built Phrases<53459 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.20s', 'datetime': '2023-04-01T23:10:38.740136', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:38: exporting phrases from Phrases<53459 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:38: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<684 phrases, min_count=5, threshold=7> from Phrases<53459 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.13s', 'datetime': '2023-04-01T23:10:38.872019', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:38: collecting all words and their counts\n",
      "INFO - 23:10:38: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:10:39: collected 54745 token types (unigram + bigrams) from a corpus of 121750 words and 9170 sentences\n",
      "INFO - 23:10:39: merged Phrases<54745 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:39: Phrases lifecycle event {'msg': 'built Phrases<54745 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.33s', 'datetime': '2023-04-01T23:10:39.200105', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:39: exporting phrases from Phrases<54745 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:39: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<686 phrases, min_count=5, threshold=7> from Phrases<54745 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.11s', 'datetime': '2023-04-01T23:10:39.316215', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:39: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:10:39.317213', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:39: collecting all words and their counts\n",
      "INFO - 23:10:39: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:10:39: collected 11204 word types from a corpus of 119794 raw words and 9170 sentences\n",
      "INFO - 23:10:39: Creating a fresh vocabulary\n",
      "INFO - 23:10:39: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 847 unique words (7.56% of original 11204, drops 10357)', 'datetime': '2023-04-01T23:10:39.723132', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:10:39: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 76158 word corpus (63.57% of original 119794, drops 43636)', 'datetime': '2023-04-01T23:10:39.724130', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:10:39: deleting the raw counts dictionary of 11204 items\n",
      "INFO - 23:10:39: sample=1e-05 downsamples 847 most-common words\n",
      "INFO - 23:10:39: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 6309.385736949723 word corpus (8.3%% of prior 76158)', 'datetime': '2023-04-01T23:10:39.740087', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:10:39: constructing a huffman tree from 847 words\n",
      "INFO - 23:10:39: built huffman tree with maximum node depth 12\n",
      "INFO - 23:10:39: estimated required memory for 847 words and 100 dimensions: 1609300 bytes\n",
      "INFO - 23:10:39: resetting layer weights\n",
      "INFO - 23:10:39: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:10:39.810219', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:10:39: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 847 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:10:39.811218', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:10:40: EPOCH 0: training on 119794 raw words (6313 effective words) took 0.4s, 16600 effective words/s\n",
      "INFO - 23:10:40: EPOCH 1: training on 119794 raw words (6217 effective words) took 0.6s, 10635 effective words/s\n",
      "INFO - 23:10:41: EPOCH 2: training on 119794 raw words (6350 effective words) took 0.4s, 14268 effective words/s\n",
      "INFO - 23:10:41: EPOCH 3: training on 119794 raw words (6249 effective words) took 0.4s, 16558 effective words/s\n",
      "INFO - 23:10:41: EPOCH 4: training on 119794 raw words (6133 effective words) took 0.3s, 18196 effective words/s\n",
      "INFO - 23:10:42: EPOCH 5: training on 119794 raw words (6338 effective words) took 0.3s, 18737 effective words/s\n",
      "INFO - 23:10:42: EPOCH 6: training on 119794 raw words (6330 effective words) took 0.4s, 18036 effective words/s\n",
      "INFO - 23:10:43: EPOCH 7: training on 119794 raw words (6344 effective words) took 0.3s, 18726 effective words/s\n",
      "INFO - 23:10:43: EPOCH 8: training on 119794 raw words (6283 effective words) took 0.4s, 17291 effective words/s\n",
      "INFO - 23:10:43: EPOCH 9: training on 119794 raw words (6352 effective words) took 0.4s, 17592 effective words/s\n",
      "INFO - 23:10:44: EPOCH 10: training on 119794 raw words (6257 effective words) took 0.4s, 15276 effective words/s\n",
      "INFO - 23:10:44: EPOCH 11: training on 119794 raw words (6341 effective words) took 0.4s, 14143 effective words/s\n",
      "INFO - 23:10:45: EPOCH 12: training on 119794 raw words (6251 effective words) took 0.6s, 11095 effective words/s\n",
      "INFO - 23:10:45: EPOCH 13: training on 119794 raw words (6261 effective words) took 0.6s, 10976 effective words/s\n",
      "INFO - 23:10:46: EPOCH 14: training on 119794 raw words (6288 effective words) took 0.5s, 12538 effective words/s\n",
      "INFO - 23:10:46: EPOCH 15: training on 119794 raw words (6407 effective words) took 0.3s, 19168 effective words/s\n",
      "INFO - 23:10:46: EPOCH 16: training on 119794 raw words (6357 effective words) took 0.3s, 18936 effective words/s\n",
      "INFO - 23:10:47: EPOCH 17: training on 119794 raw words (6334 effective words) took 0.4s, 16649 effective words/s\n",
      "INFO - 23:10:47: EPOCH 18: training on 119794 raw words (6274 effective words) took 0.3s, 18704 effective words/s\n",
      "INFO - 23:10:48: EPOCH 19: training on 119794 raw words (6330 effective words) took 0.3s, 18578 effective words/s\n",
      "INFO - 23:10:48: EPOCH 20: training on 119794 raw words (6269 effective words) took 0.3s, 18546 effective words/s\n",
      "INFO - 23:10:48: EPOCH 21: training on 119794 raw words (6211 effective words) took 0.3s, 17913 effective words/s\n",
      "INFO - 23:10:49: EPOCH 22: training on 119794 raw words (6187 effective words) took 0.3s, 17993 effective words/s\n",
      "INFO - 23:10:49: EPOCH 23: training on 119794 raw words (6198 effective words) took 0.3s, 18560 effective words/s\n",
      "INFO - 23:10:49: EPOCH 24: training on 119794 raw words (6376 effective words) took 0.3s, 19032 effective words/s\n",
      "INFO - 23:10:50: EPOCH 25: training on 119794 raw words (6301 effective words) took 0.3s, 19293 effective words/s\n",
      "INFO - 23:10:50: EPOCH 26: training on 119794 raw words (6258 effective words) took 0.3s, 19163 effective words/s\n",
      "INFO - 23:10:50: EPOCH 27: training on 119794 raw words (6274 effective words) took 0.3s, 19296 effective words/s\n",
      "INFO - 23:10:51: EPOCH 28: training on 119794 raw words (6288 effective words) took 0.3s, 19009 effective words/s\n",
      "INFO - 23:10:51: EPOCH 29: training on 119794 raw words (6305 effective words) took 0.3s, 19023 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:10:51: EPOCH 30: training on 119794 raw words (6334 effective words) took 0.3s, 18896 effective words/s\n",
      "INFO - 23:10:52: EPOCH 31: training on 119794 raw words (6466 effective words) took 0.3s, 19630 effective words/s\n",
      "INFO - 23:10:52: EPOCH 32: training on 119794 raw words (6353 effective words) took 0.3s, 19324 effective words/s\n",
      "INFO - 23:10:52: EPOCH 33: training on 119794 raw words (6434 effective words) took 0.3s, 18633 effective words/s\n",
      "INFO - 23:10:53: EPOCH 34: training on 119794 raw words (6350 effective words) took 0.4s, 16227 effective words/s\n",
      "INFO - 23:10:53: EPOCH 35: training on 119794 raw words (6311 effective words) took 0.3s, 18785 effective words/s\n",
      "INFO - 23:10:53: EPOCH 36: training on 119794 raw words (6317 effective words) took 0.3s, 18883 effective words/s\n",
      "INFO - 23:10:54: EPOCH 37: training on 119794 raw words (6540 effective words) took 0.3s, 18893 effective words/s\n",
      "INFO - 23:10:54: EPOCH 38: training on 119794 raw words (6318 effective words) took 0.4s, 17242 effective words/s\n",
      "INFO - 23:10:54: EPOCH 39: training on 119794 raw words (6180 effective words) took 0.4s, 16267 effective words/s\n",
      "INFO - 23:10:55: EPOCH 40: training on 119794 raw words (6254 effective words) took 0.3s, 19148 effective words/s\n",
      "INFO - 23:10:55: EPOCH 41: training on 119794 raw words (6460 effective words) took 0.3s, 19560 effective words/s\n",
      "INFO - 23:10:55: EPOCH 42: training on 119794 raw words (6276 effective words) took 0.3s, 19212 effective words/s\n",
      "INFO - 23:10:56: EPOCH 43: training on 119794 raw words (6262 effective words) took 0.3s, 19115 effective words/s\n",
      "INFO - 23:10:56: EPOCH 44: training on 119794 raw words (6379 effective words) took 0.3s, 19282 effective words/s\n",
      "INFO - 23:10:56: EPOCH 45: training on 119794 raw words (6347 effective words) took 0.3s, 18675 effective words/s\n",
      "INFO - 23:10:57: EPOCH 46: training on 119794 raw words (6397 effective words) took 0.3s, 19532 effective words/s\n",
      "INFO - 23:10:57: EPOCH 47: training on 119794 raw words (6386 effective words) took 0.3s, 19208 effective words/s\n",
      "INFO - 23:10:57: EPOCH 48: training on 119794 raw words (6466 effective words) took 0.3s, 19391 effective words/s\n",
      "INFO - 23:10:58: EPOCH 49: training on 119794 raw words (6270 effective words) took 0.3s, 18580 effective words/s\n",
      "INFO - 23:10:58: Word2Vec lifecycle event {'msg': 'training on 5989700 raw words (315776 effective words) took 18.5s, 17106 effective words/s', 'datetime': '2023-04-01T23:10:58.272268', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:10:58: collecting all words and their counts\n",
      "INFO - 23:10:58: PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-03-11 completed\n",
      "time taken: 0.33 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:10:58: PROGRESS: at sentence #10000, processed 146994 words and 73918 word types\n",
      "INFO - 23:10:58: collected 73918 token types (unigram + bigrams) from a corpus of 211442 words and 14262 sentences\n",
      "INFO - 23:10:58: merged Phrases<73918 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:58: Phrases lifecycle event {'msg': 'built Phrases<73918 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.33s', 'datetime': '2023-04-01T23:10:58.611366', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:58: exporting phrases from Phrases<73918 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:58: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1055 phrases, min_count=5, threshold=7> from Phrases<73918 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.15s', 'datetime': '2023-04-01T23:10:58.765989', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:58: collecting all words and their counts\n",
      "INFO - 23:10:58: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:10:59: PROGRESS: at sentence #10000, processed 135608 words and 76914 word types\n",
      "INFO - 23:10:59: collected 76914 token types (unigram + bigrams) from a corpus of 194878 words and 14262 sentences\n",
      "INFO - 23:10:59: merged Phrases<76914 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:59: Phrases lifecycle event {'msg': 'built Phrases<76914 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.50s', 'datetime': '2023-04-01T23:10:59.265617', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:59: exporting phrases from Phrases<76914 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:10:59: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1197 phrases, min_count=5, threshold=7> from Phrases<76914 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.16s', 'datetime': '2023-04-01T23:10:59.430366', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:59: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:10:59.431325', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:10:59: collecting all words and their counts\n",
      "INFO - 23:10:59: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:10:59: PROGRESS: at sentence #10000, processed 132425 words, keeping 13569 word types\n",
      "INFO - 23:10:59: collected 13569 word types from a corpus of 190236 raw words and 14262 sentences\n",
      "INFO - 23:10:59: Creating a fresh vocabulary\n",
      "INFO - 23:10:59: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1341 unique words (9.88% of original 13569, drops 12228)', 'datetime': '2023-04-01T23:10:59.956961', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:10:59: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 136292 word corpus (71.64% of original 190236, drops 53944)', 'datetime': '2023-04-01T23:10:59.956961', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:10:59: deleting the raw counts dictionary of 13569 items\n",
      "INFO - 23:10:59: sample=1e-05 downsamples 1341 most-common words\n",
      "INFO - 23:10:59: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 14303.850723477532 word corpus (10.5%% of prior 136292)', 'datetime': '2023-04-01T23:10:59.970882', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:10:59: constructing a huffman tree from 1341 words\n",
      "INFO - 23:11:00: built huffman tree with maximum node depth 13\n",
      "INFO - 23:11:00: estimated required memory for 1341 words and 100 dimensions: 2547900 bytes\n",
      "INFO - 23:11:00: resetting layer weights\n",
      "INFO - 23:11:00: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:11:00.032720', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:11:00: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1341 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:11:00.033715', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:11:00: EPOCH 0: training on 190236 raw words (14217 effective words) took 0.5s, 26738 effective words/s\n",
      "INFO - 23:11:01: EPOCH 1: training on 190236 raw words (14105 effective words) took 0.6s, 23560 effective words/s\n",
      "INFO - 23:11:01: EPOCH 2: training on 190236 raw words (14336 effective words) took 0.5s, 26506 effective words/s\n",
      "INFO - 23:11:02: EPOCH 3: training on 190236 raw words (14177 effective words) took 0.5s, 26168 effective words/s\n",
      "INFO - 23:11:02: EPOCH 4: training on 190236 raw words (14325 effective words) took 0.5s, 26444 effective words/s\n",
      "INFO - 23:11:03: EPOCH 5: training on 190236 raw words (14246 effective words) took 0.5s, 26170 effective words/s\n",
      "INFO - 23:11:03: EPOCH 6: training on 190236 raw words (14138 effective words) took 0.5s, 25988 effective words/s\n",
      "INFO - 23:11:04: EPOCH 7: training on 190236 raw words (14434 effective words) took 0.5s, 26471 effective words/s\n",
      "INFO - 23:11:05: EPOCH 8: training on 190236 raw words (14410 effective words) took 0.6s, 23625 effective words/s\n",
      "INFO - 23:11:05: EPOCH 9: training on 190236 raw words (14290 effective words) took 0.5s, 26907 effective words/s\n",
      "INFO - 23:11:06: EPOCH 10: training on 190236 raw words (14111 effective words) took 0.6s, 23261 effective words/s\n",
      "INFO - 23:11:06: EPOCH 11: training on 190236 raw words (14342 effective words) took 0.6s, 23924 effective words/s\n",
      "INFO - 23:11:07: EPOCH 12: training on 190236 raw words (14245 effective words) took 0.5s, 26484 effective words/s\n",
      "INFO - 23:11:07: EPOCH 13: training on 190236 raw words (14446 effective words) took 0.5s, 26296 effective words/s\n",
      "INFO - 23:11:08: EPOCH 14: training on 190236 raw words (14262 effective words) took 0.6s, 25545 effective words/s\n",
      "INFO - 23:11:09: EPOCH 15: training on 190236 raw words (14296 effective words) took 0.6s, 24890 effective words/s\n",
      "INFO - 23:11:09: EPOCH 16: training on 190236 raw words (14543 effective words) took 0.5s, 26632 effective words/s\n",
      "INFO - 23:11:10: EPOCH 17: training on 190236 raw words (14087 effective words) took 0.6s, 24154 effective words/s\n",
      "INFO - 23:11:10: EPOCH 18: training on 190236 raw words (14181 effective words) took 0.5s, 26942 effective words/s\n",
      "INFO - 23:11:11: EPOCH 19: training on 190236 raw words (14231 effective words) took 0.5s, 26070 effective words/s\n",
      "INFO - 23:11:11: EPOCH 20: training on 190236 raw words (14161 effective words) took 0.5s, 26715 effective words/s\n",
      "INFO - 23:11:12: EPOCH 21: training on 190236 raw words (14158 effective words) took 0.5s, 26033 effective words/s\n",
      "INFO - 23:11:13: EPOCH 22: training on 190236 raw words (14315 effective words) took 0.7s, 21898 effective words/s\n",
      "INFO - 23:11:13: EPOCH 23: training on 190236 raw words (14225 effective words) took 0.6s, 23237 effective words/s\n",
      "INFO - 23:11:14: EPOCH 24: training on 190236 raw words (14345 effective words) took 0.8s, 17217 effective words/s\n",
      "INFO - 23:11:15: EPOCH 25 - PROGRESS: at 84.37% examples, 11333 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:11:15: EPOCH 25: training on 190236 raw words (14333 effective words) took 1.3s, 11018 effective words/s\n",
      "INFO - 23:11:16: EPOCH 26: training on 190236 raw words (14350 effective words) took 0.9s, 16566 effective words/s\n",
      "INFO - 23:11:17: EPOCH 27: training on 190236 raw words (14383 effective words) took 0.8s, 17259 effective words/s\n",
      "INFO - 23:11:18: EPOCH 28: training on 190236 raw words (14157 effective words) took 0.7s, 19693 effective words/s\n",
      "INFO - 23:11:18: EPOCH 29: training on 190236 raw words (14466 effective words) took 0.7s, 21686 effective words/s\n",
      "INFO - 23:11:19: EPOCH 30: training on 190236 raw words (14327 effective words) took 0.7s, 22008 effective words/s\n",
      "INFO - 23:11:20: EPOCH 31: training on 190236 raw words (14452 effective words) took 0.6s, 22550 effective words/s\n",
      "INFO - 23:11:20: EPOCH 32: training on 190236 raw words (14216 effective words) took 0.6s, 22444 effective words/s\n",
      "INFO - 23:11:21: EPOCH 33: training on 190236 raw words (14414 effective words) took 0.8s, 18883 effective words/s\n",
      "INFO - 23:11:22: EPOCH 34: training on 190236 raw words (14221 effective words) took 0.6s, 22220 effective words/s\n",
      "INFO - 23:11:22: EPOCH 35: training on 190236 raw words (14290 effective words) took 0.6s, 22512 effective words/s\n",
      "INFO - 23:11:23: EPOCH 36: training on 190236 raw words (14368 effective words) took 0.7s, 21509 effective words/s\n",
      "INFO - 23:11:24: EPOCH 37: training on 190236 raw words (14337 effective words) took 0.7s, 20700 effective words/s\n",
      "INFO - 23:11:24: EPOCH 38: training on 190236 raw words (14351 effective words) took 0.6s, 22815 effective words/s\n",
      "INFO - 23:11:25: EPOCH 39: training on 190236 raw words (14301 effective words) took 0.7s, 21217 effective words/s\n",
      "INFO - 23:11:26: EPOCH 40: training on 190236 raw words (14266 effective words) took 0.7s, 21718 effective words/s\n",
      "INFO - 23:11:26: EPOCH 41: training on 190236 raw words (14201 effective words) took 0.7s, 21800 effective words/s\n",
      "INFO - 23:11:27: EPOCH 42: training on 190236 raw words (14123 effective words) took 0.6s, 22307 effective words/s\n",
      "INFO - 23:11:28: EPOCH 43: training on 190236 raw words (14428 effective words) took 0.7s, 21436 effective words/s\n",
      "INFO - 23:11:28: EPOCH 44: training on 190236 raw words (14148 effective words) took 0.7s, 21635 effective words/s\n",
      "INFO - 23:11:29: EPOCH 45: training on 190236 raw words (14506 effective words) took 0.6s, 22838 effective words/s\n",
      "INFO - 23:11:30: EPOCH 46: training on 190236 raw words (14466 effective words) took 0.7s, 22037 effective words/s\n",
      "INFO - 23:11:30: EPOCH 47: training on 190236 raw words (14285 effective words) took 0.6s, 22017 effective words/s\n",
      "INFO - 23:11:31: EPOCH 48: training on 190236 raw words (14343 effective words) took 0.7s, 20734 effective words/s\n",
      "INFO - 23:11:32: EPOCH 49: training on 190236 raw words (14390 effective words) took 0.6s, 23146 effective words/s\n",
      "INFO - 23:11:32: Word2Vec lifecycle event {'msg': 'training on 9511800 raw words (714748 effective words) took 32.2s, 22219 effective words/s', 'datetime': '2023-04-01T23:11:32.203061', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:11:32: collecting all words and their counts\n",
      "INFO - 23:11:32: PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-03-18 completed\n",
      "time taken: 0.57 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:11:32: PROGRESS: at sentence #10000, processed 149021 words and 95718 word types\n",
      "INFO - 23:11:32: PROGRESS: at sentence #20000, processed 296189 words and 108147 word types\n",
      "INFO - 23:11:32: collected 108147 token types (unigram + bigrams) from a corpus of 345352 words and 23420 sentences\n",
      "INFO - 23:11:32: merged Phrases<108147 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:11:32: Phrases lifecycle event {'msg': 'built Phrases<108147 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.65s', 'datetime': '2023-04-01T23:11:32.878378', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:11:32: exporting phrases from Phrases<108147 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:11:33: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1745 phrases, min_count=5, threshold=7> from Phrases<108147 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.28s', 'datetime': '2023-04-01T23:11:33.168560', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:11:33: collecting all words and their counts\n",
      "INFO - 23:11:33: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:11:33: PROGRESS: at sentence #10000, processed 136215 words and 100690 word types\n",
      "INFO - 23:11:34: PROGRESS: at sentence #20000, processed 270611 words and 113970 word types\n",
      "INFO - 23:11:34: collected 113970 token types (unigram + bigrams) from a corpus of 315882 words and 23420 sentences\n",
      "INFO - 23:11:34: merged Phrases<113970 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:11:34: Phrases lifecycle event {'msg': 'built Phrases<113970 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 1.05s', 'datetime': '2023-04-01T23:11:34.218753', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:11:34: exporting phrases from Phrases<113970 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:11:34: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1973 phrases, min_count=5, threshold=7> from Phrases<113970 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.30s', 'datetime': '2023-04-01T23:11:34.530917', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:11:34: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:11:34.532911', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:11:34: collecting all words and their counts\n",
      "INFO - 23:11:34: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:11:34: PROGRESS: at sentence #10000, processed 132626 words, keeping 16520 word types\n",
      "INFO - 23:11:35: PROGRESS: at sentence #20000, processed 263458 words, keeping 17984 word types\n",
      "INFO - 23:11:35: collected 17984 word types from a corpus of 307622 raw words and 23420 sentences\n",
      "INFO - 23:11:35: Creating a fresh vocabulary\n",
      "INFO - 23:11:35: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 2043 unique words (11.36% of original 17984, drops 15941)', 'datetime': '2023-04-01T23:11:35.562539', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:11:35: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 235300 word corpus (76.49% of original 307622, drops 72322)', 'datetime': '2023-04-01T23:11:35.563499', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:11:35: deleting the raw counts dictionary of 17984 items\n",
      "INFO - 23:11:35: sample=1e-05 downsamples 2043 most-common words\n",
      "INFO - 23:11:35: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 30609.147764259433 word corpus (13.0%% of prior 235300)', 'datetime': '2023-04-01T23:11:35.584443', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:11:35: constructing a huffman tree from 2043 words\n",
      "INFO - 23:11:35: built huffman tree with maximum node depth 14\n",
      "INFO - 23:11:35: estimated required memory for 2043 words and 100 dimensions: 3881700 bytes\n",
      "INFO - 23:11:35: resetting layer weights\n",
      "INFO - 23:11:35: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:11:35.694150', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:11:35: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 2043 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:11:35.695150', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:11:36: EPOCH 0 - PROGRESS: at 77.86% examples, 23387 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:11:36: EPOCH 0: training on 307622 raw words (30716 effective words) took 1.1s, 29223 effective words/s\n",
      "INFO - 23:11:37: EPOCH 1 - PROGRESS: at 87.28% examples, 26624 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:11:37: EPOCH 1: training on 307622 raw words (30508 effective words) took 1.2s, 26411 effective words/s\n",
      "INFO - 23:11:38: EPOCH 2: training on 307622 raw words (30507 effective words) took 1.0s, 31045 effective words/s\n",
      "INFO - 23:11:39: EPOCH 3: training on 307622 raw words (30463 effective words) took 0.9s, 34678 effective words/s\n",
      "INFO - 23:11:40: EPOCH 4: training on 307622 raw words (30543 effective words) took 0.9s, 32941 effective words/s\n",
      "INFO - 23:11:41: EPOCH 5: training on 307622 raw words (30657 effective words) took 0.9s, 34879 effective words/s\n",
      "INFO - 23:11:42: EPOCH 6: training on 307622 raw words (30370 effective words) took 0.9s, 32259 effective words/s\n",
      "INFO - 23:11:43: EPOCH 7: training on 307622 raw words (30643 effective words) took 1.0s, 31078 effective words/s\n",
      "INFO - 23:11:44: EPOCH 8: training on 307622 raw words (30788 effective words) took 0.9s, 32853 effective words/s\n",
      "INFO - 23:11:45: EPOCH 9 - PROGRESS: at 90.61% examples, 27112 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:11:45: EPOCH 9: training on 307622 raw words (30734 effective words) took 1.1s, 28713 effective words/s\n",
      "INFO - 23:11:46: EPOCH 10: training on 307622 raw words (30646 effective words) took 1.0s, 30670 effective words/s\n",
      "INFO - 23:11:47: EPOCH 11: training on 307622 raw words (30369 effective words) took 1.0s, 30684 effective words/s\n",
      "INFO - 23:11:48: EPOCH 12: training on 307622 raw words (30558 effective words) took 0.9s, 32278 effective words/s\n",
      "INFO - 23:11:49: EPOCH 13: training on 307622 raw words (30313 effective words) took 0.9s, 33275 effective words/s\n",
      "INFO - 23:11:50: EPOCH 14: training on 307622 raw words (30543 effective words) took 0.9s, 32633 effective words/s\n",
      "INFO - 23:11:51: EPOCH 15: training on 307622 raw words (30573 effective words) took 0.9s, 33642 effective words/s\n",
      "INFO - 23:11:52: EPOCH 16: training on 307622 raw words (30663 effective words) took 1.0s, 31709 effective words/s\n",
      "INFO - 23:11:53: EPOCH 17: training on 307622 raw words (30553 effective words) took 0.9s, 33236 effective words/s\n",
      "INFO - 23:11:54: EPOCH 18: training on 307622 raw words (30489 effective words) took 1.0s, 32081 effective words/s\n",
      "INFO - 23:11:55: EPOCH 19: training on 307622 raw words (30656 effective words) took 0.9s, 34550 effective words/s\n",
      "INFO - 23:11:55: EPOCH 20: training on 307622 raw words (30829 effective words) took 0.9s, 32929 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:11:56: EPOCH 21: training on 307622 raw words (30450 effective words) took 1.0s, 31937 effective words/s\n",
      "INFO - 23:11:57: EPOCH 22: training on 307622 raw words (30385 effective words) took 0.9s, 33925 effective words/s\n",
      "INFO - 23:11:58: EPOCH 23: training on 307622 raw words (30707 effective words) took 0.9s, 33918 effective words/s\n",
      "INFO - 23:11:59: EPOCH 24: training on 307622 raw words (30745 effective words) took 0.9s, 34852 effective words/s\n",
      "INFO - 23:12:00: EPOCH 25: training on 307622 raw words (30448 effective words) took 1.0s, 31388 effective words/s\n",
      "INFO - 23:12:01: EPOCH 26: training on 307622 raw words (30562 effective words) took 0.9s, 33751 effective words/s\n",
      "INFO - 23:12:02: EPOCH 27: training on 307622 raw words (30596 effective words) took 1.0s, 32104 effective words/s\n",
      "INFO - 23:12:03: EPOCH 28: training on 307622 raw words (30708 effective words) took 0.9s, 33744 effective words/s\n",
      "INFO - 23:12:04: EPOCH 29: training on 307622 raw words (30937 effective words) took 1.0s, 31950 effective words/s\n",
      "INFO - 23:12:05: EPOCH 30: training on 307622 raw words (30741 effective words) took 0.9s, 34737 effective words/s\n",
      "INFO - 23:12:06: EPOCH 31: training on 307622 raw words (30555 effective words) took 0.9s, 34053 effective words/s\n",
      "INFO - 23:12:07: EPOCH 32: training on 307622 raw words (30430 effective words) took 1.0s, 31377 effective words/s\n",
      "INFO - 23:12:08: EPOCH 33: training on 307622 raw words (30584 effective words) took 0.9s, 32763 effective words/s\n",
      "INFO - 23:12:08: EPOCH 34: training on 307622 raw words (30852 effective words) took 0.9s, 32845 effective words/s\n",
      "INFO - 23:12:09: EPOCH 35: training on 307622 raw words (30749 effective words) took 0.9s, 34822 effective words/s\n",
      "INFO - 23:12:10: EPOCH 36: training on 307622 raw words (30723 effective words) took 0.9s, 35043 effective words/s\n",
      "INFO - 23:12:11: EPOCH 37: training on 307622 raw words (30738 effective words) took 1.0s, 32121 effective words/s\n",
      "INFO - 23:12:12: EPOCH 38: training on 307622 raw words (30774 effective words) took 0.9s, 34668 effective words/s\n",
      "INFO - 23:12:13: EPOCH 39: training on 307622 raw words (30618 effective words) took 0.9s, 33809 effective words/s\n",
      "INFO - 23:12:14: EPOCH 40: training on 307622 raw words (30483 effective words) took 0.9s, 33428 effective words/s\n",
      "INFO - 23:12:15: EPOCH 41: training on 307622 raw words (30451 effective words) took 0.9s, 33706 effective words/s\n",
      "INFO - 23:12:16: EPOCH 42: training on 307622 raw words (30442 effective words) took 0.9s, 32192 effective words/s\n",
      "INFO - 23:12:17: EPOCH 43: training on 307622 raw words (30724 effective words) took 0.9s, 34486 effective words/s\n",
      "INFO - 23:12:18: EPOCH 44: training on 307622 raw words (30566 effective words) took 0.9s, 34227 effective words/s\n",
      "INFO - 23:12:19: EPOCH 45: training on 307622 raw words (30488 effective words) took 0.9s, 33200 effective words/s\n",
      "INFO - 23:12:19: EPOCH 46: training on 307622 raw words (30601 effective words) took 0.9s, 33799 effective words/s\n",
      "INFO - 23:12:20: EPOCH 47: training on 307622 raw words (30677 effective words) took 0.9s, 32940 effective words/s\n",
      "INFO - 23:12:21: EPOCH 48: training on 307622 raw words (30703 effective words) took 0.9s, 33394 effective words/s\n",
      "INFO - 23:12:22: EPOCH 49: training on 307622 raw words (30712 effective words) took 0.9s, 32633 effective words/s\n",
      "INFO - 23:12:22: Word2Vec lifecycle event {'msg': 'training on 15381100 raw words (1530270 effective words) took 47.0s, 32542 effective words/s', 'datetime': '2023-04-01T23:12:22.720281', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:12:22: collecting all words and their counts\n",
      "INFO - 23:12:22: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:12:22: PROGRESS: at sentence #10000, processed 139939 words and 93022 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-03-25 completed\n",
      "time taken: 0.84 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:12:23: PROGRESS: at sentence #20000, processed 284489 words and 102838 word types\n",
      "INFO - 23:12:23: collected 102838 token types (unigram + bigrams) from a corpus of 315586 words and 22206 sentences\n",
      "INFO - 23:12:23: merged Phrases<102838 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:12:23: Phrases lifecycle event {'msg': 'built Phrases<102838 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.48s', 'datetime': '2023-04-01T23:12:23.221328', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:12:23: exporting phrases from Phrases<102838 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:12:23: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1640 phrases, min_count=5, threshold=7> from Phrases<102838 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.24s', 'datetime': '2023-04-01T23:12:23.463890', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:12:23: collecting all words and their counts\n",
      "INFO - 23:12:23: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:12:23: PROGRESS: at sentence #10000, processed 128819 words and 97287 word types\n",
      "INFO - 23:12:24: PROGRESS: at sentence #20000, processed 262078 words and 107656 word types\n",
      "INFO - 23:12:24: collected 107656 token types (unigram + bigrams) from a corpus of 290746 words and 22206 sentences\n",
      "INFO - 23:12:24: merged Phrases<107656 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:12:24: Phrases lifecycle event {'msg': 'built Phrases<107656 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.80s', 'datetime': '2023-04-01T23:12:24.264708', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:12:24: exporting phrases from Phrases<107656 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:12:24: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1809 phrases, min_count=5, threshold=7> from Phrases<107656 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.25s', 'datetime': '2023-04-01T23:12:24.517205', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:12:24: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:12:24.518215', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:12:24: collecting all words and their counts\n",
      "INFO - 23:12:24: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:12:24: PROGRESS: at sentence #10000, processed 125951 words, keeping 16557 word types\n",
      "INFO - 23:12:25: PROGRESS: at sentence #20000, processed 256292 words, keeping 17777 word types\n",
      "INFO - 23:12:25: collected 17777 word types from a corpus of 284330 raw words and 22206 sentences\n",
      "INFO - 23:12:25: Creating a fresh vocabulary\n",
      "INFO - 23:12:25: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1859 unique words (10.46% of original 17777, drops 15918)', 'datetime': '2023-04-01T23:12:25.333269', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:12:25: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 212938 word corpus (74.89% of original 284330, drops 71392)', 'datetime': '2023-04-01T23:12:25.335228', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:12:25: deleting the raw counts dictionary of 17777 items\n",
      "INFO - 23:12:25: sample=1e-05 downsamples 1859 most-common words\n",
      "INFO - 23:12:25: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 26171.428383515075 word corpus (12.3%% of prior 212938)', 'datetime': '2023-04-01T23:12:25.353181', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:12:25: constructing a huffman tree from 1859 words\n",
      "INFO - 23:12:25: built huffman tree with maximum node depth 13\n",
      "INFO - 23:12:25: estimated required memory for 1859 words and 100 dimensions: 3532100 bytes\n",
      "INFO - 23:12:25: resetting layer weights\n",
      "INFO - 23:12:25: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:12:25.437120', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:12:25: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1859 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:12:25.438077', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:12:26: EPOCH 0: training on 284330 raw words (26093 effective words) took 0.8s, 32250 effective words/s\n",
      "INFO - 23:12:27: EPOCH 1: training on 284330 raw words (26315 effective words) took 0.8s, 32691 effective words/s\n",
      "INFO - 23:12:27: EPOCH 2: training on 284330 raw words (26217 effective words) took 0.9s, 29622 effective words/s\n",
      "INFO - 23:12:28: EPOCH 3: training on 284330 raw words (25830 effective words) took 0.9s, 28681 effective words/s\n",
      "INFO - 23:12:29: EPOCH 4: training on 284330 raw words (26049 effective words) took 0.9s, 28468 effective words/s\n",
      "INFO - 23:12:30: EPOCH 5: training on 284330 raw words (26542 effective words) took 0.9s, 30469 effective words/s\n",
      "INFO - 23:12:31: EPOCH 6: training on 284330 raw words (26070 effective words) took 0.8s, 32066 effective words/s\n",
      "INFO - 23:12:32: EPOCH 7: training on 284330 raw words (26324 effective words) took 0.9s, 29923 effective words/s\n",
      "INFO - 23:12:33: EPOCH 8: training on 284330 raw words (26141 effective words) took 0.9s, 28506 effective words/s\n",
      "INFO - 23:12:34: EPOCH 9: training on 284330 raw words (25899 effective words) took 0.8s, 31315 effective words/s\n",
      "INFO - 23:12:34: EPOCH 10: training on 284330 raw words (25822 effective words) took 0.9s, 29995 effective words/s\n",
      "INFO - 23:12:35: EPOCH 11: training on 284330 raw words (26113 effective words) took 0.9s, 30321 effective words/s\n",
      "INFO - 23:12:36: EPOCH 12: training on 284330 raw words (26265 effective words) took 0.8s, 31968 effective words/s\n",
      "INFO - 23:12:37: EPOCH 13: training on 284330 raw words (26285 effective words) took 0.9s, 28196 effective words/s\n",
      "INFO - 23:12:38: EPOCH 14: training on 284330 raw words (26317 effective words) took 0.8s, 31543 effective words/s\n",
      "INFO - 23:12:39: EPOCH 15: training on 284330 raw words (26027 effective words) took 0.9s, 30442 effective words/s\n",
      "INFO - 23:12:40: EPOCH 16: training on 284330 raw words (25989 effective words) took 0.8s, 31321 effective words/s\n",
      "INFO - 23:12:40: EPOCH 17: training on 284330 raw words (26116 effective words) took 0.8s, 32319 effective words/s\n",
      "INFO - 23:12:41: EPOCH 18: training on 284330 raw words (26119 effective words) took 0.9s, 29459 effective words/s\n",
      "INFO - 23:12:42: EPOCH 19: training on 284330 raw words (25775 effective words) took 0.8s, 30338 effective words/s\n",
      "INFO - 23:12:43: EPOCH 20: training on 284330 raw words (26147 effective words) took 0.8s, 30886 effective words/s\n",
      "INFO - 23:12:44: EPOCH 21: training on 284330 raw words (26258 effective words) took 0.8s, 31147 effective words/s\n",
      "INFO - 23:12:45: EPOCH 22: training on 284330 raw words (26056 effective words) took 0.8s, 31376 effective words/s\n",
      "INFO - 23:12:46: EPOCH 23: training on 284330 raw words (26113 effective words) took 0.8s, 31563 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:12:46: EPOCH 24: training on 284330 raw words (26000 effective words) took 0.9s, 29644 effective words/s\n",
      "INFO - 23:12:47: EPOCH 25: training on 284330 raw words (26133 effective words) took 0.8s, 32153 effective words/s\n",
      "INFO - 23:12:48: EPOCH 26: training on 284330 raw words (26005 effective words) took 0.8s, 31034 effective words/s\n",
      "INFO - 23:12:49: EPOCH 27: training on 284330 raw words (25932 effective words) took 0.8s, 31300 effective words/s\n",
      "INFO - 23:12:50: EPOCH 28: training on 284330 raw words (26302 effective words) took 0.9s, 29590 effective words/s\n",
      "INFO - 23:12:51: EPOCH 29: training on 284330 raw words (26244 effective words) took 0.8s, 31726 effective words/s\n",
      "INFO - 23:12:52: EPOCH 30: training on 284330 raw words (25994 effective words) took 0.9s, 30244 effective words/s\n",
      "INFO - 23:12:52: EPOCH 31: training on 284330 raw words (25991 effective words) took 0.9s, 30457 effective words/s\n",
      "INFO - 23:12:53: EPOCH 32: training on 284330 raw words (26138 effective words) took 0.9s, 30139 effective words/s\n",
      "INFO - 23:12:54: EPOCH 33: training on 284330 raw words (26049 effective words) took 0.8s, 30949 effective words/s\n",
      "INFO - 23:12:55: EPOCH 34: training on 284330 raw words (26116 effective words) took 0.8s, 31428 effective words/s\n",
      "INFO - 23:12:56: EPOCH 35: training on 284330 raw words (26297 effective words) took 0.9s, 29164 effective words/s\n",
      "INFO - 23:12:57: EPOCH 36: training on 284330 raw words (26252 effective words) took 0.8s, 32126 effective words/s\n",
      "INFO - 23:12:57: EPOCH 37: training on 284330 raw words (26178 effective words) took 0.8s, 31863 effective words/s\n",
      "INFO - 23:12:58: EPOCH 38: training on 284330 raw words (26135 effective words) took 0.8s, 30912 effective words/s\n",
      "INFO - 23:12:59: EPOCH 39: training on 284330 raw words (25994 effective words) took 0.9s, 30579 effective words/s\n",
      "INFO - 23:13:00: EPOCH 40: training on 284330 raw words (26299 effective words) took 0.8s, 31415 effective words/s\n",
      "INFO - 23:13:01: EPOCH 41: training on 284330 raw words (26242 effective words) took 0.9s, 29187 effective words/s\n",
      "INFO - 23:13:02: EPOCH 42: training on 284330 raw words (25985 effective words) took 0.8s, 30917 effective words/s\n",
      "INFO - 23:13:03: EPOCH 43: training on 284330 raw words (26099 effective words) took 0.8s, 31052 effective words/s\n",
      "INFO - 23:13:03: EPOCH 44: training on 284330 raw words (25993 effective words) took 0.8s, 30767 effective words/s\n",
      "INFO - 23:13:04: EPOCH 45: training on 284330 raw words (25950 effective words) took 0.8s, 31179 effective words/s\n",
      "INFO - 23:13:05: EPOCH 46: training on 284330 raw words (26354 effective words) took 0.8s, 31874 effective words/s\n",
      "INFO - 23:13:06: EPOCH 47: training on 284330 raw words (26089 effective words) took 0.9s, 29209 effective words/s\n",
      "INFO - 23:13:07: EPOCH 48: training on 284330 raw words (26075 effective words) took 0.8s, 31466 effective words/s\n",
      "INFO - 23:13:08: EPOCH 49: training on 284330 raw words (26486 effective words) took 0.8s, 32432 effective words/s\n",
      "INFO - 23:13:08: Word2Vec lifecycle event {'msg': 'training on 14216500 raw words (1306214 effective words) took 42.7s, 30557 effective words/s', 'datetime': '2023-04-01T23:13:08.185729', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:13:08: collecting all words and their counts\n",
      "INFO - 23:13:08: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:13:08: PROGRESS: at sentence #10000, processed 143427 words and 96244 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-04-01 completed\n",
      "time taken: 0.76 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:13:08: PROGRESS: at sentence #20000, processed 286978 words and 101252 word types\n",
      "INFO - 23:13:08: collected 101252 token types (unigram + bigrams) from a corpus of 306626 words and 21278 sentences\n",
      "INFO - 23:13:08: merged Phrases<101252 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:13:08: Phrases lifecycle event {'msg': 'built Phrases<101252 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.45s', 'datetime': '2023-04-01T23:13:08.654477', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:08: exporting phrases from Phrases<101252 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:13:08: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1598 phrases, min_count=5, threshold=7> from Phrases<101252 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.23s', 'datetime': '2023-04-01T23:13:08.884896', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:08: collecting all words and their counts\n",
      "INFO - 23:13:08: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:13:09: PROGRESS: at sentence #10000, processed 132031 words and 100352 word types\n",
      "INFO - 23:13:09: PROGRESS: at sentence #20000, processed 264165 words and 105705 word types\n",
      "INFO - 23:13:09: collected 105705 token types (unigram + bigrams) from a corpus of 282208 words and 21278 sentences\n",
      "INFO - 23:13:09: merged Phrases<105705 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:13:09: Phrases lifecycle event {'msg': 'built Phrases<105705 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.76s', 'datetime': '2023-04-01T23:13:09.647821', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:09: exporting phrases from Phrases<105705 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:13:09: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1787 phrases, min_count=5, threshold=7> from Phrases<105705 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.23s', 'datetime': '2023-04-01T23:13:09.882193', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:09: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:13:09.883223', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:09: collecting all words and their counts\n",
      "INFO - 23:13:09: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:13:10: PROGRESS: at sentence #10000, processed 128724 words, keeping 17302 word types\n",
      "INFO - 23:13:10: PROGRESS: at sentence #20000, processed 257521 words, keeping 17860 word types\n",
      "INFO - 23:13:10: collected 17860 word types from a corpus of 275106 raw words and 21278 sentences\n",
      "INFO - 23:13:10: Creating a fresh vocabulary\n",
      "INFO - 23:13:10: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1878 unique words (10.52% of original 17860, drops 15982)', 'datetime': '2023-04-01T23:13:10.643157', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:13:10: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 204634 word corpus (74.38% of original 275106, drops 70472)', 'datetime': '2023-04-01T23:13:10.643157', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:13:10: deleting the raw counts dictionary of 17860 items\n",
      "INFO - 23:13:10: sample=1e-05 downsamples 1878 most-common words\n",
      "INFO - 23:13:10: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 25413.617386632097 word corpus (12.4%% of prior 204634)', 'datetime': '2023-04-01T23:13:10.661110', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:13:10: constructing a huffman tree from 1878 words\n",
      "INFO - 23:13:10: built huffman tree with maximum node depth 13\n",
      "INFO - 23:13:10: estimated required memory for 1878 words and 100 dimensions: 3568200 bytes\n",
      "INFO - 23:13:10: resetting layer weights\n",
      "INFO - 23:13:10: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:13:10.749872', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:13:10: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1878 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:13:10.751868', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:13:11: EPOCH 0: training on 275106 raw words (25261 effective words) took 0.8s, 32490 effective words/s\n",
      "INFO - 23:13:12: EPOCH 1: training on 275106 raw words (25499 effective words) took 0.8s, 32326 effective words/s\n",
      "INFO - 23:13:13: EPOCH 2: training on 275106 raw words (25450 effective words) took 0.8s, 30613 effective words/s\n",
      "INFO - 23:13:14: EPOCH 3: training on 275106 raw words (25303 effective words) took 0.9s, 29428 effective words/s\n",
      "INFO - 23:13:14: EPOCH 4: training on 275106 raw words (25569 effective words) took 0.8s, 31820 effective words/s\n",
      "INFO - 23:13:15: EPOCH 5: training on 275106 raw words (25390 effective words) took 0.8s, 31219 effective words/s\n",
      "INFO - 23:13:16: EPOCH 6: training on 275106 raw words (25365 effective words) took 0.8s, 32233 effective words/s\n",
      "INFO - 23:13:17: EPOCH 7: training on 275106 raw words (25623 effective words) took 0.8s, 32259 effective words/s\n",
      "INFO - 23:13:18: EPOCH 8: training on 275106 raw words (25445 effective words) took 0.9s, 29852 effective words/s\n",
      "INFO - 23:13:18: EPOCH 9: training on 275106 raw words (25476 effective words) took 0.9s, 28988 effective words/s\n",
      "INFO - 23:13:19: EPOCH 10: training on 275106 raw words (25601 effective words) took 0.8s, 31585 effective words/s\n",
      "INFO - 23:13:20: EPOCH 11: training on 275106 raw words (25490 effective words) took 0.8s, 32274 effective words/s\n",
      "INFO - 23:13:21: EPOCH 12: training on 275106 raw words (25477 effective words) took 0.8s, 32030 effective words/s\n",
      "INFO - 23:13:22: EPOCH 13: training on 275106 raw words (25607 effective words) took 0.8s, 31976 effective words/s\n",
      "INFO - 23:13:23: EPOCH 14: training on 275106 raw words (25354 effective words) took 0.9s, 28968 effective words/s\n",
      "INFO - 23:13:23: EPOCH 15: training on 275106 raw words (25258 effective words) took 0.8s, 31193 effective words/s\n",
      "INFO - 23:13:24: EPOCH 16: training on 275106 raw words (25463 effective words) took 0.8s, 32546 effective words/s\n",
      "INFO - 23:13:25: EPOCH 17: training on 275106 raw words (25726 effective words) took 0.8s, 32888 effective words/s\n",
      "INFO - 23:13:26: EPOCH 18: training on 275106 raw words (25499 effective words) took 0.8s, 32401 effective words/s\n",
      "INFO - 23:13:27: EPOCH 19: training on 275106 raw words (25352 effective words) took 0.8s, 32054 effective words/s\n",
      "INFO - 23:13:27: EPOCH 20: training on 275106 raw words (25425 effective words) took 0.9s, 29585 effective words/s\n",
      "INFO - 23:13:28: EPOCH 21: training on 275106 raw words (25292 effective words) took 0.8s, 30405 effective words/s\n",
      "INFO - 23:13:29: EPOCH 22: training on 275106 raw words (25521 effective words) took 0.8s, 31403 effective words/s\n",
      "INFO - 23:13:30: EPOCH 23: training on 275106 raw words (25646 effective words) took 0.8s, 32568 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:13:31: EPOCH 24: training on 275106 raw words (25474 effective words) took 0.8s, 31542 effective words/s\n",
      "INFO - 23:13:31: EPOCH 25: training on 275106 raw words (25383 effective words) took 0.8s, 31764 effective words/s\n",
      "INFO - 23:13:32: EPOCH 26: training on 275106 raw words (25458 effective words) took 0.9s, 29847 effective words/s\n",
      "INFO - 23:13:33: EPOCH 27: training on 275106 raw words (25615 effective words) took 0.8s, 32167 effective words/s\n",
      "INFO - 23:13:34: EPOCH 28: training on 275106 raw words (25575 effective words) took 0.8s, 31402 effective words/s\n",
      "INFO - 23:13:35: EPOCH 29: training on 275106 raw words (25216 effective words) took 0.8s, 30812 effective words/s\n",
      "INFO - 23:13:36: EPOCH 30: training on 275106 raw words (25471 effective words) took 0.8s, 31431 effective words/s\n",
      "INFO - 23:13:36: EPOCH 31: training on 275106 raw words (25467 effective words) took 0.8s, 31643 effective words/s\n",
      "INFO - 23:13:37: EPOCH 32: training on 275106 raw words (25552 effective words) took 0.8s, 30201 effective words/s\n",
      "INFO - 23:13:38: EPOCH 33: training on 275106 raw words (25142 effective words) took 0.8s, 30435 effective words/s\n",
      "INFO - 23:13:39: EPOCH 34: training on 275106 raw words (25499 effective words) took 0.8s, 30496 effective words/s\n",
      "INFO - 23:13:40: EPOCH 35: training on 275106 raw words (25460 effective words) took 0.8s, 32539 effective words/s\n",
      "INFO - 23:13:41: EPOCH 36: training on 275106 raw words (25642 effective words) took 0.8s, 32315 effective words/s\n",
      "INFO - 23:13:41: EPOCH 37: training on 275106 raw words (25359 effective words) took 0.8s, 31958 effective words/s\n",
      "INFO - 23:13:42: EPOCH 38: training on 275106 raw words (25264 effective words) took 0.9s, 29257 effective words/s\n",
      "INFO - 23:13:43: EPOCH 39: training on 275106 raw words (25340 effective words) took 0.8s, 31154 effective words/s\n",
      "INFO - 23:13:44: EPOCH 40: training on 275106 raw words (25486 effective words) took 0.8s, 32139 effective words/s\n",
      "INFO - 23:13:45: EPOCH 41: training on 275106 raw words (25254 effective words) took 0.8s, 30799 effective words/s\n",
      "INFO - 23:13:45: EPOCH 42: training on 275106 raw words (25529 effective words) took 0.8s, 31826 effective words/s\n",
      "INFO - 23:13:46: EPOCH 43: training on 275106 raw words (25335 effective words) took 0.8s, 31412 effective words/s\n",
      "INFO - 23:13:47: EPOCH 44: training on 275106 raw words (25459 effective words) took 0.8s, 30032 effective words/s\n",
      "INFO - 23:13:48: EPOCH 45: training on 275106 raw words (25417 effective words) took 0.9s, 28887 effective words/s\n",
      "INFO - 23:13:49: EPOCH 46: training on 275106 raw words (25426 effective words) took 0.9s, 29501 effective words/s\n",
      "INFO - 23:13:50: EPOCH 47: training on 275106 raw words (25361 effective words) took 0.8s, 32228 effective words/s\n",
      "INFO - 23:13:50: EPOCH 48: training on 275106 raw words (25188 effective words) took 0.8s, 31197 effective words/s\n",
      "INFO - 23:13:51: EPOCH 49: training on 275106 raw words (25546 effective words) took 0.8s, 31409 effective words/s\n",
      "INFO - 23:13:51: Word2Vec lifecycle event {'msg': 'training on 13755300 raw words (1272010 effective words) took 41.0s, 31024 effective words/s', 'datetime': '2023-04-01T23:13:51.752835', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:13:51: collecting all words and their counts\n",
      "INFO - 23:13:51: PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-04-08 completed\n",
      "time taken: 0.73 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:13:52: PROGRESS: at sentence #10000, processed 146346 words and 98756 word types\n",
      "INFO - 23:13:52: PROGRESS: at sentence #20000, processed 293098 words and 106740 word types\n",
      "INFO - 23:13:52: collected 106740 token types (unigram + bigrams) from a corpus of 323270 words and 21982 sentences\n",
      "INFO - 23:13:52: merged Phrases<106740 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:13:52: Phrases lifecycle event {'msg': 'built Phrases<106740 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.48s', 'datetime': '2023-04-01T23:13:52.298904', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:52: exporting phrases from Phrases<106740 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:13:52: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1805 phrases, min_count=5, threshold=7> from Phrases<106740 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.23s', 'datetime': '2023-04-01T23:13:52.532497', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:52: collecting all words and their counts\n",
      "INFO - 23:13:52: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:13:52: PROGRESS: at sentence #10000, processed 134342 words and 102989 word types\n",
      "INFO - 23:13:53: PROGRESS: at sentence #20000, processed 269177 words and 111495 word types\n",
      "INFO - 23:13:53: collected 111495 token types (unigram + bigrams) from a corpus of 296850 words and 21982 sentences\n",
      "INFO - 23:13:53: merged Phrases<111495 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:13:53: Phrases lifecycle event {'msg': 'built Phrases<111495 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.79s', 'datetime': '2023-04-01T23:13:53.328339', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:53: exporting phrases from Phrases<111495 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:13:53: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2000 phrases, min_count=5, threshold=7> from Phrases<111495 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.25s', 'datetime': '2023-04-01T23:13:53.578677', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:53: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:13:53.579637', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:13:53: collecting all words and their counts\n",
      "INFO - 23:13:53: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:13:53: PROGRESS: at sentence #10000, processed 130499 words, keeping 17665 word types\n",
      "INFO - 23:13:54: PROGRESS: at sentence #20000, processed 261562 words, keeping 18566 word types\n",
      "INFO - 23:13:54: collected 18566 word types from a corpus of 288474 raw words and 21982 sentences\n",
      "INFO - 23:13:54: Creating a fresh vocabulary\n",
      "INFO - 23:13:54: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1972 unique words (10.62% of original 18566, drops 16594)', 'datetime': '2023-04-01T23:13:54.403434', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:13:54: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 213836 word corpus (74.13% of original 288474, drops 74638)', 'datetime': '2023-04-01T23:13:54.404431', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:13:54: deleting the raw counts dictionary of 18566 items\n",
      "INFO - 23:13:54: sample=1e-05 downsamples 1972 most-common words\n",
      "INFO - 23:13:54: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 27408.632911392513 word corpus (12.8%% of prior 213836)', 'datetime': '2023-04-01T23:13:54.423380', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:13:54: constructing a huffman tree from 1972 words\n",
      "INFO - 23:13:54: built huffman tree with maximum node depth 13\n",
      "INFO - 23:13:54: estimated required memory for 1972 words and 100 dimensions: 3746800 bytes\n",
      "INFO - 23:13:54: resetting layer weights\n",
      "INFO - 23:13:54: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:13:54.516132', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:13:54: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1972 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:13:54.516132', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:13:55: EPOCH 0: training on 288474 raw words (27345 effective words) took 0.8s, 32254 effective words/s\n",
      "INFO - 23:13:56: EPOCH 1: training on 288474 raw words (27433 effective words) took 0.8s, 32496 effective words/s\n",
      "INFO - 23:13:57: EPOCH 2: training on 288474 raw words (27380 effective words) took 0.9s, 31794 effective words/s\n",
      "INFO - 23:13:57: EPOCH 3: training on 288474 raw words (27362 effective words) took 0.9s, 30018 effective words/s\n",
      "INFO - 23:13:58: EPOCH 4: training on 288474 raw words (27538 effective words) took 0.9s, 31369 effective words/s\n",
      "INFO - 23:13:59: EPOCH 5: training on 288474 raw words (27518 effective words) took 0.9s, 30894 effective words/s\n",
      "INFO - 23:14:00: EPOCH 6: training on 288474 raw words (27348 effective words) took 0.8s, 32490 effective words/s\n",
      "INFO - 23:14:01: EPOCH 7: training on 288474 raw words (27583 effective words) took 0.8s, 32770 effective words/s\n",
      "INFO - 23:14:02: EPOCH 8: training on 288474 raw words (27241 effective words) took 0.8s, 32385 effective words/s\n",
      "INFO - 23:14:03: EPOCH 9: training on 288474 raw words (27248 effective words) took 0.8s, 32536 effective words/s\n",
      "INFO - 23:14:04: EPOCH 10: training on 288474 raw words (27337 effective words) took 0.9s, 29925 effective words/s\n",
      "INFO - 23:14:04: EPOCH 11: training on 288474 raw words (27531 effective words) took 0.8s, 32458 effective words/s\n",
      "INFO - 23:14:05: EPOCH 12: training on 288474 raw words (27132 effective words) took 0.8s, 32803 effective words/s\n",
      "INFO - 23:14:06: EPOCH 13: training on 288474 raw words (27175 effective words) took 0.8s, 32640 effective words/s\n",
      "INFO - 23:14:07: EPOCH 14: training on 288474 raw words (27406 effective words) took 0.8s, 32812 effective words/s\n",
      "INFO - 23:14:08: EPOCH 15: training on 288474 raw words (27527 effective words) took 0.9s, 30004 effective words/s\n",
      "INFO - 23:14:09: EPOCH 16: training on 288474 raw words (27335 effective words) took 0.9s, 31236 effective words/s\n",
      "INFO - 23:14:10: EPOCH 17: training on 288474 raw words (27526 effective words) took 0.8s, 33132 effective words/s\n",
      "INFO - 23:14:10: EPOCH 18: training on 288474 raw words (27381 effective words) took 0.8s, 33011 effective words/s\n",
      "INFO - 23:14:11: EPOCH 19: training on 288474 raw words (27392 effective words) took 0.9s, 31435 effective words/s\n",
      "INFO - 23:14:12: EPOCH 20: training on 288474 raw words (27649 effective words) took 0.8s, 32968 effective words/s\n",
      "INFO - 23:14:13: EPOCH 21: training on 288474 raw words (27344 effective words) took 1.0s, 28400 effective words/s\n",
      "INFO - 23:14:14: EPOCH 22: training on 288474 raw words (27352 effective words) took 0.9s, 31010 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:14:15: EPOCH 23: training on 288474 raw words (27600 effective words) took 0.9s, 30582 effective words/s\n",
      "INFO - 23:14:16: EPOCH 24: training on 288474 raw words (27417 effective words) took 0.9s, 30984 effective words/s\n",
      "INFO - 23:14:17: EPOCH 25: training on 288474 raw words (27423 effective words) took 0.8s, 33129 effective words/s\n",
      "INFO - 23:14:18: EPOCH 26: training on 288474 raw words (27297 effective words) took 0.9s, 30484 effective words/s\n",
      "INFO - 23:14:18: EPOCH 27: training on 288474 raw words (27299 effective words) took 0.9s, 29798 effective words/s\n",
      "INFO - 23:14:19: EPOCH 28: training on 288474 raw words (27339 effective words) took 0.9s, 30420 effective words/s\n",
      "INFO - 23:14:20: EPOCH 29: training on 288474 raw words (27511 effective words) took 0.9s, 29567 effective words/s\n",
      "INFO - 23:14:21: EPOCH 30: training on 288474 raw words (27302 effective words) took 0.9s, 30001 effective words/s\n",
      "INFO - 23:14:22: EPOCH 31: training on 288474 raw words (27727 effective words) took 0.9s, 31932 effective words/s\n",
      "INFO - 23:14:23: EPOCH 32: training on 288474 raw words (27531 effective words) took 0.9s, 31361 effective words/s\n",
      "INFO - 23:14:24: EPOCH 33: training on 288474 raw words (27349 effective words) took 0.9s, 32088 effective words/s\n",
      "INFO - 23:14:25: EPOCH 34: training on 288474 raw words (27435 effective words) took 0.9s, 31273 effective words/s\n",
      "INFO - 23:14:26: EPOCH 35: training on 288474 raw words (27291 effective words) took 0.8s, 33043 effective words/s\n",
      "INFO - 23:14:26: EPOCH 36: training on 288474 raw words (27268 effective words) took 0.8s, 32730 effective words/s\n",
      "INFO - 23:14:27: EPOCH 37: training on 288474 raw words (27521 effective words) took 0.8s, 32889 effective words/s\n",
      "INFO - 23:14:28: EPOCH 38: training on 288474 raw words (27346 effective words) took 0.9s, 29853 effective words/s\n",
      "INFO - 23:14:29: EPOCH 39: training on 288474 raw words (27401 effective words) took 0.9s, 29020 effective words/s\n",
      "INFO - 23:14:30: EPOCH 40: training on 288474 raw words (27366 effective words) took 0.9s, 30872 effective words/s\n",
      "INFO - 23:14:31: EPOCH 41: training on 288474 raw words (27370 effective words) took 0.8s, 33028 effective words/s\n",
      "INFO - 23:14:32: EPOCH 42: training on 288474 raw words (27435 effective words) took 0.8s, 33520 effective words/s\n",
      "INFO - 23:14:32: EPOCH 43: training on 288474 raw words (27509 effective words) took 0.8s, 32460 effective words/s\n",
      "INFO - 23:14:33: EPOCH 44: training on 288474 raw words (27453 effective words) took 0.9s, 32225 effective words/s\n",
      "INFO - 23:14:34: EPOCH 45: training on 288474 raw words (27120 effective words) took 0.9s, 30709 effective words/s\n",
      "INFO - 23:14:35: EPOCH 46: training on 288474 raw words (27206 effective words) took 0.9s, 30961 effective words/s\n",
      "INFO - 23:14:36: EPOCH 47: training on 288474 raw words (27340 effective words) took 0.9s, 29958 effective words/s\n",
      "INFO - 23:14:37: EPOCH 48: training on 288474 raw words (27301 effective words) took 0.9s, 31149 effective words/s\n",
      "INFO - 23:14:38: EPOCH 49: training on 288474 raw words (27416 effective words) took 0.9s, 31137 effective words/s\n",
      "INFO - 23:14:38: Word2Vec lifecycle event {'msg': 'training on 14423700 raw words (1369656 effective words) took 43.7s, 31310 effective words/s', 'datetime': '2023-04-01T23:14:38.262436', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:14:38: collecting all words and their counts\n",
      "INFO - 23:14:38: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:14:38: PROGRESS: at sentence #10000, processed 138507 words and 93853 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-04-15 completed\n",
      "time taken: 0.77 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:14:38: PROGRESS: at sentence #20000, processed 273416 words and 120989 word types\n",
      "INFO - 23:14:38: collected 120989 token types (unigram + bigrams) from a corpus of 380454 words and 28024 sentences\n",
      "INFO - 23:14:38: merged Phrases<120989 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:14:38: Phrases lifecycle event {'msg': 'built Phrases<120989 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.59s', 'datetime': '2023-04-01T23:14:38.875144', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:14:38: exporting phrases from Phrases<120989 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:14:39: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2081 phrases, min_count=5, threshold=7> from Phrases<120989 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.26s', 'datetime': '2023-04-01T23:14:39.147376', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:14:39: collecting all words and their counts\n",
      "INFO - 23:14:39: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:14:39: PROGRESS: at sentence #10000, processed 126655 words and 97870 word types\n",
      "INFO - 23:14:39: PROGRESS: at sentence #20000, processed 249994 words and 126760 word types\n",
      "INFO - 23:14:40: collected 126760 token types (unigram + bigrams) from a corpus of 347468 words and 28024 sentences\n",
      "INFO - 23:14:40: merged Phrases<126760 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:14:40: Phrases lifecycle event {'msg': 'built Phrases<126760 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.92s', 'datetime': '2023-04-01T23:14:40.063924', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:14:40: exporting phrases from Phrases<126760 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:14:40: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2314 phrases, min_count=5, threshold=7> from Phrases<126760 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.27s', 'datetime': '2023-04-01T23:14:40.337233', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:14:40: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:14:40.339234', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:14:40: collecting all words and their counts\n",
      "INFO - 23:14:40: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:14:40: PROGRESS: at sentence #10000, processed 123023 words, keeping 17216 word types\n",
      "INFO - 23:14:40: PROGRESS: at sentence #20000, processed 242833 words, keeping 20483 word types\n",
      "INFO - 23:14:41: collected 20483 word types from a corpus of 337412 raw words and 28024 sentences\n",
      "INFO - 23:14:41: Creating a fresh vocabulary\n",
      "INFO - 23:14:41: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 2302 unique words (11.24% of original 20483, drops 18181)', 'datetime': '2023-04-01T23:14:41.266744', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:14:41: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 255472 word corpus (75.72% of original 337412, drops 81940)', 'datetime': '2023-04-01T23:14:41.267704', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:14:41: deleting the raw counts dictionary of 20483 items\n",
      "INFO - 23:14:41: sample=1e-05 downsamples 2302 most-common words\n",
      "INFO - 23:14:41: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35666.284395053604 word corpus (14.0%% of prior 255472)', 'datetime': '2023-04-01T23:14:41.288649', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:14:41: constructing a huffman tree from 2302 words\n",
      "INFO - 23:14:41: built huffman tree with maximum node depth 14\n",
      "INFO - 23:14:41: estimated required memory for 2302 words and 100 dimensions: 4373800 bytes\n",
      "INFO - 23:14:41: resetting layer weights\n",
      "INFO - 23:14:41: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:14:41.393369', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:14:41: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 2302 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:14:41.394366', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:14:42: EPOCH 0: training on 337412 raw words (35638 effective words) took 1.0s, 36752 effective words/s\n",
      "INFO - 23:14:43: EPOCH 1 - PROGRESS: at 85.02% examples, 30477 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:14:43: EPOCH 1: training on 337412 raw words (35691 effective words) took 1.1s, 33449 effective words/s\n",
      "INFO - 23:14:44: EPOCH 2 - PROGRESS: at 73.31% examples, 25689 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:14:44: EPOCH 2: training on 337412 raw words (35907 effective words) took 1.1s, 34067 effective words/s\n",
      "INFO - 23:14:45: EPOCH 3: training on 337412 raw words (35590 effective words) took 1.0s, 36345 effective words/s\n",
      "INFO - 23:14:46: EPOCH 4: training on 337412 raw words (35836 effective words) took 1.0s, 36736 effective words/s\n",
      "INFO - 23:14:47: EPOCH 5 - PROGRESS: at 85.02% examples, 30170 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:14:47: EPOCH 5: training on 337412 raw words (35508 effective words) took 1.0s, 34711 effective words/s\n",
      "INFO - 23:14:48: EPOCH 6 - PROGRESS: at 85.02% examples, 29814 words/s, in_qsize 2, out_qsize 1\n",
      "INFO - 23:14:48: EPOCH 6: training on 337412 raw words (35591 effective words) took 1.1s, 32994 effective words/s\n",
      "INFO - 23:14:49: EPOCH 7 - PROGRESS: at 97.48% examples, 34839 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:14:49: EPOCH 7: training on 337412 raw words (35811 effective words) took 1.0s, 35505 effective words/s\n",
      "INFO - 23:14:50: EPOCH 8 - PROGRESS: at 94.63% examples, 33607 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:14:50: EPOCH 8: training on 337412 raw words (35605 effective words) took 1.0s, 33985 effective words/s\n",
      "INFO - 23:14:51: EPOCH 9 - PROGRESS: at 49.84% examples, 17891 words/s, in_qsize 10, out_qsize 6\n",
      "INFO - 23:14:51: EPOCH 9: training on 337412 raw words (35827 effective words) took 1.0s, 34883 effective words/s\n",
      "INFO - 23:14:52: EPOCH 10 - PROGRESS: at 73.31% examples, 25756 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:14:52: EPOCH 10: training on 337412 raw words (35676 effective words) took 1.1s, 33924 effective words/s\n",
      "INFO - 23:14:53: EPOCH 11: training on 337412 raw words (35724 effective words) took 1.0s, 35841 effective words/s\n",
      "INFO - 23:14:54: EPOCH 12: training on 337412 raw words (35723 effective words) took 1.0s, 36435 effective words/s\n",
      "INFO - 23:14:55: EPOCH 13: training on 337412 raw words (35584 effective words) took 1.0s, 36278 effective words/s\n",
      "INFO - 23:14:56: EPOCH 14: training on 337412 raw words (35804 effective words) took 1.0s, 36469 effective words/s\n",
      "INFO - 23:14:57: EPOCH 15 - PROGRESS: at 43.64% examples, 15377 words/s, in_qsize 11, out_qsize 1\n",
      "INFO - 23:14:57: EPOCH 15: training on 337412 raw words (35575 effective words) took 1.1s, 32757 effective words/s\n",
      "INFO - 23:14:58: EPOCH 16 - PROGRESS: at 97.15% examples, 34543 words/s, in_qsize 1, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:14:58: EPOCH 16: training on 337412 raw words (35595 effective words) took 1.0s, 35525 effective words/s\n",
      "INFO - 23:14:59: EPOCH 17 - PROGRESS: at 67.73% examples, 24223 words/s, in_qsize 6, out_qsize 5\n",
      "INFO - 23:14:59: EPOCH 17: training on 337412 raw words (35904 effective words) took 1.0s, 34909 effective words/s\n",
      "INFO - 23:15:00: EPOCH 18: training on 337412 raw words (35727 effective words) took 1.0s, 35795 effective words/s\n",
      "INFO - 23:15:01: EPOCH 19 - PROGRESS: at 90.82% examples, 32018 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:01: EPOCH 19: training on 337412 raw words (35744 effective words) took 1.1s, 33691 effective words/s\n",
      "INFO - 23:15:02: EPOCH 20 - PROGRESS: at 82.19% examples, 29382 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 23:15:02: EPOCH 20: training on 337412 raw words (35845 effective words) took 1.0s, 35242 effective words/s\n",
      "INFO - 23:15:03: EPOCH 21 - PROGRESS: at 87.91% examples, 31022 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:03: EPOCH 21: training on 337412 raw words (35631 effective words) took 1.1s, 32673 effective words/s\n",
      "INFO - 23:15:05: EPOCH 22 - PROGRESS: at 73.31% examples, 26020 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:15:05: EPOCH 22: training on 337412 raw words (35527 effective words) took 1.0s, 34504 effective words/s\n",
      "INFO - 23:15:06: EPOCH 23 - PROGRESS: at 94.63% examples, 32642 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:06: EPOCH 23: training on 337412 raw words (35447 effective words) took 1.0s, 34215 effective words/s\n",
      "INFO - 23:15:07: EPOCH 24 - PROGRESS: at 87.91% examples, 30228 words/s, in_qsize 4, out_qsize 2\n",
      "INFO - 23:15:07: EPOCH 24: training on 337412 raw words (35494 effective words) took 1.1s, 33722 effective words/s\n",
      "INFO - 23:15:08: EPOCH 25 - PROGRESS: at 81.46% examples, 29608 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 23:15:08: EPOCH 25: training on 337412 raw words (35815 effective words) took 1.0s, 35289 effective words/s\n",
      "INFO - 23:15:09: EPOCH 26 - PROGRESS: at 85.02% examples, 29133 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:09: EPOCH 26: training on 337412 raw words (35458 effective words) took 1.1s, 33400 effective words/s\n",
      "INFO - 23:15:10: EPOCH 27 - PROGRESS: at 67.73% examples, 23148 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:15:10: EPOCH 27: training on 337412 raw words (35463 effective words) took 1.1s, 33308 effective words/s\n",
      "INFO - 23:15:11: EPOCH 28 - PROGRESS: at 87.91% examples, 31132 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:15:11: EPOCH 28: training on 337412 raw words (35718 effective words) took 1.0s, 34425 effective words/s\n",
      "INFO - 23:15:12: EPOCH 29 - PROGRESS: at 90.82% examples, 32517 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:15:12: EPOCH 29: training on 337412 raw words (35582 effective words) took 1.1s, 33095 effective words/s\n",
      "INFO - 23:15:13: EPOCH 30 - PROGRESS: at 78.63% examples, 28348 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 23:15:13: EPOCH 30: training on 337412 raw words (35638 effective words) took 1.0s, 34787 effective words/s\n",
      "INFO - 23:15:14: EPOCH 31 - PROGRESS: at 47.20% examples, 16302 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 23:15:14: EPOCH 31: training on 337412 raw words (35679 effective words) took 1.1s, 32939 effective words/s\n",
      "INFO - 23:15:15: EPOCH 32 - PROGRESS: at 90.76% examples, 32646 words/s, in_qsize 3, out_qsize 1\n",
      "INFO - 23:15:15: EPOCH 32: training on 337412 raw words (35742 effective words) took 1.0s, 35535 effective words/s\n",
      "INFO - 23:15:16: EPOCH 33 - PROGRESS: at 47.49% examples, 16810 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:15:16: EPOCH 33: training on 337412 raw words (35846 effective words) took 1.1s, 32773 effective words/s\n",
      "INFO - 23:15:17: EPOCH 34 - PROGRESS: at 70.21% examples, 25144 words/s, in_qsize 6, out_qsize 4\n",
      "INFO - 23:15:17: EPOCH 34: training on 337412 raw words (35647 effective words) took 1.0s, 34747 effective words/s\n",
      "INFO - 23:15:18: EPOCH 35 - PROGRESS: at 87.91% examples, 30571 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:18: EPOCH 35: training on 337412 raw words (35520 effective words) took 1.1s, 31628 effective words/s\n",
      "INFO - 23:15:19: EPOCH 36 - PROGRESS: at 94.63% examples, 32085 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 23:15:19: EPOCH 36: training on 337412 raw words (35562 effective words) took 1.1s, 33650 effective words/s\n",
      "INFO - 23:15:20: EPOCH 37: training on 337412 raw words (35805 effective words) took 1.0s, 36830 effective words/s\n",
      "INFO - 23:15:21: EPOCH 38 - PROGRESS: at 97.48% examples, 33699 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 23:15:21: EPOCH 38: training on 337412 raw words (35788 effective words) took 1.0s, 34472 effective words/s\n",
      "INFO - 23:15:22: EPOCH 39 - PROGRESS: at 97.48% examples, 34605 words/s, in_qsize 2, out_qsize 2\n",
      "INFO - 23:15:22: EPOCH 39: training on 337412 raw words (35803 effective words) took 1.0s, 35388 effective words/s\n",
      "INFO - 23:15:23: EPOCH 40 - PROGRESS: at 64.73% examples, 22851 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 23:15:23: EPOCH 40: training on 337412 raw words (35757 effective words) took 1.1s, 33891 effective words/s\n",
      "INFO - 23:15:24: EPOCH 41 - PROGRESS: at 87.91% examples, 31406 words/s, in_qsize 3, out_qsize 3\n",
      "INFO - 23:15:24: EPOCH 41: training on 337412 raw words (35714 effective words) took 1.0s, 35251 effective words/s\n",
      "INFO - 23:15:26: EPOCH 42 - PROGRESS: at 53.39% examples, 17968 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 23:15:26: EPOCH 42: training on 337412 raw words (35715 effective words) took 1.2s, 30282 effective words/s\n",
      "INFO - 23:15:27: EPOCH 43 - PROGRESS: at 87.91% examples, 30794 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:15:27: EPOCH 43: training on 337412 raw words (35611 effective words) took 1.1s, 33793 effective words/s\n",
      "INFO - 23:15:28: EPOCH 44 - PROGRESS: at 94.63% examples, 33641 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:15:28: EPOCH 44: training on 337412 raw words (35610 effective words) took 1.0s, 35258 effective words/s\n",
      "INFO - 23:15:29: EPOCH 45 - PROGRESS: at 85.02% examples, 30294 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:29: EPOCH 45: training on 337412 raw words (35690 effective words) took 1.1s, 33646 effective words/s\n",
      "INFO - 23:15:30: EPOCH 46 - PROGRESS: at 87.91% examples, 31334 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:15:30: EPOCH 46: training on 337412 raw words (35788 effective words) took 1.1s, 33380 effective words/s\n",
      "INFO - 23:15:31: EPOCH 47 - PROGRESS: at 55.96% examples, 19453 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 23:15:31: EPOCH 47: training on 337412 raw words (35420 effective words) took 1.1s, 32665 effective words/s\n",
      "INFO - 23:15:32: EPOCH 48 - PROGRESS: at 75.92% examples, 27324 words/s, in_qsize 7, out_qsize 1\n",
      "INFO - 23:15:32: EPOCH 48: training on 337412 raw words (35727 effective words) took 1.0s, 35060 effective words/s\n",
      "INFO - 23:15:33: EPOCH 49 - PROGRESS: at 61.90% examples, 22085 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 23:15:33: EPOCH 49: training on 337412 raw words (35689 effective words) took 1.0s, 34214 effective words/s\n",
      "INFO - 23:15:33: Word2Vec lifecycle event {'msg': 'training on 16870600 raw words (1783791 effective words) took 52.1s, 34221 effective words/s', 'datetime': '2023-04-01T23:15:33.520807', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:15:33: collecting all words and their counts\n",
      "INFO - 23:15:33: PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-04-22 completed\n",
      "time taken: 0.92 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:15:33: PROGRESS: at sentence #10000, processed 144846 words and 93055 word types\n",
      "INFO - 23:15:34: PROGRESS: at sentence #20000, processed 295104 words and 113628 word types\n",
      "INFO - 23:15:34: collected 113628 token types (unigram + bigrams) from a corpus of 382074 words and 25126 sentences\n",
      "INFO - 23:15:34: merged Phrases<113628 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:15:34: Phrases lifecycle event {'msg': 'built Phrases<113628 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.63s', 'datetime': '2023-04-01T23:15:34.164879', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:15:34: exporting phrases from Phrases<113628 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:15:34: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2097 phrases, min_count=5, threshold=7> from Phrases<113628 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.24s', 'datetime': '2023-04-01T23:15:34.414250', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:15:34: collecting all words and their counts\n",
      "INFO - 23:15:34: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:15:34: PROGRESS: at sentence #10000, processed 130671 words and 97379 word types\n",
      "INFO - 23:15:35: PROGRESS: at sentence #20000, processed 264683 words and 119280 word types\n",
      "INFO - 23:15:35: collected 119280 token types (unigram + bigrams) from a corpus of 340902 words and 25126 sentences\n",
      "INFO - 23:15:35: merged Phrases<119280 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:15:35: Phrases lifecycle event {'msg': 'built Phrases<119280 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.89s', 'datetime': '2023-04-01T23:15:35.301888', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:15:35: exporting phrases from Phrases<119280 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:15:35: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2492 phrases, min_count=5, threshold=7> from Phrases<119280 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.27s', 'datetime': '2023-04-01T23:15:35.573372', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:15:35: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:15:35.575367', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:15:35: collecting all words and their counts\n",
      "INFO - 23:15:35: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:15:35: PROGRESS: at sentence #10000, processed 125369 words, keeping 17150 word types\n",
      "INFO - 23:15:36: PROGRESS: at sentence #20000, processed 253309 words, keeping 19696 word types\n",
      "INFO - 23:15:36: collected 19696 word types from a corpus of 325184 raw words and 25126 sentences\n",
      "INFO - 23:15:36: Creating a fresh vocabulary\n",
      "INFO - 23:15:36: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 2248 unique words (11.41% of original 19696, drops 17448)', 'datetime': '2023-04-01T23:15:36.509254', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:15:36: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 246552 word corpus (75.82% of original 325184, drops 78632)', 'datetime': '2023-04-01T23:15:36.511212', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:15:36: deleting the raw counts dictionary of 19696 items\n",
      "INFO - 23:15:36: sample=1e-05 downsamples 2248 most-common words\n",
      "INFO - 23:15:36: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 33917.94509209286 word corpus (13.8%% of prior 246552)', 'datetime': '2023-04-01T23:15:36.531158', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:15:36: constructing a huffman tree from 2248 words\n",
      "INFO - 23:15:36: built huffman tree with maximum node depth 14\n",
      "INFO - 23:15:36: estimated required memory for 2248 words and 100 dimensions: 4271200 bytes\n",
      "INFO - 23:15:36: resetting layer weights\n",
      "INFO - 23:15:36: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:15:36.633887', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:15:36: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 2248 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:15:36.634881', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:15:37: EPOCH 0: training on 325184 raw words (33768 effective words) took 1.0s, 34121 effective words/s\n",
      "INFO - 23:15:38: EPOCH 1 - PROGRESS: at 65.35% examples, 21794 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 23:15:38: EPOCH 1: training on 325184 raw words (33917 effective words) took 1.0s, 32675 effective words/s\n",
      "INFO - 23:15:39: EPOCH 2: training on 325184 raw words (33991 effective words) took 1.0s, 34928 effective words/s\n",
      "INFO - 23:15:40: EPOCH 3: training on 325184 raw words (33972 effective words) took 1.0s, 34715 effective words/s\n",
      "INFO - 23:15:41: EPOCH 4: training on 325184 raw words (33724 effective words) took 1.0s, 35292 effective words/s\n",
      "INFO - 23:15:42: EPOCH 5: training on 325184 raw words (33759 effective words) took 0.9s, 35711 effective words/s\n",
      "INFO - 23:15:43: EPOCH 6 - PROGRESS: at 52.28% examples, 17707 words/s, in_qsize 7, out_qsize 1\n",
      "INFO - 23:15:43: EPOCH 6: training on 325184 raw words (34068 effective words) took 1.1s, 31819 effective words/s\n",
      "INFO - 23:15:44: EPOCH 7: training on 325184 raw words (33997 effective words) took 1.0s, 35158 effective words/s\n",
      "INFO - 23:15:45: EPOCH 8 - PROGRESS: at 90.09% examples, 29858 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 23:15:45: EPOCH 8: training on 325184 raw words (33667 effective words) took 1.0s, 33376 effective words/s\n",
      "INFO - 23:15:46: EPOCH 9 - PROGRESS: at 78.81% examples, 26116 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:15:46: EPOCH 9: training on 325184 raw words (33921 effective words) took 1.0s, 32786 effective words/s\n",
      "INFO - 23:15:47: EPOCH 10 - PROGRESS: at 62.37% examples, 20367 words/s, in_qsize 2, out_qsize 15\n",
      "INFO - 23:15:47: EPOCH 10: training on 325184 raw words (33947 effective words) took 1.0s, 32921 effective words/s\n",
      "INFO - 23:15:48: EPOCH 11 - PROGRESS: at 62.37% examples, 20889 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:15:48: EPOCH 11: training on 325184 raw words (34156 effective words) took 1.0s, 32546 effective words/s\n",
      "INFO - 23:15:49: EPOCH 12 - PROGRESS: at 84.38% examples, 28217 words/s, in_qsize 5, out_qsize 2\n",
      "INFO - 23:15:49: EPOCH 12: training on 325184 raw words (33793 effective words) took 1.0s, 33515 effective words/s\n",
      "INFO - 23:15:50: EPOCH 13: training on 325184 raw words (33521 effective words) took 1.0s, 33976 effective words/s\n",
      "INFO - 23:15:51: EPOCH 14 - PROGRESS: at 93.23% examples, 31269 words/s, in_qsize 3, out_qsize 1\n",
      "INFO - 23:15:51: EPOCH 14: training on 325184 raw words (33994 effective words) took 1.0s, 33727 effective words/s\n",
      "INFO - 23:15:52: EPOCH 15: training on 325184 raw words (33787 effective words) took 1.0s, 33867 effective words/s\n",
      "INFO - 23:15:53: EPOCH 16 - PROGRESS: at 90.26% examples, 29803 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:15:53: EPOCH 16: training on 325184 raw words (34145 effective words) took 1.1s, 30685 effective words/s\n",
      "INFO - 23:15:54: EPOCH 17 - PROGRESS: at 95.55% examples, 30683 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:15:54: EPOCH 17: training on 325184 raw words (33963 effective words) took 1.1s, 31895 effective words/s\n",
      "INFO - 23:15:55: EPOCH 18 - PROGRESS: at 87.22% examples, 29390 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:56: EPOCH 18: training on 325184 raw words (34094 effective words) took 1.2s, 29635 effective words/s\n",
      "INFO - 23:15:57: EPOCH 19 - PROGRESS: at 93.12% examples, 30789 words/s, in_qsize 3, out_qsize 3\n",
      "INFO - 23:15:57: EPOCH 19: training on 325184 raw words (33853 effective words) took 1.0s, 33274 effective words/s\n",
      "INFO - 23:15:58: EPOCH 20 - PROGRESS: at 87.22% examples, 29375 words/s, in_qsize 2, out_qsize 7\n",
      "INFO - 23:15:58: EPOCH 20: training on 325184 raw words (34030 effective words) took 1.0s, 33790 effective words/s\n",
      "INFO - 23:15:59: EPOCH 21 - PROGRESS: at 96.09% examples, 31866 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:59: EPOCH 21: training on 325184 raw words (33712 effective words) took 1.0s, 33166 effective words/s\n",
      "INFO - 23:16:00: EPOCH 22 - PROGRESS: at 93.12% examples, 29785 words/s, in_qsize 3, out_qsize 3\n",
      "INFO - 23:16:00: EPOCH 22: training on 325184 raw words (33973 effective words) took 1.1s, 32044 effective words/s\n",
      "INFO - 23:16:01: EPOCH 23 - PROGRESS: at 84.38% examples, 27669 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:01: EPOCH 23: training on 325184 raw words (33759 effective words) took 1.2s, 28032 effective words/s\n",
      "INFO - 23:16:02: EPOCH 24 - PROGRESS: at 84.38% examples, 27781 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:02: EPOCH 24: training on 325184 raw words (33872 effective words) took 1.3s, 26113 effective words/s\n",
      "INFO - 23:16:03: EPOCH 25 - PROGRESS: at 87.22% examples, 28647 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:03: EPOCH 25: training on 325184 raw words (33976 effective words) took 1.2s, 28891 effective words/s\n",
      "INFO - 23:16:04: EPOCH 26 - PROGRESS: at 75.84% examples, 23872 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:05: EPOCH 26: training on 325184 raw words (33642 effective words) took 1.3s, 24961 effective words/s\n",
      "INFO - 23:16:06: EPOCH 27 - PROGRESS: at 69.57% examples, 22761 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 23:16:06: EPOCH 27: training on 325184 raw words (33908 effective words) took 1.1s, 30029 effective words/s\n",
      "INFO - 23:16:07: EPOCH 28 - PROGRESS: at 90.26% examples, 29605 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:07: EPOCH 28: training on 325184 raw words (33834 effective words) took 1.1s, 29745 effective words/s\n",
      "INFO - 23:16:08: EPOCH 29 - PROGRESS: at 81.51% examples, 25905 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:16:08: EPOCH 29: training on 325184 raw words (33943 effective words) took 1.3s, 26939 effective words/s\n",
      "INFO - 23:16:09: EPOCH 30 - PROGRESS: at 65.35% examples, 19874 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 23:16:09: EPOCH 30: training on 325184 raw words (33988 effective words) took 1.1s, 29602 effective words/s\n",
      "INFO - 23:16:11: EPOCH 31 - PROGRESS: at 87.22% examples, 27346 words/s, in_qsize 5, out_qsize 1\n",
      "INFO - 23:16:11: EPOCH 31: training on 325184 raw words (34060 effective words) took 1.1s, 31363 effective words/s\n",
      "INFO - 23:16:12: EPOCH 32 - PROGRESS: at 90.26% examples, 29673 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:12: EPOCH 32: training on 325184 raw words (33604 effective words) took 1.1s, 31023 effective words/s\n",
      "INFO - 23:16:13: EPOCH 33 - PROGRESS: at 84.38% examples, 27639 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:13: EPOCH 33: training on 325184 raw words (33980 effective words) took 1.1s, 30468 effective words/s\n",
      "INFO - 23:16:14: EPOCH 34 - PROGRESS: at 72.90% examples, 24160 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:16:14: EPOCH 34: training on 325184 raw words (34249 effective words) took 1.0s, 33149 effective words/s\n",
      "INFO - 23:16:15: EPOCH 35 - PROGRESS: at 72.51% examples, 23152 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:16:15: EPOCH 35: training on 325184 raw words (34040 effective words) took 1.1s, 31626 effective words/s\n",
      "INFO - 23:16:16: EPOCH 36 - PROGRESS: at 98.52% examples, 32815 words/s, in_qsize 0, out_qsize 3\n",
      "INFO - 23:16:16: EPOCH 36: training on 325184 raw words (33900 effective words) took 1.0s, 33369 effective words/s\n",
      "INFO - 23:16:17: EPOCH 37: training on 325184 raw words (34020 effective words) took 1.0s, 35587 effective words/s\n",
      "INFO - 23:16:18: EPOCH 38: training on 325184 raw words (34041 effective words) took 1.0s, 34426 effective words/s\n",
      "INFO - 23:16:19: EPOCH 39: training on 325184 raw words (34166 effective words) took 1.0s, 34234 effective words/s\n",
      "INFO - 23:16:20: EPOCH 40 - PROGRESS: at 49.29% examples, 16352 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 23:16:20: EPOCH 40: training on 325184 raw words (33761 effective words) took 1.1s, 31669 effective words/s\n",
      "INFO - 23:16:21: EPOCH 41: training on 325184 raw words (33816 effective words) took 0.9s, 35846 effective words/s\n",
      "INFO - 23:16:22: EPOCH 42: training on 325184 raw words (34073 effective words) took 0.9s, 36230 effective words/s\n",
      "INFO - 23:16:23: EPOCH 43: training on 325184 raw words (33776 effective words) took 1.0s, 35472 effective words/s\n",
      "INFO - 23:16:24: EPOCH 44 - PROGRESS: at 55.26% examples, 18803 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 23:16:24: EPOCH 44: training on 325184 raw words (34142 effective words) took 1.0s, 32845 effective words/s\n",
      "INFO - 23:16:25: EPOCH 45: training on 325184 raw words (34026 effective words) took 1.0s, 35618 effective words/s\n",
      "INFO - 23:16:26: EPOCH 46 - PROGRESS: at 61.77% examples, 20676 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 23:16:26: EPOCH 46: training on 325184 raw words (33876 effective words) took 1.0s, 32412 effective words/s\n",
      "INFO - 23:16:27: EPOCH 47: training on 325184 raw words (33857 effective words) took 1.0s, 33998 effective words/s\n",
      "INFO - 23:16:28: EPOCH 48: training on 325184 raw words (34131 effective words) took 1.0s, 34968 effective words/s\n",
      "INFO - 23:16:29: EPOCH 49 - PROGRESS: at 52.28% examples, 17350 words/s, in_qsize 10, out_qsize 5\n",
      "INFO - 23:16:29: EPOCH 49: training on 325184 raw words (33898 effective words) took 1.0s, 32489 effective words/s\n",
      "INFO - 23:16:29: Word2Vec lifecycle event {'msg': 'training on 16259200 raw words (1696090 effective words) took 52.7s, 32182 effective words/s', 'datetime': '2023-04-01T23:16:29.337825', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:16:29: collecting all words and their counts\n",
      "INFO - 23:16:29: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:16:29: PROGRESS: at sentence #10000, processed 143521 words and 90373 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-04-29 completed\n",
      "time taken: 0.93 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:16:29: PROGRESS: at sentence #20000, processed 289999 words and 120192 word types\n",
      "INFO - 23:16:29: collected 120192 token types (unigram + bigrams) from a corpus of 404244 words and 28716 sentences\n",
      "INFO - 23:16:29: merged Phrases<120192 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:16:29: Phrases lifecycle event {'msg': 'built Phrases<120192 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.64s', 'datetime': '2023-04-01T23:16:29.993525', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:16:29: exporting phrases from Phrases<120192 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:16:30: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2237 phrases, min_count=5, threshold=7> from Phrases<120192 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.26s', 'datetime': '2023-04-01T23:16:30.258852', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:16:30: collecting all words and their counts\n",
      "INFO - 23:16:30: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:16:30: PROGRESS: at sentence #10000, processed 128584 words and 94917 word types\n",
      "INFO - 23:16:30: PROGRESS: at sentence #20000, processed 260392 words and 126918 word types\n",
      "INFO - 23:16:31: collected 126918 token types (unigram + bigrams) from a corpus of 363358 words and 28716 sentences\n",
      "INFO - 23:16:31: merged Phrases<126918 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:16:31: Phrases lifecycle event {'msg': 'built Phrases<126918 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.98s', 'datetime': '2023-04-01T23:16:31.243699', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:16:31: exporting phrases from Phrases<126918 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:16:31: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2657 phrases, min_count=5, threshold=7> from Phrases<126918 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.31s', 'datetime': '2023-04-01T23:16:31.556512', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:16:31: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:16:31.557470', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:16:31: collecting all words and their counts\n",
      "INFO - 23:16:31: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:16:31: PROGRESS: at sentence #10000, processed 123247 words, keeping 16799 word types\n",
      "INFO - 23:16:32: PROGRESS: at sentence #20000, processed 249984 words, keeping 20555 word types\n",
      "INFO - 23:16:32: collected 20555 word types from a corpus of 348990 raw words and 28716 sentences\n",
      "INFO - 23:16:32: Creating a fresh vocabulary\n",
      "INFO - 23:16:32: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 2381 unique words (11.58% of original 20555, drops 18174)', 'datetime': '2023-04-01T23:16:32.520894', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:16:32: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 266150 word corpus (76.26% of original 348990, drops 82840)', 'datetime': '2023-04-01T23:16:32.521892', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:16:32: deleting the raw counts dictionary of 20555 items\n",
      "INFO - 23:16:32: sample=1e-05 downsamples 2381 most-common words\n",
      "INFO - 23:16:32: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 37802.19255475775 word corpus (14.2%% of prior 266150)', 'datetime': '2023-04-01T23:16:32.541838', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:16:32: constructing a huffman tree from 2381 words\n",
      "INFO - 23:16:32: built huffman tree with maximum node depth 14\n",
      "INFO - 23:16:32: estimated required memory for 2381 words and 100 dimensions: 4523900 bytes\n",
      "INFO - 23:16:32: resetting layer weights\n",
      "INFO - 23:16:32: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:16:32.647594', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:16:32: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 2381 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:16:32.649551', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:16:33: EPOCH 0 - PROGRESS: at 89.01% examples, 33111 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 23:16:33: EPOCH 0: training on 348990 raw words (37664 effective words) took 1.1s, 35106 effective words/s\n",
      "INFO - 23:16:34: EPOCH 1 - PROGRESS: at 46.34% examples, 17124 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 23:16:34: EPOCH 1: training on 348990 raw words (37688 effective words) took 1.1s, 34753 effective words/s\n",
      "INFO - 23:16:35: EPOCH 2 - PROGRESS: at 92.05% examples, 33941 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:16:35: EPOCH 2: training on 348990 raw words (37601 effective words) took 1.0s, 36606 effective words/s\n",
      "INFO - 23:16:36: EPOCH 3 - PROGRESS: at 51.72% examples, 18754 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 23:16:36: EPOCH 3: training on 348990 raw words (37719 effective words) took 1.1s, 34914 effective words/s\n",
      "INFO - 23:16:37: EPOCH 4 - PROGRESS: at 79.12% examples, 30152 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 23:16:37: EPOCH 4: training on 348990 raw words (37537 effective words) took 1.0s, 36759 effective words/s\n",
      "INFO - 23:16:38: EPOCH 5 - PROGRESS: at 54.05% examples, 20088 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:16:39: EPOCH 5: training on 348990 raw words (37415 effective words) took 1.0s, 35801 effective words/s\n",
      "INFO - 23:16:40: EPOCH 6 - PROGRESS: at 75.60% examples, 27955 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 23:16:40: EPOCH 6: training on 348990 raw words (38026 effective words) took 1.1s, 35633 effective words/s\n",
      "INFO - 23:16:41: EPOCH 7 - PROGRESS: at 64.44% examples, 23495 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 23:16:41: EPOCH 7: training on 348990 raw words (37654 effective words) took 1.1s, 33920 effective words/s\n",
      "INFO - 23:16:42: EPOCH 8 - PROGRESS: at 51.53% examples, 18987 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:16:42: EPOCH 8: training on 348990 raw words (38057 effective words) took 1.1s, 33374 effective words/s\n",
      "INFO - 23:16:43: EPOCH 9 - PROGRESS: at 64.49% examples, 25062 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 23:16:43: EPOCH 9: training on 348990 raw words (37862 effective words) took 1.1s, 33796 effective words/s\n",
      "INFO - 23:16:44: EPOCH 10 - PROGRESS: at 69.56% examples, 26238 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:16:44: EPOCH 10: training on 348990 raw words (37992 effective words) took 1.1s, 35780 effective words/s\n",
      "INFO - 23:16:45: EPOCH 11 - PROGRESS: at 51.53% examples, 18444 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 23:16:45: EPOCH 11: training on 348990 raw words (37631 effective words) took 1.1s, 34434 effective words/s\n",
      "INFO - 23:16:46: EPOCH 12 - PROGRESS: at 82.42% examples, 30573 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:16:46: EPOCH 12: training on 348990 raw words (37740 effective words) took 1.2s, 32496 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:16:47: EPOCH 13 - PROGRESS: at 86.01% examples, 30082 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:16:47: EPOCH 13: training on 348990 raw words (37865 effective words) took 1.2s, 31649 effective words/s\n",
      "INFO - 23:16:49: EPOCH 14 - PROGRESS: at 69.56% examples, 26176 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:49: EPOCH 14: training on 348990 raw words (37823 effective words) took 1.3s, 29008 effective words/s\n",
      "INFO - 23:16:50: EPOCH 15 - PROGRESS: at 85.72% examples, 31891 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:16:50: EPOCH 15: training on 348990 raw words (37798 effective words) took 1.2s, 31965 effective words/s\n",
      "INFO - 23:16:51: EPOCH 16 - PROGRESS: at 79.12% examples, 28438 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:51: EPOCH 16: training on 348990 raw words (37823 effective words) took 1.2s, 30938 effective words/s\n",
      "INFO - 23:16:52: EPOCH 17 - PROGRESS: at 75.30% examples, 29339 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:16:52: EPOCH 17: training on 348990 raw words (38018 effective words) took 1.2s, 31984 effective words/s\n",
      "INFO - 23:16:53: EPOCH 18 - PROGRESS: at 69.75% examples, 24874 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 23:16:54: EPOCH 18: training on 348990 raw words (37430 effective words) took 1.1s, 34092 effective words/s\n",
      "INFO - 23:16:55: EPOCH 19 - PROGRESS: at 67.29% examples, 24426 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:16:55: EPOCH 19: training on 348990 raw words (37712 effective words) took 1.1s, 34622 effective words/s\n",
      "INFO - 23:16:56: EPOCH 20 - PROGRESS: at 85.42% examples, 31916 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:56: EPOCH 20: training on 348990 raw words (37646 effective words) took 1.2s, 32399 effective words/s\n",
      "INFO - 23:16:57: EPOCH 21 - PROGRESS: at 85.42% examples, 31508 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:57: EPOCH 21: training on 348990 raw words (37960 effective words) took 1.3s, 28725 effective words/s\n",
      "INFO - 23:16:58: EPOCH 22 - PROGRESS: at 59.37% examples, 22524 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:59: EPOCH 22: training on 348990 raw words (37585 effective words) took 1.7s, 22488 effective words/s\n",
      "INFO - 23:17:00: EPOCH 23 - PROGRESS: at 79.12% examples, 28212 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:17:00: EPOCH 23: training on 348990 raw words (38051 effective words) took 1.3s, 30082 effective words/s\n",
      "INFO - 23:17:01: EPOCH 24 - PROGRESS: at 89.01% examples, 32257 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:17:01: EPOCH 24: training on 348990 raw words (37866 effective words) took 1.1s, 33401 effective words/s\n",
      "INFO - 23:17:02: EPOCH 25 - PROGRESS: at 89.01% examples, 33448 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 23:17:02: EPOCH 25: training on 348990 raw words (37994 effective words) took 1.0s, 36277 effective words/s\n",
      "INFO - 23:17:03: EPOCH 26 - PROGRESS: at 64.44% examples, 24184 words/s, in_qsize 11, out_qsize 1\n",
      "INFO - 23:17:03: EPOCH 26: training on 348990 raw words (37728 effective words) took 1.1s, 35474 effective words/s\n",
      "INFO - 23:17:04: EPOCH 27 - PROGRESS: at 69.75% examples, 26209 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 23:17:04: EPOCH 27: training on 348990 raw words (37559 effective words) took 1.1s, 34378 effective words/s\n",
      "INFO - 23:17:05: EPOCH 28 - PROGRESS: at 72.27% examples, 27921 words/s, in_qsize 7, out_qsize 2\n",
      "INFO - 23:17:05: EPOCH 28: training on 348990 raw words (37626 effective words) took 1.0s, 36911 effective words/s\n",
      "INFO - 23:17:06: EPOCH 29 - PROGRESS: at 75.30% examples, 29059 words/s, in_qsize 7, out_qsize 1\n",
      "INFO - 23:17:06: EPOCH 29: training on 348990 raw words (37743 effective words) took 1.0s, 36934 effective words/s\n",
      "INFO - 23:17:07: EPOCH 30 - PROGRESS: at 46.21% examples, 17307 words/s, in_qsize 8, out_qsize 4\n",
      "INFO - 23:17:08: EPOCH 30: training on 348990 raw words (37898 effective words) took 1.1s, 35835 effective words/s\n",
      "INFO - 23:17:09: EPOCH 31 - PROGRESS: at 54.25% examples, 19267 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 23:17:09: EPOCH 31: training on 348990 raw words (37865 effective words) took 1.1s, 34130 effective words/s\n",
      "INFO - 23:17:10: EPOCH 32 - PROGRESS: at 75.30% examples, 29161 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 23:17:10: EPOCH 32: training on 348990 raw words (37816 effective words) took 1.0s, 36999 effective words/s\n",
      "INFO - 23:17:11: EPOCH 33 - PROGRESS: at 69.93% examples, 26593 words/s, in_qsize 2, out_qsize 12\n",
      "INFO - 23:17:11: EPOCH 33: training on 348990 raw words (37602 effective words) took 1.0s, 37154 effective words/s\n",
      "INFO - 23:17:12: EPOCH 34 - PROGRESS: at 92.05% examples, 33783 words/s, in_qsize 2, out_qsize 6\n",
      "INFO - 23:17:12: EPOCH 34: training on 348990 raw words (37853 effective words) took 1.0s, 36705 effective words/s\n",
      "INFO - 23:17:13: EPOCH 35 - PROGRESS: at 82.42% examples, 31163 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:17:13: EPOCH 35: training on 348990 raw words (37687 effective words) took 1.0s, 36092 effective words/s\n",
      "INFO - 23:17:14: EPOCH 36 - PROGRESS: at 48.75% examples, 17586 words/s, in_qsize 8, out_qsize 4\n",
      "INFO - 23:17:14: EPOCH 36: training on 348990 raw words (37924 effective words) took 1.1s, 34544 effective words/s\n",
      "INFO - 23:17:15: EPOCH 37 - PROGRESS: at 85.42% examples, 32491 words/s, in_qsize 5, out_qsize 1\n",
      "INFO - 23:17:15: EPOCH 37: training on 348990 raw words (37829 effective words) took 1.0s, 37384 effective words/s\n",
      "INFO - 23:17:16: EPOCH 38 - PROGRESS: at 66.71% examples, 25908 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:17:16: EPOCH 38: training on 348990 raw words (37876 effective words) took 1.0s, 36690 effective words/s\n",
      "INFO - 23:17:17: EPOCH 39 - PROGRESS: at 79.12% examples, 30563 words/s, in_qsize 5, out_qsize 3\n",
      "INFO - 23:17:17: EPOCH 39: training on 348990 raw words (37964 effective words) took 1.0s, 37442 effective words/s\n",
      "INFO - 23:17:18: EPOCH 40 - PROGRESS: at 59.37% examples, 21829 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 23:17:18: EPOCH 40: training on 348990 raw words (37939 effective words) took 1.1s, 35029 effective words/s\n",
      "INFO - 23:17:19: EPOCH 41 - PROGRESS: at 56.79% examples, 21528 words/s, in_qsize 10, out_qsize 4\n",
      "INFO - 23:17:19: EPOCH 41: training on 348990 raw words (37833 effective words) took 1.0s, 36938 effective words/s\n",
      "INFO - 23:17:20: EPOCH 42 - PROGRESS: at 69.75% examples, 27106 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 23:17:20: EPOCH 42: training on 348990 raw words (37998 effective words) took 1.0s, 37112 effective words/s\n",
      "INFO - 23:17:21: EPOCH 43 - PROGRESS: at 82.42% examples, 31637 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 23:17:21: EPOCH 43: training on 348990 raw words (37992 effective words) took 1.0s, 37229 effective words/s\n",
      "INFO - 23:17:22: EPOCH 44 - PROGRESS: at 89.01% examples, 32107 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:17:22: EPOCH 44: training on 348990 raw words (38019 effective words) took 1.1s, 35341 effective words/s\n",
      "INFO - 23:17:23: EPOCH 45 - PROGRESS: at 56.95% examples, 21533 words/s, in_qsize 9, out_qsize 2\n",
      "INFO - 23:17:23: EPOCH 45: training on 348990 raw words (37839 effective words) took 1.0s, 36189 effective words/s\n",
      "INFO - 23:17:24: EPOCH 46 - PROGRESS: at 61.65% examples, 23698 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 23:17:24: EPOCH 46: training on 348990 raw words (37728 effective words) took 1.0s, 36352 effective words/s\n",
      "INFO - 23:17:25: EPOCH 47 - PROGRESS: at 67.47% examples, 25914 words/s, in_qsize 9, out_qsize 2\n",
      "INFO - 23:17:25: EPOCH 47: training on 348990 raw words (37698 effective words) took 1.0s, 36832 effective words/s\n",
      "INFO - 23:17:26: EPOCH 48 - PROGRESS: at 92.05% examples, 32807 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:17:26: EPOCH 48: training on 348990 raw words (37715 effective words) took 1.1s, 35246 effective words/s\n",
      "INFO - 23:17:27: EPOCH 49 - PROGRESS: at 76.41% examples, 28861 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 23:17:27: EPOCH 49: training on 348990 raw words (37428 effective words) took 1.0s, 36586 effective words/s\n",
      "INFO - 23:17:27: Word2Vec lifecycle event {'msg': 'training on 17449500 raw words (1889316 effective words) took 55.2s, 34201 effective words/s', 'datetime': '2023-04-01T23:17:27.892387', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:17:27: collecting all words and their counts\n",
      "INFO - 23:17:27: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:17:28: PROGRESS: at sentence #10000, processed 145897 words and 97212 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-05-06 completed\n",
      "time taken: 0.98 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:17:28: PROGRESS: at sentence #20000, processed 294128 words and 105565 word types\n",
      "INFO - 23:17:28: collected 105565 token types (unigram + bigrams) from a corpus of 323986 words and 22326 sentences\n",
      "INFO - 23:17:28: merged Phrases<105565 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:17:28: Phrases lifecycle event {'msg': 'built Phrases<105565 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.46s', 'datetime': '2023-04-01T23:17:28.371236', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:17:28: exporting phrases from Phrases<105565 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:17:28: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1795 phrases, min_count=5, threshold=7> from Phrases<105565 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.23s', 'datetime': '2023-04-01T23:17:28.602654', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:17:28: collecting all words and their counts\n",
      "INFO - 23:17:28: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:17:28: PROGRESS: at sentence #10000, processed 133775 words and 101417 word types\n",
      "INFO - 23:17:29: PROGRESS: at sentence #20000, processed 270004 words and 110331 word types\n",
      "INFO - 23:17:29: collected 110331 token types (unigram + bigrams) from a corpus of 296930 words and 22326 sentences\n",
      "INFO - 23:17:29: merged Phrases<110331 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:17:29: Phrases lifecycle event {'msg': 'built Phrases<110331 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.78s', 'datetime': '2023-04-01T23:17:29.386520', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:17:29: exporting phrases from Phrases<110331 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:17:29: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2027 phrases, min_count=5, threshold=7> from Phrases<110331 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.24s', 'datetime': '2023-04-01T23:17:29.629062', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:17:29: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:17:29.631020', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:17:29: collecting all words and their counts\n",
      "INFO - 23:17:29: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:17:29: PROGRESS: at sentence #10000, processed 129877 words, keeping 17421 word types\n",
      "INFO - 23:17:30: PROGRESS: at sentence #20000, processed 262284 words, keeping 18381 word types\n",
      "INFO - 23:17:30: collected 18381 word types from a corpus of 288192 raw words and 22326 sentences\n",
      "INFO - 23:17:30: Creating a fresh vocabulary\n",
      "INFO - 23:17:30: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1980 unique words (10.77% of original 18381, drops 16401)', 'datetime': '2023-04-01T23:17:30.413016', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:17:30: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 214460 word corpus (74.42% of original 288192, drops 73732)', 'datetime': '2023-04-01T23:17:30.413016', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:17:30: deleting the raw counts dictionary of 18381 items\n",
      "INFO - 23:17:30: sample=1e-05 downsamples 1980 most-common words\n",
      "INFO - 23:17:30: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 27544.63462406532 word corpus (12.8%% of prior 214460)', 'datetime': '2023-04-01T23:17:30.431966', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:17:30: constructing a huffman tree from 1980 words\n",
      "INFO - 23:17:30: built huffman tree with maximum node depth 14\n",
      "INFO - 23:17:30: estimated required memory for 1980 words and 100 dimensions: 3762000 bytes\n",
      "INFO - 23:17:30: resetting layer weights\n",
      "INFO - 23:17:30: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:17:30.526714', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:17:30: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1980 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:17:30.527751', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:17:31: EPOCH 0: training on 288192 raw words (27418 effective words) took 0.8s, 33288 effective words/s\n",
      "INFO - 23:17:32: EPOCH 1: training on 288192 raw words (27528 effective words) took 0.8s, 33388 effective words/s\n",
      "INFO - 23:17:33: EPOCH 2: training on 288192 raw words (27531 effective words) took 0.8s, 33351 effective words/s\n",
      "INFO - 23:17:33: EPOCH 3: training on 288192 raw words (27249 effective words) took 0.9s, 30440 effective words/s\n",
      "INFO - 23:17:34: EPOCH 4: training on 288192 raw words (27698 effective words) took 0.8s, 33341 effective words/s\n",
      "INFO - 23:17:35: EPOCH 5: training on 288192 raw words (27365 effective words) took 0.8s, 33720 effective words/s\n",
      "INFO - 23:17:36: EPOCH 6: training on 288192 raw words (27453 effective words) took 0.8s, 33663 effective words/s\n",
      "INFO - 23:17:37: EPOCH 7: training on 288192 raw words (27417 effective words) took 0.8s, 33116 effective words/s\n",
      "INFO - 23:17:38: EPOCH 8: training on 288192 raw words (27609 effective words) took 0.8s, 33267 effective words/s\n",
      "INFO - 23:17:38: EPOCH 9: training on 288192 raw words (27725 effective words) took 0.9s, 30745 effective words/s\n",
      "INFO - 23:17:39: EPOCH 10: training on 288192 raw words (27568 effective words) took 0.8s, 33536 effective words/s\n",
      "INFO - 23:17:40: EPOCH 11: training on 288192 raw words (27380 effective words) took 0.8s, 33803 effective words/s\n",
      "INFO - 23:17:41: EPOCH 12: training on 288192 raw words (27563 effective words) took 0.8s, 33259 effective words/s\n",
      "INFO - 23:17:42: EPOCH 13: training on 288192 raw words (27645 effective words) took 0.8s, 33216 effective words/s\n",
      "INFO - 23:17:43: EPOCH 14: training on 288192 raw words (27202 effective words) took 0.8s, 32863 effective words/s\n",
      "INFO - 23:17:43: EPOCH 15: training on 288192 raw words (27407 effective words) took 0.9s, 30960 effective words/s\n",
      "INFO - 23:17:44: EPOCH 16: training on 288192 raw words (27412 effective words) took 0.8s, 33367 effective words/s\n",
      "INFO - 23:17:45: EPOCH 17: training on 288192 raw words (27432 effective words) took 0.8s, 33865 effective words/s\n",
      "INFO - 23:17:46: EPOCH 18: training on 288192 raw words (27808 effective words) took 0.8s, 34352 effective words/s\n",
      "INFO - 23:17:47: EPOCH 19: training on 288192 raw words (27646 effective words) took 0.8s, 33377 effective words/s\n",
      "INFO - 23:17:48: EPOCH 20: training on 288192 raw words (27482 effective words) took 0.9s, 30238 effective words/s\n",
      "INFO - 23:17:49: EPOCH 21: training on 288192 raw words (27557 effective words) took 0.8s, 32657 effective words/s\n",
      "INFO - 23:17:49: EPOCH 22: training on 288192 raw words (27430 effective words) took 0.8s, 33599 effective words/s\n",
      "INFO - 23:17:50: EPOCH 23: training on 288192 raw words (27517 effective words) took 0.8s, 33735 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:17:51: EPOCH 24: training on 288192 raw words (27468 effective words) took 0.8s, 33447 effective words/s\n",
      "INFO - 23:17:52: EPOCH 25: training on 288192 raw words (27596 effective words) took 0.8s, 33841 effective words/s\n",
      "INFO - 23:17:53: EPOCH 26: training on 288192 raw words (27637 effective words) took 0.9s, 30806 effective words/s\n",
      "INFO - 23:17:54: EPOCH 27: training on 288192 raw words (27774 effective words) took 0.8s, 32966 effective words/s\n",
      "INFO - 23:17:54: EPOCH 28: training on 288192 raw words (27622 effective words) took 0.8s, 32817 effective words/s\n",
      "INFO - 23:17:55: EPOCH 29: training on 288192 raw words (27805 effective words) took 0.8s, 34000 effective words/s\n",
      "INFO - 23:17:56: EPOCH 30: training on 288192 raw words (27563 effective words) took 0.8s, 33485 effective words/s\n",
      "INFO - 23:17:57: EPOCH 31: training on 288192 raw words (27700 effective words) took 0.9s, 31597 effective words/s\n",
      "INFO - 23:17:58: EPOCH 32: training on 288192 raw words (27414 effective words) took 0.8s, 32539 effective words/s\n",
      "INFO - 23:17:59: EPOCH 33: training on 288192 raw words (27650 effective words) took 0.8s, 32664 effective words/s\n",
      "INFO - 23:17:59: EPOCH 34: training on 288192 raw words (27296 effective words) took 0.8s, 33100 effective words/s\n",
      "INFO - 23:18:00: EPOCH 35: training on 288192 raw words (27585 effective words) took 0.8s, 32547 effective words/s\n",
      "INFO - 23:18:01: EPOCH 36: training on 288192 raw words (27423 effective words) took 0.9s, 31684 effective words/s\n",
      "INFO - 23:18:02: EPOCH 37: training on 288192 raw words (27538 effective words) took 0.9s, 31262 effective words/s\n",
      "INFO - 23:18:03: EPOCH 38: training on 288192 raw words (27648 effective words) took 0.8s, 32794 effective words/s\n",
      "INFO - 23:18:04: EPOCH 39: training on 288192 raw words (27481 effective words) took 0.8s, 33003 effective words/s\n",
      "INFO - 23:18:05: EPOCH 40: training on 288192 raw words (27354 effective words) took 0.8s, 33117 effective words/s\n",
      "INFO - 23:18:05: EPOCH 41: training on 288192 raw words (27715 effective words) took 0.8s, 33886 effective words/s\n",
      "INFO - 23:18:06: EPOCH 42: training on 288192 raw words (27625 effective words) took 0.9s, 31739 effective words/s\n",
      "INFO - 23:18:07: EPOCH 43: training on 288192 raw words (27543 effective words) took 0.8s, 32859 effective words/s\n",
      "INFO - 23:18:08: EPOCH 44: training on 288192 raw words (27601 effective words) took 0.8s, 32750 effective words/s\n",
      "INFO - 23:18:09: EPOCH 45: training on 288192 raw words (27435 effective words) took 0.8s, 32523 effective words/s\n",
      "INFO - 23:18:10: EPOCH 46: training on 288192 raw words (27552 effective words) took 0.8s, 33106 effective words/s\n",
      "INFO - 23:18:10: EPOCH 47: training on 288192 raw words (27563 effective words) took 0.8s, 34142 effective words/s\n",
      "INFO - 23:18:11: EPOCH 48: training on 288192 raw words (27530 effective words) took 0.8s, 33189 effective words/s\n",
      "INFO - 23:18:12: EPOCH 49: training on 288192 raw words (27667 effective words) took 0.9s, 30642 effective words/s\n",
      "INFO - 23:18:12: Word2Vec lifecycle event {'msg': 'training on 14409600 raw words (1376827 effective words) took 42.2s, 32635 effective words/s', 'datetime': '2023-04-01T23:18:12.717966', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:18:12: collecting all words and their counts\n",
      "INFO - 23:18:12: PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-05-13 completed\n",
      "time taken: 0.75 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:18:13: PROGRESS: at sentence #10000, processed 138284 words and 91481 word types\n",
      "INFO - 23:18:13: PROGRESS: at sentence #20000, processed 277421 words and 100379 word types\n",
      "INFO - 23:18:13: collected 100379 token types (unigram + bigrams) from a corpus of 307662 words and 21922 sentences\n",
      "INFO - 23:18:13: merged Phrases<100379 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:18:13: Phrases lifecycle event {'msg': 'built Phrases<100379 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.61s', 'datetime': '2023-04-01T23:18:13.339394', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:13: exporting phrases from Phrases<100379 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:18:13: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1669 phrases, min_count=5, threshold=7> from Phrases<100379 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.21s', 'datetime': '2023-04-01T23:18:13.554856', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:13: collecting all words and their counts\n",
      "INFO - 23:18:13: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:18:13: PROGRESS: at sentence #10000, processed 126028 words and 95412 word types\n",
      "INFO - 23:18:14: PROGRESS: at sentence #20000, processed 253034 words and 104731 word types\n",
      "INFO - 23:18:14: collected 104731 token types (unigram + bigrams) from a corpus of 281170 words and 21922 sentences\n",
      "INFO - 23:18:14: merged Phrases<104731 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:18:14: Phrases lifecycle event {'msg': 'built Phrases<104731 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.74s', 'datetime': '2023-04-01T23:18:14.293841', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:14: exporting phrases from Phrases<104731 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:18:14: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1895 phrases, min_count=5, threshold=7> from Phrases<104731 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.22s', 'datetime': '2023-04-01T23:18:14.524262', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:14: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:18:14.525265', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:14: collecting all words and their counts\n",
      "INFO - 23:18:14: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:18:14: PROGRESS: at sentence #10000, processed 121959 words, keeping 16791 word types\n",
      "INFO - 23:18:15: PROGRESS: at sentence #20000, processed 244993 words, keeping 17867 word types\n",
      "INFO - 23:18:15: collected 17867 word types from a corpus of 272474 raw words and 21922 sentences\n",
      "INFO - 23:18:15: Creating a fresh vocabulary\n",
      "INFO - 23:18:15: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1883 unique words (10.54% of original 17867, drops 15984)', 'datetime': '2023-04-01T23:18:15.268234', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:18:15: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 201500 word corpus (73.95% of original 272474, drops 70974)', 'datetime': '2023-04-01T23:18:15.269232', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:18:15: deleting the raw counts dictionary of 17867 items\n",
      "INFO - 23:18:15: sample=1e-05 downsamples 1883 most-common words\n",
      "INFO - 23:18:15: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 25219.77904177345 word corpus (12.5%% of prior 201500)', 'datetime': '2023-04-01T23:18:15.286188', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:18:15: constructing a huffman tree from 1883 words\n",
      "INFO - 23:18:15: built huffman tree with maximum node depth 13\n",
      "INFO - 23:18:15: estimated required memory for 1883 words and 100 dimensions: 3577700 bytes\n",
      "INFO - 23:18:15: resetting layer weights\n",
      "INFO - 23:18:15: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:18:15.371996', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:18:15: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1883 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:18:15.372957', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:18:16: EPOCH 0: training on 272474 raw words (25197 effective words) took 0.8s, 31793 effective words/s\n",
      "INFO - 23:18:16: EPOCH 1: training on 272474 raw words (25083 effective words) took 0.8s, 31966 effective words/s\n",
      "INFO - 23:18:17: EPOCH 2: training on 272474 raw words (24980 effective words) took 0.8s, 31255 effective words/s\n",
      "INFO - 23:18:18: EPOCH 3: training on 272474 raw words (25286 effective words) took 0.8s, 31512 effective words/s\n",
      "INFO - 23:18:19: EPOCH 4: training on 272474 raw words (25102 effective words) took 0.8s, 29568 effective words/s\n",
      "INFO - 23:18:20: EPOCH 5: training on 272474 raw words (25106 effective words) took 0.8s, 32034 effective words/s\n",
      "INFO - 23:18:20: EPOCH 6: training on 272474 raw words (25317 effective words) took 0.8s, 32560 effective words/s\n",
      "INFO - 23:18:21: EPOCH 7: training on 272474 raw words (24838 effective words) took 0.8s, 31859 effective words/s\n",
      "INFO - 23:18:22: EPOCH 8: training on 272474 raw words (25264 effective words) took 0.8s, 32630 effective words/s\n",
      "INFO - 23:18:23: EPOCH 9: training on 272474 raw words (25104 effective words) took 0.8s, 31729 effective words/s\n",
      "INFO - 23:18:24: EPOCH 10: training on 272474 raw words (25445 effective words) took 0.9s, 29717 effective words/s\n",
      "INFO - 23:18:24: EPOCH 11: training on 272474 raw words (25216 effective words) took 0.8s, 32049 effective words/s\n",
      "INFO - 23:18:25: EPOCH 12: training on 272474 raw words (25185 effective words) took 0.8s, 32571 effective words/s\n",
      "INFO - 23:18:26: EPOCH 13: training on 272474 raw words (24987 effective words) took 0.8s, 32131 effective words/s\n",
      "INFO - 23:18:27: EPOCH 14: training on 272474 raw words (25369 effective words) took 0.8s, 32252 effective words/s\n",
      "INFO - 23:18:28: EPOCH 15: training on 272474 raw words (25170 effective words) took 0.9s, 29354 effective words/s\n",
      "INFO - 23:18:29: EPOCH 16: training on 272474 raw words (25121 effective words) took 0.8s, 31384 effective words/s\n",
      "INFO - 23:18:29: EPOCH 17: training on 272474 raw words (25379 effective words) took 0.8s, 32318 effective words/s\n",
      "INFO - 23:18:30: EPOCH 18: training on 272474 raw words (25218 effective words) took 0.8s, 31917 effective words/s\n",
      "INFO - 23:18:31: EPOCH 19: training on 272474 raw words (25068 effective words) took 0.8s, 32274 effective words/s\n",
      "INFO - 23:18:32: EPOCH 20: training on 272474 raw words (25415 effective words) took 0.8s, 30644 effective words/s\n",
      "INFO - 23:18:33: EPOCH 21: training on 272474 raw words (25274 effective words) took 0.8s, 32173 effective words/s\n",
      "INFO - 23:18:33: EPOCH 22: training on 272474 raw words (25072 effective words) took 0.8s, 31225 effective words/s\n",
      "INFO - 23:18:34: EPOCH 23: training on 272474 raw words (25245 effective words) took 0.8s, 32001 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:18:35: EPOCH 24: training on 272474 raw words (25536 effective words) took 0.8s, 32865 effective words/s\n",
      "INFO - 23:18:36: EPOCH 25: training on 272474 raw words (25075 effective words) took 0.8s, 32301 effective words/s\n",
      "INFO - 23:18:37: EPOCH 26: training on 272474 raw words (25279 effective words) took 0.8s, 30056 effective words/s\n",
      "INFO - 23:18:37: EPOCH 27: training on 272474 raw words (25207 effective words) took 0.8s, 31562 effective words/s\n",
      "INFO - 23:18:38: EPOCH 28: training on 272474 raw words (25453 effective words) took 0.8s, 32166 effective words/s\n",
      "INFO - 23:18:39: EPOCH 29: training on 272474 raw words (25331 effective words) took 0.8s, 31941 effective words/s\n",
      "INFO - 23:18:40: EPOCH 30: training on 272474 raw words (25232 effective words) took 0.8s, 32439 effective words/s\n",
      "INFO - 23:18:40: EPOCH 31: training on 272474 raw words (24949 effective words) took 0.8s, 32080 effective words/s\n",
      "INFO - 23:18:41: EPOCH 32: training on 272474 raw words (25301 effective words) took 0.8s, 30384 effective words/s\n",
      "INFO - 23:18:42: EPOCH 33: training on 272474 raw words (25270 effective words) took 0.8s, 31937 effective words/s\n",
      "INFO - 23:18:43: EPOCH 34: training on 272474 raw words (25213 effective words) took 0.8s, 31440 effective words/s\n",
      "INFO - 23:18:44: EPOCH 35: training on 272474 raw words (25162 effective words) took 0.8s, 31121 effective words/s\n",
      "INFO - 23:18:45: EPOCH 36: training on 272474 raw words (25257 effective words) took 0.8s, 32360 effective words/s\n",
      "INFO - 23:18:45: EPOCH 37: training on 272474 raw words (25106 effective words) took 0.8s, 30323 effective words/s\n",
      "INFO - 23:18:46: EPOCH 38: training on 272474 raw words (25074 effective words) took 0.8s, 31603 effective words/s\n",
      "INFO - 23:18:47: EPOCH 39: training on 272474 raw words (25051 effective words) took 0.8s, 32117 effective words/s\n",
      "INFO - 23:18:48: EPOCH 40: training on 272474 raw words (25241 effective words) took 0.8s, 31318 effective words/s\n",
      "INFO - 23:18:49: EPOCH 41: training on 272474 raw words (25059 effective words) took 0.8s, 31273 effective words/s\n",
      "INFO - 23:18:49: EPOCH 42: training on 272474 raw words (25336 effective words) took 0.8s, 31599 effective words/s\n",
      "INFO - 23:18:50: EPOCH 43: training on 272474 raw words (25347 effective words) took 0.9s, 29275 effective words/s\n",
      "INFO - 23:18:51: EPOCH 44: training on 272474 raw words (25267 effective words) took 0.9s, 26995 effective words/s\n",
      "INFO - 23:18:52: EPOCH 45: training on 272474 raw words (25147 effective words) took 0.8s, 31834 effective words/s\n",
      "INFO - 23:18:53: EPOCH 46: training on 272474 raw words (25204 effective words) took 0.8s, 31144 effective words/s\n",
      "INFO - 23:18:54: EPOCH 47: training on 272474 raw words (25078 effective words) took 0.8s, 30728 effective words/s\n",
      "INFO - 23:18:54: EPOCH 48: training on 272474 raw words (25166 effective words) took 0.8s, 31946 effective words/s\n",
      "INFO - 23:18:55: EPOCH 49: training on 272474 raw words (25145 effective words) took 0.9s, 29289 effective words/s\n",
      "INFO - 23:18:55: Word2Vec lifecycle event {'msg': 'training on 13623700 raw words (1259927 effective words) took 40.4s, 31218 effective words/s', 'datetime': '2023-04-01T23:18:55.732939', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:18:55: collecting all words and their counts\n",
      "INFO - 23:18:55: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:18:56: PROGRESS: at sentence #10000, processed 146133 words and 61917 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-05-20 completed\n",
      "time taken: 0.72 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:18:56: collected 61917 token types (unigram + bigrams) from a corpus of 161632 words and 11076 sentences\n",
      "INFO - 23:18:56: merged Phrases<61917 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:18:56: Phrases lifecycle event {'msg': 'built Phrases<61917 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.29s', 'datetime': '2023-04-01T23:18:56.038617', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:56: exporting phrases from Phrases<61917 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:18:56: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<873 phrases, min_count=5, threshold=7> from Phrases<61917 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.12s', 'datetime': '2023-04-01T23:18:56.168270', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:56: collecting all words and their counts\n",
      "INFO - 23:18:56: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:18:56: PROGRESS: at sentence #10000, processed 136031 words and 63756 word types\n",
      "INFO - 23:18:56: collected 63756 token types (unigram + bigrams) from a corpus of 150374 words and 11076 sentences\n",
      "INFO - 23:18:56: merged Phrases<63756 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:18:56: Phrases lifecycle event {'msg': 'built Phrases<63756 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.39s', 'datetime': '2023-04-01T23:18:56.559225', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:56: exporting phrases from Phrases<63756 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "INFO - 23:18:56: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<905 phrases, min_count=5, threshold=7> from Phrases<63756 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.13s', 'datetime': '2023-04-01T23:18:56.691909', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:56: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T23:18:56.692905', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "INFO - 23:18:56: collecting all words and their counts\n",
      "INFO - 23:18:56: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:18:57: PROGRESS: at sentence #10000, processed 133492 words, keeping 12156 word types\n",
      "INFO - 23:18:57: collected 12156 word types from a corpus of 147554 raw words and 11076 sentences\n",
      "INFO - 23:18:57: Creating a fresh vocabulary\n",
      "INFO - 23:18:57: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 1054 unique words (8.67% of original 12156, drops 11102)', 'datetime': '2023-04-01T23:18:57.096100', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:18:57: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 99102 word corpus (67.16% of original 147554, drops 48452)', 'datetime': '2023-04-01T23:18:57.097095', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:18:57: deleting the raw counts dictionary of 12156 items\n",
      "INFO - 23:18:57: sample=1e-05 downsamples 1054 most-common words\n",
      "INFO - 23:18:57: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 9156.022976736172 word corpus (9.2%% of prior 99102)', 'datetime': '2023-04-01T23:18:57.106952', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:18:57: constructing a huffman tree from 1054 words\n",
      "INFO - 23:18:57: built huffman tree with maximum node depth 12\n",
      "INFO - 23:18:57: estimated required memory for 1054 words and 100 dimensions: 2002600 bytes\n",
      "INFO - 23:18:57: resetting layer weights\n",
      "INFO - 23:18:57: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T23:18:57.156817', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "INFO - 23:18:57: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 1054 vocabulary and 100 features, using sg=0 hs=1 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T23:18:57.157817', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 23:18:57: EPOCH 0: training on 147554 raw words (9253 effective words) took 0.5s, 18451 effective words/s\n",
      "INFO - 23:18:58: EPOCH 1: training on 147554 raw words (9305 effective words) took 0.4s, 21466 effective words/s\n",
      "INFO - 23:18:58: EPOCH 2: training on 147554 raw words (9093 effective words) took 0.4s, 21494 effective words/s\n",
      "INFO - 23:18:58: EPOCH 3: training on 147554 raw words (9134 effective words) took 0.4s, 21747 effective words/s\n",
      "INFO - 23:18:59: EPOCH 4: training on 147554 raw words (9140 effective words) took 0.4s, 21062 effective words/s\n",
      "INFO - 23:18:59: EPOCH 5: training on 147554 raw words (9232 effective words) took 0.4s, 22288 effective words/s\n",
      "INFO - 23:19:00: EPOCH 6: training on 147554 raw words (9176 effective words) took 0.4s, 22431 effective words/s\n",
      "INFO - 23:19:00: EPOCH 7: training on 147554 raw words (9146 effective words) took 0.4s, 21577 effective words/s\n",
      "INFO - 23:19:01: EPOCH 8: training on 147554 raw words (9082 effective words) took 0.4s, 21859 effective words/s\n",
      "INFO - 23:19:01: EPOCH 9: training on 147554 raw words (9224 effective words) took 0.4s, 22410 effective words/s\n",
      "INFO - 23:19:01: EPOCH 10: training on 147554 raw words (9137 effective words) took 0.4s, 21650 effective words/s\n",
      "INFO - 23:19:02: EPOCH 11: training on 147554 raw words (9089 effective words) took 0.4s, 21465 effective words/s\n",
      "INFO - 23:19:02: EPOCH 12: training on 147554 raw words (9156 effective words) took 0.4s, 21778 effective words/s\n",
      "INFO - 23:19:03: EPOCH 13: training on 147554 raw words (9062 effective words) took 0.4s, 21053 effective words/s\n",
      "INFO - 23:19:03: EPOCH 14: training on 147554 raw words (9166 effective words) took 0.4s, 21668 effective words/s\n",
      "INFO - 23:19:04: EPOCH 15: training on 147554 raw words (9079 effective words) took 0.4s, 21376 effective words/s\n",
      "INFO - 23:19:04: EPOCH 16: training on 147554 raw words (9090 effective words) took 0.5s, 19266 effective words/s\n",
      "INFO - 23:19:04: EPOCH 17: training on 147554 raw words (9122 effective words) took 0.5s, 19559 effective words/s\n",
      "INFO - 23:19:05: EPOCH 18: training on 147554 raw words (9122 effective words) took 0.4s, 22267 effective words/s\n",
      "INFO - 23:19:05: EPOCH 19: training on 147554 raw words (9344 effective words) took 0.4s, 22577 effective words/s\n",
      "INFO - 23:19:06: EPOCH 20: training on 147554 raw words (9125 effective words) took 0.4s, 22142 effective words/s\n",
      "INFO - 23:19:06: EPOCH 21: training on 147554 raw words (9106 effective words) took 0.4s, 21280 effective words/s\n",
      "INFO - 23:19:07: EPOCH 22: training on 147554 raw words (9238 effective words) took 0.4s, 22353 effective words/s\n",
      "INFO - 23:19:07: EPOCH 23: training on 147554 raw words (9068 effective words) took 0.4s, 21241 effective words/s\n",
      "INFO - 23:19:07: EPOCH 24: training on 147554 raw words (9083 effective words) took 0.4s, 21128 effective words/s\n",
      "INFO - 23:19:08: EPOCH 25: training on 147554 raw words (9226 effective words) took 0.4s, 21605 effective words/s\n",
      "INFO - 23:19:08: EPOCH 26: training on 147554 raw words (9227 effective words) took 0.4s, 21619 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:19:09: EPOCH 27: training on 147554 raw words (9215 effective words) took 0.4s, 22217 effective words/s\n",
      "INFO - 23:19:09: EPOCH 28: training on 147554 raw words (9017 effective words) took 0.5s, 19347 effective words/s\n",
      "INFO - 23:19:10: EPOCH 29: training on 147554 raw words (9058 effective words) took 0.4s, 22150 effective words/s\n",
      "INFO - 23:19:10: EPOCH 30: training on 147554 raw words (9363 effective words) took 0.4s, 22592 effective words/s\n",
      "INFO - 23:19:10: EPOCH 31: training on 147554 raw words (9270 effective words) took 0.4s, 22624 effective words/s\n",
      "INFO - 23:19:11: EPOCH 32: training on 147554 raw words (9189 effective words) took 0.4s, 21764 effective words/s\n",
      "INFO - 23:19:11: EPOCH 33: training on 147554 raw words (9252 effective words) took 0.4s, 22260 effective words/s\n",
      "INFO - 23:19:12: EPOCH 34: training on 147554 raw words (9075 effective words) took 0.4s, 22226 effective words/s\n",
      "INFO - 23:19:12: EPOCH 35: training on 147554 raw words (9179 effective words) took 0.4s, 21479 effective words/s\n",
      "INFO - 23:19:13: EPOCH 36: training on 147554 raw words (9085 effective words) took 0.4s, 20531 effective words/s\n",
      "INFO - 23:19:13: EPOCH 37: training on 147554 raw words (9054 effective words) took 0.4s, 21225 effective words/s\n",
      "INFO - 23:19:13: EPOCH 38: training on 147554 raw words (9064 effective words) took 0.4s, 21150 effective words/s\n",
      "INFO - 23:19:14: EPOCH 39: training on 147554 raw words (9148 effective words) took 0.4s, 21848 effective words/s\n",
      "INFO - 23:19:14: EPOCH 40: training on 147554 raw words (9195 effective words) took 0.4s, 22214 effective words/s\n",
      "INFO - 23:19:15: EPOCH 41: training on 147554 raw words (9105 effective words) took 0.4s, 22555 effective words/s\n",
      "INFO - 23:19:15: EPOCH 42: training on 147554 raw words (9222 effective words) took 0.4s, 22616 effective words/s\n",
      "INFO - 23:19:16: EPOCH 43: training on 147554 raw words (9091 effective words) took 0.4s, 21917 effective words/s\n",
      "INFO - 23:19:16: EPOCH 44: training on 147554 raw words (9217 effective words) took 0.4s, 22428 effective words/s\n",
      "INFO - 23:19:16: EPOCH 45: training on 147554 raw words (9149 effective words) took 0.5s, 19807 effective words/s\n",
      "INFO - 23:19:17: EPOCH 46: training on 147554 raw words (9147 effective words) took 0.4s, 22189 effective words/s\n",
      "INFO - 23:19:17: EPOCH 47: training on 147554 raw words (9021 effective words) took 0.4s, 21578 effective words/s\n",
      "INFO - 23:19:18: EPOCH 48: training on 147554 raw words (9166 effective words) took 0.4s, 21429 effective words/s\n",
      "INFO - 23:19:18: EPOCH 49: training on 147554 raw words (9136 effective words) took 0.4s, 21408 effective words/s\n",
      "INFO - 23:19:18: Word2Vec lifecycle event {'msg': 'training on 7377700 raw words (457643 effective words) took 21.5s, 21320 effective words/s', 'datetime': '2023-04-01T23:19:18.623811', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week starting 2020-05-27 completed\n",
      "time taken: 0.38 mins\n",
      "time taken: 11.41 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "for start_ in start_of_weeks:\n",
    "    t_1 = time()\n",
    "    sentences = MyCorpus(\"weekly training text for word2vec\\\\{} week tweets.txt\".format(start_))\n",
    "    bigrams = Phrases(sentences, min_count = 5, threshold = 7, delimiter = \" \")\n",
    "    bigram_mod = Phraser(bigrams)\n",
    "    bigram_sentences = MyBigramCorpus(\"weekly training text for word2vec\\\\{} week tweets.txt\".format(start_), bigram_mod)\n",
    "    trigrams = Phrases(bigram_sentences, min_count = 5, threshold = 7, delimiter = \" \")\n",
    "    trigram_mod = Phraser(trigrams)\n",
    "    trigram_sentences = MyTrigramCorpus(\"weekly training text for word2vec\\\\{} week tweets.txt\".format(start_), bigram_mod, trigram_mod)\n",
    "    w2v_model = Word2Vec(\n",
    "        min_count = 20,\n",
    "        window = 5,\n",
    "        vector_size = 100,\n",
    "        sample = 1e-5,\n",
    "        alpha = 0.025,\n",
    "        min_alpha = 0.0001,\n",
    "        hs = 1,\n",
    "        sg = 0,\n",
    "        workers = cores - 2)\n",
    "    w2v_model.build_vocab(trigram_sentences, progress_per = 10000)\n",
    "    w2v_model.train(trigram_sentences, total_examples = w2v_model.corpus_count, epochs = 50, report_delay = 1)\n",
    "    with open(\"weekly word2vec models\\\\week_{}.pkl\".format(start_), \"wb\") as f:\n",
    "        pickle.dump(w2v_model, f)\n",
    "    print(\"week starting {} completed\".format(start_))\n",
    "    print(\"time taken: {} mins\".format(round((time() - t_1)/60, 2)))\n",
    "print(\"time taken: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you are going through the entire thesis workflow, you must also save these word2vec models in a folder \n",
    "#named the same within the \"Contextual Analysis\" main folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
