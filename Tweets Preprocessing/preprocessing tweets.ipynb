{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3028ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "import csv\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891713ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing list of slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d03743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {}\n",
    "\n",
    "for _row in csv.reader(open(\"slang.txt\", \"r\"), delimiter = \"=\"):\n",
    "    if _row != \"\":\n",
    "        slang_dict[_row[0]] = _row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eaab286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a list of punctuation symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18eb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list = [\",\", \".\", \"!\", \"?\", \"$\", \"%\", \"^\", \"&\", \"*\", \")\", \"(\", \"=\", \\\n",
    "                    \":\", \"|\", \";\", \"<\", \">\", \"~\", \"‘\", \"'\", \"/\", \"\\\\\", \"’\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc844ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a class that goes through all csv files(1 file per day) within a folder, applies all \n",
    "#the cleaning functionswithin it and saves the cleaned tweets in a dataframe, saves each day's dataframe\n",
    "#in a pickled file in a folder title \"cleaned_folder\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e899830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_cleaner:\n",
    "    \n",
    "    def __init__(self, _file, _folder, save_folder, necessary_columns = [\"id\", \"conversation_id\", \"date\", \"time\", \\\n",
    "                                                            \"user_id\", \"tweet\", \"retweets_count\",\\\n",
    "                                                            \"replies_count\", \"likes_count\", \"retweet\"]):\n",
    "        self.folder = _folder\n",
    "        self.file = _file\n",
    "        self.save_folder = save_folder\n",
    "        self.necessary_columns = necessary_columns\n",
    "        self.df = pd.read_csv(os.path.join(os.path.abspath(self.folder), self.file))\n",
    "        self.cleaned_df = self.clean_tweet()\n",
    "        self.output = self.save_output()\n",
    "        \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    #find the share of capital letters used in a tweet\n",
    "    def find_caps_share(tweet):\n",
    "        caps_length = len(re.findall(r\"[A-Z]\", tweet))\n",
    "        tweet_length = len(re.sub(r\"\\s\", \"\", tweet))\n",
    "        \n",
    "        return caps_length/(1 + tweet_length)\n",
    "    \n",
    "    @staticmethod\n",
    "    #replace contractions such as \"can't\", \"won't\" etc with \"cannot\", \"will not\"\n",
    "    def remove_contractions(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub(r\"can\\s?'\\s?t\", \"cannot\", tweet)\n",
    "        tweet = re.sub(r\"won\\s?'\\s?t\", \"will not\", tweet)\n",
    "        tweet = re.sub(r\"n\\s?'\\s?t\", \" not\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?ve\", \" have\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?ll\", \" will\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?re\", \" are\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?m\", \" am\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?d\", \" would\", tweet)\n",
    "        \n",
    "        tweet = re.sub(r\"can\\s?‘\\s?t\", \"cannot\", tweet)\n",
    "        tweet = re.sub(r\"won\\s?‘\\s?t\", \"will not\", tweet)\n",
    "        tweet = re.sub(r\"n\\s?‘\\s?t\", \" not\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?ve\", \" have\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?ll\", \" will\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?re\", \" are\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?m\", \" am\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?d\", \" would\", tweet)\n",
    "        \n",
    "        tweet = re.sub(r\"can\\s?’\\s?t\", \"cannot\", tweet)\n",
    "        tweet = re.sub(r\"won\\s?’\\s?t\", \"will not\", tweet)\n",
    "        tweet = re.sub(r\"n\\s?’\\s?t\", \" not\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?ve\", \" have\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?ll\", \" will\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?re\", \" are\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?m\", \" am\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?d\", \" would\", tweet)\n",
    "        \n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove all the punctuation symbols defined before from the tweet\n",
    "    def remove_punctuation(tweet):\n",
    "        for _punct in punctuation_list:\n",
    "            tweet = tweet.replace(_punct, \" \")\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #return the hashtag used in the tweet\n",
    "    def tweet_hashtag(tweet):\n",
    "        splits = tweet.split()\n",
    "        splits = [split[1:] for split in splits if split[0] == \"#\"]\n",
    "        return \" \".join(splits)\n",
    "    \n",
    "    @staticmethod\n",
    "    #return all users mentioned in the tweet\n",
    "    def tweet_target(tweet):\n",
    "        splits = tweet.split()\n",
    "        splits = [split[1:] for split in splits if split[0] == \"@\"]\n",
    "        return \" \".join(splits)\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove any users mentioned in the tweet\n",
    "    def remove_targets(tweet):\n",
    "        tweet = re.sub(r\"@_*\\w+(_+\\w+)*_*\", \" \", tweet)\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove any hashtags in the tweet\n",
    "    def remove_hashtags(tweet):\n",
    "        tweet = re.sub(r\"#\\w+\", \" \", tweet)\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove all urls in the tweet\n",
    "    def remove_url(tweet):\n",
    "        tweet = re.sub(r\"https?://t.co/\\w+\", \" \", tweet)\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #identify all emojis used in the tweet, find their meaning in string format, return a string made up of all emoji meanings\n",
    "    def find_emojis(tweet):\n",
    "        emojis = []\n",
    "        for _emoji in UNICODE_EMOJI:\n",
    "            if _emoji in tweet:\n",
    "                for i in range(len((re.findall(_emoji, tweet)))):\n",
    "                    emojis.append(UNICODE_EMOJI[_emoji].replace(\":\", \"\"))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return \" \".join(emojis)\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove all emojis in the tweet\n",
    "    def remove_emojis(tweet):\n",
    "        for _emoji in UNICODE_EMOJI:\n",
    "            tweet = tweet.replace(_emoji, \" \")\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #replace all slang words in the tweet by their meaning using previously imported list\n",
    "    def remove_slang(tweet):\n",
    "        new_text = []\n",
    "        for _word in tweet.split():\n",
    "            if _word.upper() in slang_dict.keys():\n",
    "                new_text.append(slang_dict[_word.upper()])\n",
    "            else:\n",
    "                new_text.append(_word)\n",
    "        \n",
    "        return \" \".join(new_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove remaning punctuation marks\n",
    "    def make_alphabetic(tweet):\n",
    "        tweet = tweet.replace(\"…\", \" \")\n",
    "        tweet = tweet.replace(\"_\", \" \")\n",
    "        tweet = tweet.replace(\"@\", \" \")\n",
    "        tweet = tweet.replace(\"#\", \" \")\n",
    "        return \" \".join([_word for _word in tweet.split() if _word.isalpha() and len(_word) > 1 \\\n",
    "                        and _word != \"rt\"])\n",
    "    \n",
    "    #create dataframe out of day's data. create the new columns \"cleaned_tweet\", \"caps_share\", \"target\", \"hashtag\", \n",
    "    #\"emojis\" \n",
    "    def clean_tweet(self):\n",
    "        _cleaned = self.df.copy()\n",
    "        _cleaned = _cleaned.where(_cleaned.language == \"en\").dropna(subset = [\"language\"])\n",
    "        _cleaned.reset_index(drop = True, inplace = True)\n",
    "        _cleaned = _cleaned[self.necessary_columns]\n",
    "        _cleaned[\"caps_share\"] = _cleaned[\"tweet\"].apply(self.find_caps_share)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"tweet\"].apply(self.remove_contractions)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_url)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_punctuation)\n",
    "        _cleaned[\"target\"] = _cleaned[\"cleaned_tweet\"].apply(self.tweet_target)\n",
    "        _cleaned[\"hashtag\"] = _cleaned[\"cleaned_tweet\"].apply(self.tweet_hashtag)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_targets)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_hashtags)\n",
    "        _cleaned[\"emojis\"] = _cleaned[\"cleaned_tweet\"].apply(self.find_emojis)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_emojis)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_slang)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.make_alphabetic)\n",
    "        _cleaned.dropna(subset = [\"cleaned_tweet\"], inplace = True)\n",
    "        _cleaned.reset_index(drop = True, inplace = True)\n",
    "        return _cleaned\n",
    "    \n",
    "    #save the dataframe as pickled file titled \"cleaned filename\" in a new folder titled \"cleaned foldername\" with \n",
    "    def save_output(self):\n",
    "        try:\n",
    "            state_name = self.folder.rpartition(\"\\\\\")[-1]\n",
    "            dirname = \"cleaned {}\".format(state_name)\n",
    "            folder_name = self.save_folder + \"\\\\\" +  dirname\n",
    "            os.mkdir(folder_name)\n",
    "            print(\"Folder {} created\".format(dirname))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        with open(os.path.join(os.path.abspath(folder_name), \"cleaned {}.pkl\".format(self.file)), \"wb\") as f:\n",
    "            pickle.dump(self.cleaned_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44de5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a loop through all states in the Raw Data folder and saving the cleaned output in the \n",
    "#Cleaned Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23613d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _folder in os.listdir(\"Raw Data\"):\n",
    "    t_1 = time()\n",
    "    file_list = [_file for _file in os.listdir(\"raw data\\\\{}\".format(_folder)) if \".csv\" in _file]\n",
    "    for _file in file_list:\n",
    "        tweet_cleaner(_file, os.path.join(\"raw data\", _folder), \"Cleaned Data\")\n",
    "    print(\"{} completed\".format(_folder))\n",
    "    print(\"time taken: {} mins\".format(round((time() - t_1)/60, 2)))\n",
    "print(\"time taken: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa3b751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting all tweets written in Roman script to train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2341b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_collecter:\n",
    "    \n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.file_list = [file for file in os.listdir(self.folder) if \".csv\" in file]\n",
    "        self.tweets = self.extract_tweets()\n",
    "     \n",
    "    @staticmethod\n",
    "    def push_to_csv(_df):\n",
    "        _df.to_csv(\"word2vec_tweets.csv\", mode = \"a\", header = False, index = False)\n",
    "        with open(\"word2vec_tweets.csv\") as f:\n",
    "            f.close()\n",
    "        \n",
    "        \n",
    "    def extract_tweets(self):\n",
    "        for file in self.file_list:\n",
    "            df = pickle.load(open(os.path.join(os.path.abspath(self.folder), file), \"rb\"))\n",
    "            df = df.dropna(subset = [\"cleaned_tweet\"])\n",
    "            df.drop_duplicates(subset = [\"cleaned_tweet\"], inplace = True)\n",
    "            df.reset_index(inplace = True, drop = True)\n",
    "            self.push_to_csv(df.loc[:, [\"cleaned_tweet\", \"id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5507b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = time()\n",
    "for _folder in os.listdir(\"Cleaned Data\"):\n",
    "    t_2 = time()\n",
    "    _path = os.path.join(\"Cleaned Data\", _folder)\n",
    "    tweet_collecter(_path)\n",
    "    print(\"completed {}\".format(_folder))\n",
    "    print(\"time taken to complete: {} mins\".format(round((time() - t_2)/60, 2)))\n",
    "print(\"time taken to complete program: {} mins\".format(round((time() - t_1)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "832f4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the file of collected tweets and removing duplicate tweets from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"word2vec_tweets.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop_duplicates(subset = [0], inplace = True)\n",
    "tweets.drop_duplicates(subset = [1], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93ad0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the tweets to a txt file for feeding into the word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bfda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word2vec_tweets.txt\", \"w\", encoding = \"utf-8\") as f:\n",
    "    for _row in tweets[0]:\n",
    "        f.write(str(_row))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49324a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving tweet ids to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tweets[1].values, open(\"unique tweet ids.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "186e0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding tweet ids of all tweets across all states, taking care to drop all duplicates within the same day and\n",
    "#then adding all these ids to a list. Only the unique values in this list are retained as the final ids of\n",
    "#state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "state_list = [state for state in os.listdir(\"Cleaned Data\")]\n",
    "for state in state_list:\n",
    "    ids = np.array(0)\n",
    "    t_1 = time()\n",
    "    _path = os.path.join(\"Cleaned Data\", state)\n",
    "    file_list = [file for file in os.listdir(_path) if \".csv\" in file]\n",
    "    for file in file_list:\n",
    "        df = pickle.load(open(os.path.join(_path, file), \"rb\"))\n",
    "        df = df.where(df[\"id\"].isin(tweets[1].values)).dropna(subset = [\"id\"])\n",
    "        df.drop_duplicates(subset = [\"id\"], inplace = True)\n",
    "        df.drop_duplicates(subset = [\"cleaned_tweet\"], inplace = True)\n",
    "        ids = np.append(ids, df[\"id\"].values)\n",
    "    ids = np.unique(ids)\n",
    "    pickle.dump(ids, open(os.path.join(_path, \"{} ids.pkl\".format(state.split()[-1])), \"wb\"))\n",
    "print(\"time taken: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminating duplicate tweets across states. code snippet compares the ids list of two states, keeping only the\n",
    "#unique values of list 2 in list 2, and saving list 1 as is. b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8eefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = os.listdir(\"Cleaned Data\")\n",
    "for state in state_list:\n",
    "    search_list = state_list[state_list.index(state) + 1: ]\n",
    "    _path = os.path.join(\"Cleaned Data\", state)\n",
    "    state_ids = pickle.load(open(os.path.join(_path, \"{} ids.pkl\".format(state.split()[-1])), \"rb\"))\n",
    "    for _state in search_list:\n",
    "        _path_2 = os.path.join(\"Cleaned Data\", _state)\n",
    "        _state_ids = pickle.load(open(os.path.join(_path_2, \"{} ids.pkl\".format(_state.split()[-1])), \"rb\"))\n",
    "        mask = np.isin(_state_ids, state_ids, invert = True)\n",
    "        _state = _state_ids[mask]\n",
    "        pickle.dump(_state_ids, open(os.path.join(_path_2, \"{} ids.pkl\".format(_state.split()[-1])), \"wb\"))\n",
    "    pickle.dump(state_ids, open(os.path.join(_path, \"{} ids.pkl\".format(state.split()[-1])), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0925dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
