{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55added",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "import csv\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f7c229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing list of slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e0eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {}\n",
    "\n",
    "for _row in csv.reader(open(\"slang.txt\", \"r\"), delimiter = \"=\"):\n",
    "    if _row != \"\":\n",
    "        slang_dict[_row[0]] = _row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb8e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a list of punctuation symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3032a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list = [\",\", \".\", \"!\", \"?\", \"$\", \"%\", \"^\", \"&\", \"*\", \")\", \"(\", \"=\", \\\n",
    "                    \":\", \"|\", \";\", \"<\", \">\", \"~\", \"‘\", \"'\", \"/\", \"\\\\\", \"’\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c48d9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a class that goes through all csv files(1 file per day) within a folder, applies all \n",
    "#the cleaning functionswithin it and saves the cleaned tweets in a dataframe, saves each day's dataframe\n",
    "#in a pickled file in a folder title \"cleaned_folder\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f9c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_cleaner:\n",
    "    \n",
    "    def __init__(self, _file, _folder, save_folder, necessary_columns = [\"id\", \"conversation_id\", \"date\", \"time\", \\\n",
    "                                                            \"user_id\", \"tweet\", \"retweets_count\",\\\n",
    "                                                            \"replies_count\", \"likes_count\", \"retweet\"]):\n",
    "        self.folder = _folder\n",
    "        self.file = _file\n",
    "        self.save_folder = save_folder\n",
    "        self.necessary_columns = necessary_columns\n",
    "        self.df = pd.read_csv(os.path.join(os.path.abspath(self.folder), self.file))\n",
    "        self.cleaned_df = self.clean_tweet()\n",
    "        self.output = self.save_output()\n",
    "        \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    #find the share of capital letters used in a tweet\n",
    "    def find_caps_share(tweet):\n",
    "        caps_length = len(re.findall(r\"[A-Z]\", tweet))\n",
    "        tweet_length = len(re.sub(r\"\\s\", \"\", tweet))\n",
    "        \n",
    "        return caps_length/(1 + tweet_length)\n",
    "    \n",
    "    @staticmethod\n",
    "    #replace contractions such as \"can't\", \"won't\" etc with \"cannot\", \"will not\"\n",
    "    def remove_contractions(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub(r\"can\\s?'\\s?t\", \"cannot\", tweet)\n",
    "        tweet = re.sub(r\"won\\s?'\\s?t\", \"will not\", tweet)\n",
    "        tweet = re.sub(r\"n\\s?'\\s?t\", \" not\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?ve\", \" have\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?ll\", \" will\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?re\", \" are\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?m\", \" am\", tweet)\n",
    "        tweet = re.sub(r\"'\\s?d\", \" would\", tweet)\n",
    "        \n",
    "        tweet = re.sub(r\"can\\s?‘\\s?t\", \"cannot\", tweet)\n",
    "        tweet = re.sub(r\"won\\s?‘\\s?t\", \"will not\", tweet)\n",
    "        tweet = re.sub(r\"n\\s?‘\\s?t\", \" not\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?ve\", \" have\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?ll\", \" will\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?re\", \" are\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?m\", \" am\", tweet)\n",
    "        tweet = re.sub(r\"‘\\s?d\", \" would\", tweet)\n",
    "        \n",
    "        tweet = re.sub(r\"can\\s?’\\s?t\", \"cannot\", tweet)\n",
    "        tweet = re.sub(r\"won\\s?’\\s?t\", \"will not\", tweet)\n",
    "        tweet = re.sub(r\"n\\s?’\\s?t\", \" not\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?ve\", \" have\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?ll\", \" will\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?re\", \" are\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?m\", \" am\", tweet)\n",
    "        tweet = re.sub(r\"’\\s?d\", \" would\", tweet)\n",
    "        \n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove all the punctuation symbols defined before from the tweet\n",
    "    def remove_punctuation(tweet):\n",
    "        for _punct in punctuation_list:\n",
    "            tweet = tweet.replace(_punct, \" \")\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #return the hashtag used in the tweet\n",
    "    def tweet_hashtag(tweet):\n",
    "        splits = tweet.split()\n",
    "        splits = [split[1:] for split in splits if split[0] == \"#\"]\n",
    "        return \" \".join(splits)\n",
    "    \n",
    "    @staticmethod\n",
    "    #return all users mentioned in the tweet\n",
    "    def tweet_target(tweet):\n",
    "        splits = tweet.split()\n",
    "        splits = [split[1:] for split in splits if split[0] == \"@\"]\n",
    "        return \" \".join(splits)\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove any users mentioned in the tweet\n",
    "    def remove_targets(tweet):\n",
    "        tweet = re.sub(r\"@_*\\w+(_+\\w+)*_*\", \" \", tweet)\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove any hashtags in the tweet\n",
    "    def remove_hashtags(tweet):\n",
    "        tweet = re.sub(r\"#\\w+\", \" \", tweet)\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove all urls in the tweet\n",
    "    def remove_url(tweet):\n",
    "        tweet = re.sub(r\"https?://t.co/\\w+\", \" \", tweet)\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #identify all emojis used in the tweet, find their meaning in string format, return a string made up of all emoji meanings\n",
    "    def find_emojis(tweet):\n",
    "        emojis = []\n",
    "        for _emoji in UNICODE_EMOJI:\n",
    "            if _emoji in tweet:\n",
    "                for i in range(len((re.findall(_emoji, tweet)))):\n",
    "                    emojis.append(UNICODE_EMOJI[_emoji].replace(\":\", \"\"))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return \" \".join(emojis)\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove all emojis in the tweet\n",
    "    def remove_emojis(tweet):\n",
    "        for _emoji in UNICODE_EMOJI:\n",
    "            tweet = tweet.replace(_emoji, \" \")\n",
    "        return \" \".join(tweet.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    #replace all slang words in the tweet by their meaning using previously imported list\n",
    "    def remove_slang(tweet):\n",
    "        new_text = []\n",
    "        for _word in tweet.split():\n",
    "            if _word.upper() in slang_dict.keys():\n",
    "                new_text.append(slang_dict[_word.upper()])\n",
    "            else:\n",
    "                new_text.append(_word)\n",
    "        \n",
    "        return \" \".join(new_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    #remove remaning punctuation marks\n",
    "    def make_alphabetic(tweet):\n",
    "        tweet = tweet.replace(\"…\", \" \")\n",
    "        tweet = tweet.replace(\"_\", \" \")\n",
    "        tweet = tweet.replace(\"@\", \" \")\n",
    "        tweet = tweet.replace(\"#\", \" \")\n",
    "        return \" \".join([_word for _word in tweet.split() if _word.isalpha() and len(_word) > 1 \\\n",
    "                        and _word != \"rt\"])\n",
    "    \n",
    "    #create dataframe out of day's data. create the new columns \"cleaned_tweet\", \"caps_share\", \"target\", \"hashtag\", \n",
    "    #\"emojis\" \n",
    "    def clean_tweet(self):\n",
    "        _cleaned = self.df.copy()\n",
    "        _cleaned = _cleaned.where(_cleaned.language == \"en\").dropna(subset = [\"language\"])\n",
    "        _cleaned.reset_index(drop = True, inplace = True)\n",
    "        _cleaned = _cleaned[self.necessary_columns]\n",
    "        _cleaned[\"caps_share\"] = _cleaned[\"tweet\"].apply(self.find_caps_share)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"tweet\"].apply(self.remove_contractions)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_url)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_punctuation)\n",
    "        _cleaned[\"target\"] = _cleaned[\"cleaned_tweet\"].apply(self.tweet_target)\n",
    "        _cleaned[\"hashtag\"] = _cleaned[\"cleaned_tweet\"].apply(self.tweet_hashtag)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_targets)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_hashtags)\n",
    "        _cleaned[\"emojis\"] = _cleaned[\"cleaned_tweet\"].apply(self.find_emojis)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_emojis)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.remove_slang)\n",
    "        _cleaned[\"cleaned_tweet\"] = _cleaned[\"cleaned_tweet\"].apply(self.make_alphabetic)\n",
    "        _cleaned.dropna(subset = [\"cleaned_tweet\"], inplace = True)\n",
    "        _cleaned.reset_index(drop = True, inplace = True)\n",
    "        return _cleaned\n",
    "    \n",
    "    #save the dataframe as pickled file titled \"cleaned filename\" in a new folder titled \"cleaned foldername\" with \n",
    "    def save_output(self):\n",
    "        try:\n",
    "            state_name = self.folder.rpartition(\"\\\\\")[-1]\n",
    "            dirname = \"cleaned {}\".format(state_name)\n",
    "            folder_name = self.save_folder + \"\\\\\" +  dirname\n",
    "            os.mkdir(folder_name)\n",
    "            print(\"Folder {} created\".format(dirname))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        with open(os.path.join(os.path.abspath(folder_name), \"cleaned {}.pkl\".format(self.file)), \"wb\") as f:\n",
    "            pickle.dump(self.cleaned_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e35a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a loop through all states in the Raw Data folder and saving the cleaned output state-wise in the \n",
    "#\"Cleaned Data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732cdcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder cleaned Andhra_Pradesh created\n",
      "Andhra_Pradesh completed\n",
      "time taken: 3.2 mins\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m completed\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_folder))\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime taken: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m mins\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m((time() \u001b[38;5;241m-\u001b[39m t_1)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime taken: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m mins\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m((time() \u001b[38;5;241m-\u001b[39m \u001b[43mt\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "for _folder in os.listdir(\"Raw Data\"):\n",
    "    t_1 = time()\n",
    "    file_list = [_file for _file in os.listdir(\"Raw Data\\\\{}\".format(_folder)) if \".csv\" in _file]\n",
    "    for _file in file_list:\n",
    "        tweet_cleaner(_file, os.path.join(\"Raw Data\", _folder), \"Cleaned Data\")\n",
    "    print(\"{} completed\".format(_folder))\n",
    "    print(\"time taken: {} mins\".format(round((time() - t_1)/60, 2)))\n",
    "print(\"time taken: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14d0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open(\"Cleaned Data\\\\cleaned Andhra_Pradesh\\\\cleaned 2020-01-22.csv.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48f6e693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1345, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bcb8e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>retweet</th>\n",
       "      <th>caps_share</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.220133e+18</td>\n",
       "      <td>1.220133e+18</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>00:57:35</td>\n",
       "      <td>6.341111e+07</td>\n",
       "      <td>Follow @tokslabossmua on YouTube: thanks so mu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>follow on youtube thanks so much dossier perfu...</td>\n",
       "      <td>tokslabossmua</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.220077e+18</td>\n",
       "      <td>1.220077e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>21:12:47</td>\n",
       "      <td>3.309983e+09</td>\n",
       "      <td>Enjoy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>enjoy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.220059e+18</td>\n",
       "      <td>1.220059e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>19:59:54</td>\n",
       "      <td>1.032304e+18</td>\n",
       "      <td>#JustAskSachin Sir what is u r stands on CAA  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.212329</td>\n",
       "      <td>sir what is You stands on caa while ago You ha...</td>\n",
       "      <td></td>\n",
       "      <td>justasksachin</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.220054e+18</td>\n",
       "      <td>1.220054e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>19:43:49</td>\n",
       "      <td>8.816434e+07</td>\n",
       "      <td>THINKING OF A SEA CHANGE ? HERE IS YOUR OPPORT...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>thinking of sea change here is your opportunity</td>\n",
       "      <td></td>\n",
       "      <td>residentialplot land forsale beachroad visakha...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.220051e+18</td>\n",
       "      <td>1.220011e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>19:30:00</td>\n",
       "      <td>1.160402e+18</td>\n",
       "      <td>@turagasudhakar @BeSriSri @prasana_kumar @saib...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>ok how deciding there in andhra three capital ...</td>\n",
       "      <td>turagasudhakar besrisri prasana_kumar saibolli...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  conversation_id        date      time       user_id  \\\n",
       "0  1.220133e+18     1.220133e+18  2020-01-23  00:57:35  6.341111e+07   \n",
       "1  1.220077e+18     1.220077e+18  2020-01-22  21:12:47  3.309983e+09   \n",
       "2  1.220059e+18     1.220059e+18  2020-01-22  19:59:54  1.032304e+18   \n",
       "3  1.220054e+18     1.220054e+18  2020-01-22  19:43:49  8.816434e+07   \n",
       "4  1.220051e+18     1.220011e+18  2020-01-22  19:30:00  1.160402e+18   \n",
       "\n",
       "                                               tweet  retweets_count  \\\n",
       "0  Follow @tokslabossmua on YouTube: thanks so mu...             0.0   \n",
       "1                                              Enjoy             0.0   \n",
       "2  #JustAskSachin Sir what is u r stands on CAA  ...             0.0   \n",
       "3  THINKING OF A SEA CHANGE ? HERE IS YOUR OPPORT...             0.0   \n",
       "4  @turagasudhakar @BeSriSri @prasana_kumar @saib...             0.0   \n",
       "\n",
       "   replies_count  likes_count retweet  caps_share  \\\n",
       "0            0.0          0.0   False    0.086957   \n",
       "1            0.0          0.0   False    0.166667   \n",
       "2            0.0          0.0   False    0.212329   \n",
       "3            0.0          0.0   False    0.385714   \n",
       "4            0.0          0.0   False    0.054795   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  follow on youtube thanks so much dossier perfu...   \n",
       "1                                              enjoy   \n",
       "2  sir what is You stands on caa while ago You ha...   \n",
       "3    thinking of sea change here is your opportunity   \n",
       "4  ok how deciding there in andhra three capital ...   \n",
       "\n",
       "                                              target  \\\n",
       "0                                      tokslabossmua   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  turagasudhakar besrisri prasana_kumar saibolli...   \n",
       "\n",
       "                                             hashtag emojis  \n",
       "0                                                            \n",
       "1                                                            \n",
       "2                                      justasksachin         \n",
       "3  residentialplot land forsale beachroad visakha...         \n",
       "4                                                            "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "566a4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting all tweets written in Roman script to train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "159908f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_collecter:\n",
    "    \n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.file_list = [file for file in os.listdir(self.folder) if \".csv\" in file]\n",
    "        self.tweets = self.extract_tweets()\n",
    "    \n",
    "    #take a dataframe as input and save it in a csv file\n",
    "    @staticmethod\n",
    "    def push_to_csv(_df):\n",
    "        _df.to_csv(\"word2vec_tweets.csv\", mode = \"a\", header = False, index = False)\n",
    "        with open(\"word2vec_tweets.csv\") as f:\n",
    "            f.close()\n",
    "        \n",
    "    \n",
    "    #load every file in the folder as a dataframe\n",
    "    #drop duplicate tweets and any tweets with NaN in the \"cleaned_tweet\" column\n",
    "    #use the push_to_csv method to save the \"cleaned_tweet\" and \"id\" column in a csv file\n",
    "    def extract_tweets(self):\n",
    "        for file in self.file_list:\n",
    "            df = pickle.load(open(os.path.join(os.path.abspath(self.folder), file), \"rb\"))\n",
    "            df = df.dropna(subset = [\"cleaned_tweet\"])\n",
    "            df.drop_duplicates(subset = [\"cleaned_tweet\"], inplace = True)\n",
    "            df.reset_index(inplace = True, drop = True)\n",
    "            self.push_to_csv(df.loc[:, [\"cleaned_tweet\", \"id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3494dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through every folder in \"Cleaned Data\" folder and apply the above class to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbd99fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed cleaned Andhra_Pradesh\n",
      "time taken to complete: 0.07 mins\n",
      "time taken to complete program: 0.07 mins\n"
     ]
    }
   ],
   "source": [
    "t_1 = time()\n",
    "for _folder in os.listdir(\"Cleaned Data\"):\n",
    "    t_2 = time()\n",
    "    _path = os.path.join(\"Cleaned Data\", _folder)\n",
    "    tweet_collecter(_path)\n",
    "    print(\"completed {}\".format(_folder))\n",
    "    print(\"time taken to complete: {} mins\".format(round((time() - t_2)/60, 2)))\n",
    "print(\"time taken to complete program: {} mins\".format(round((time() - t_1)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f51c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the file of collected tweets and removing duplicate tweets from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2250bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"word2vec_tweets.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7b4fced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195870, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98bf85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop_duplicates(subset = [0], inplace = True)\n",
    "tweets.drop_duplicates(subset = [1], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6144c317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175192, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bafefc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the tweets to a txt file for feeding into the word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dda98d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word2vec_tweets.txt\", \"w\", encoding = \"utf-8\") as f:\n",
    "    for _row in tweets[0]:\n",
    "        f.write(str(_row))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1325906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving tweet ids to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "042f3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tweets[1].values, open(\"unique tweet ids.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fed46e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding tweet ids of all tweets across all states, taking care to drop all duplicates within the same day and\n",
    "#then adding all these ids to a list. Only the unique values in this list are retained as the final ids of\n",
    "#state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03dec0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0.03 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "state_list = [state for state in os.listdir(\"Cleaned Data\")]\n",
    "for state in state_list:\n",
    "    ids = np.array(0)\n",
    "    t_1 = time()\n",
    "    _path = os.path.join(\"Cleaned Data\", state)\n",
    "    file_list = [file for file in os.listdir(_path) if \".csv\" in file]\n",
    "    for file in file_list:\n",
    "        df = pickle.load(open(os.path.join(_path, file), \"rb\"))\n",
    "        df = df.where(df[\"id\"].isin(tweets[1].values)).dropna(subset = [\"id\"])\n",
    "        df.drop_duplicates(subset = [\"id\"], inplace = True)\n",
    "        df.drop_duplicates(subset = [\"cleaned_tweet\"], inplace = True)\n",
    "        ids = np.append(ids, df[\"id\"].values)\n",
    "    ids = np.unique(ids)\n",
    "    pickle.dump(ids, open(os.path.join(_path, \"{} ids.pkl\".format(state.split()[-1])), \"wb\"))\n",
    "print(\"time taken: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminating duplicate tweets across states. code snippet compares the ids list of two states, keeping only the\n",
    "#unique values of list 2 in list 2, and saving list 1 as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "984a544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = os.listdir(\"Cleaned Data\")\n",
    "for state in state_list:\n",
    "    search_list = state_list[state_list.index(state) + 1: ]\n",
    "    _path = os.path.join(\"Cleaned Data\", state)\n",
    "    state_ids = pickle.load(open(os.path.join(_path, \"{} ids.pkl\".format(state.split()[-1])), \"rb\"))\n",
    "    for _state in search_list:\n",
    "        _path_2 = os.path.join(\"Cleaned Data\", _state)\n",
    "        _state_ids = pickle.load(open(os.path.join(_path_2, \"{} ids.pkl\".format(_state.split()[-1])), \"rb\"))\n",
    "        mask = np.isin(_state_ids, state_ids, invert = True)\n",
    "        _state = _state_ids[mask]\n",
    "        pickle.dump(_state_ids, open(os.path.join(_path_2, \"{} ids.pkl\".format(_state.split()[-1])), \"wb\"))\n",
    "    pickle.dump(state_ids, open(os.path.join(_path, \"{} ids.pkl\".format(state.split()[-1])), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8ea1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets were scraped from EU timezone, so fixing the date and time mismatch such that each tweet carries the\n",
    "#correct time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd1a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we convert the timezone from EU to Asia/Kolkata such that each tweet shows its India time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95faa945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class timezone_converter:\n",
    "    \n",
    "    def __init__(self, _folder):\n",
    "        self.folder = _folder\n",
    "        self.file_list = [_file for _file in os.listdir(self.folder) if \".csv\" in _file]\n",
    "        self.convert = self.convert_timezone()\n",
    "        \n",
    "    def convert_timezone(self):\n",
    "        for _file in self.file_list:\n",
    "            _filepath = os.path.join(self.folder, _file)\n",
    "            with open(_filepath, \"rb\") as f:\n",
    "                _df = pickle.load(f)\n",
    "            _df[\"date_time\"] = pd.to_datetime(_df[\"date\"] + \" \" + _df[\"time\"], utc = True)\n",
    "            _df[\"date_time\"] = _df[\"date_time\"].dt.tz_convert(\"Asia/Kolkata\").dt.tz_localize(None)\n",
    "            with open(_filepath, \"wb\") as f:\n",
    "                pickle.dump(_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5a9e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andhra_Pradesh finished\n",
      "time taken: 0.05 mins\n",
      "time taken: 0.05 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "state_list = os.listdir(\"Cleaned Data\")\n",
    "for _state in state_list:\n",
    "    t_1 = time()\n",
    "    _path = os.path.join(\"Cleaned Data\", _state)\n",
    "    timezone_converter(_path)\n",
    "    print(\"{} finished\".format(_state.split()[-1]))\n",
    "    print(\"time taken: {} mins\".format(round((time() - t_1)/60, 2)))\n",
    "print(\"time taken: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c64fb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open(\"Cleaned Data\\\\cleaned Andhra_Pradesh\\\\cleaned 2020-01-22.csv.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963141be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1195, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656a0d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>retweet</th>\n",
       "      <th>caps_share</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>emojis</th>\n",
       "      <th>polarity_new</th>\n",
       "      <th>muslim</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.220133e+18</td>\n",
       "      <td>1.220133e+18</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>00:57:35</td>\n",
       "      <td>6.341111e+07</td>\n",
       "      <td>Follow @tokslabossmua on YouTube: thanks so mu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>follow on youtube thanks so much dossier perfu...</td>\n",
       "      <td>tokslabossmua</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-23 06:27:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.220077e+18</td>\n",
       "      <td>1.220077e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>21:12:47</td>\n",
       "      <td>3.309983e+09</td>\n",
       "      <td>Enjoy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>enjoy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-23 02:42:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.220059e+18</td>\n",
       "      <td>1.220059e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>19:59:54</td>\n",
       "      <td>1.032304e+18</td>\n",
       "      <td>#JustAskSachin Sir what is u r stands on CAA  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.212329</td>\n",
       "      <td>sir what is You stands on caa while ago You ha...</td>\n",
       "      <td></td>\n",
       "      <td>justasksachin</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-23 01:29:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.220054e+18</td>\n",
       "      <td>1.220054e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>19:43:49</td>\n",
       "      <td>8.816434e+07</td>\n",
       "      <td>THINKING OF A SEA CHANGE ? HERE IS YOUR OPPORT...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>thinking of sea change here is your opportunity</td>\n",
       "      <td></td>\n",
       "      <td>residentialplot land forsale beachroad visakha...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-23 01:13:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.220051e+18</td>\n",
       "      <td>1.220011e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>19:30:00</td>\n",
       "      <td>1.160402e+18</td>\n",
       "      <td>@turagasudhakar @BeSriSri @prasana_kumar @saib...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>ok how deciding there in andhra three capital ...</td>\n",
       "      <td>turagasudhakar besrisri prasana_kumar saibolli...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-23 01:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  conversation_id        date      time       user_id  \\\n",
       "0  1.220133e+18     1.220133e+18  2020-01-23  00:57:35  6.341111e+07   \n",
       "1  1.220077e+18     1.220077e+18  2020-01-22  21:12:47  3.309983e+09   \n",
       "2  1.220059e+18     1.220059e+18  2020-01-22  19:59:54  1.032304e+18   \n",
       "3  1.220054e+18     1.220054e+18  2020-01-22  19:43:49  8.816434e+07   \n",
       "4  1.220051e+18     1.220011e+18  2020-01-22  19:30:00  1.160402e+18   \n",
       "\n",
       "                                               tweet  retweets_count  \\\n",
       "0  Follow @tokslabossmua on YouTube: thanks so mu...             0.0   \n",
       "1                                              Enjoy             0.0   \n",
       "2  #JustAskSachin Sir what is u r stands on CAA  ...             0.0   \n",
       "3  THINKING OF A SEA CHANGE ? HERE IS YOUR OPPORT...             0.0   \n",
       "4  @turagasudhakar @BeSriSri @prasana_kumar @saib...             0.0   \n",
       "\n",
       "   replies_count  likes_count retweet  caps_share  \\\n",
       "0            0.0          0.0   False    0.086957   \n",
       "1            0.0          0.0   False    0.166667   \n",
       "2            0.0          0.0   False    0.212329   \n",
       "3            0.0          0.0   False    0.385714   \n",
       "4            0.0          0.0   False    0.054795   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  follow on youtube thanks so much dossier perfu...   \n",
       "1                                              enjoy   \n",
       "2  sir what is You stands on caa while ago You ha...   \n",
       "3    thinking of sea change here is your opportunity   \n",
       "4  ok how deciding there in andhra three capital ...   \n",
       "\n",
       "                                              target  \\\n",
       "0                                      tokslabossmua   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  turagasudhakar besrisri prasana_kumar saibolli...   \n",
       "\n",
       "                                             hashtag emojis  polarity_new  \\\n",
       "0                                                                     1.0   \n",
       "1                                                                     1.0   \n",
       "2                                      justasksachin                  1.0   \n",
       "3  residentialplot land forsale beachroad visakha...                  1.0   \n",
       "4                                                                     1.0   \n",
       "\n",
       "   muslim           date_time  \n",
       "0       0 2020-01-23 06:27:35  \n",
       "1       0 2020-01-23 02:42:47  \n",
       "2       0 2020-01-23 01:29:54  \n",
       "3       0 2020-01-23 01:13:49  \n",
       "4       0 2020-01-23 01:00:00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d31b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#going through each state in \"Cleaned Data\" folder. loading each file and the next day's file. \n",
    "#keep 1st day's tweets in 1st day's file and transfer the next day's tweets to the 2nd file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60c07004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class arrange_dates:\n",
    "    \n",
    "    def __init__(self, _folder):\n",
    "        self.folder = _folder\n",
    "        self.file_list = [_file for _file in os.listdir(self.folder) if \".csv\" in _file]\n",
    "        self.fix_dates = self.transfer_data()\n",
    "        \n",
    "    def transfer_data(self):\n",
    "        for _file in self.file_list:\n",
    "            try:\n",
    "                _date = _file.rpartition(\".csv\")[0].rpartition(\" \")[2]\n",
    "                with open(os.path.join(os.path.abspath(self.folder), _file), \"rb\") as f:\n",
    "                    same_day_df = pickle.load(f)\n",
    "                same_day_df.drop_duplicates(subset = [\"id\"], inplace = True)\n",
    "                same_day_df.reset_index(drop = True, inplace = True)\n",
    "                day_end_timestamp = pd.to_datetime(_date, format = \"%Y-%m-%d\") + pd.Timedelta(\"1 days\")\n",
    "                next_day = str(day_end_timestamp).rpartition(\" \")[0]\n",
    "                with open(os.path.join(os.path.abspath(self.folder), \"cleaned {}.csv.pkl\".format(next_day)), \"rb\") as f:\n",
    "                    next_day_df = pickle.load(f)\n",
    "                next_day_df.drop_duplicates(subset = [\"id\"], inplace = True)\n",
    "                next_day_df.reset_index(drop = True, inplace = True)\n",
    "                next_day_df = pd.concat([next_day_df, same_day_df[same_day_df[\"date_time\"] >= day_end_timestamp]], axis = 0, ignore_index = True)\n",
    "                next_day_df.drop_duplicates(subset = [\"id\"], inplace = True)\n",
    "                next_day_df.reset_index(drop = True, inplace = True)\n",
    "                same_day_df = same_day_df[same_day_df[\"date_time\"] <= day_end_timestamp]\n",
    "                same_day_df.drop_duplicates(subset = [\"id\"], inplace = True)\n",
    "                same_day_df.reset_index(drop = True, inplace = True)\n",
    "                with open(os.path.join(os.path.abspath(self.folder), _file), \"wb\") as f:\n",
    "                    pickle.dump(same_day_df, f)\n",
    "                with open(os.path.join(os.path.abspath(self.folder), \"cleaned {}.csv.pkl\".format(next_day)), \"wb\") as f:\n",
    "                    pickle.dump(next_day_df, f)\n",
    "            except:\n",
    "                pass\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "054e41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating through all the states in the \"Cleaned Data\" folder and applying the defined class to each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e66bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andhra_Pradesh complete\n",
      "time taken: 0.09 mins\n",
      "time taken: 0.09 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "state_list = os.listdir(\"Cleaned Data\")\n",
    "for state in state_list:\n",
    "    t_1 = time()\n",
    "    _path = os.path.join(\"Cleaned Data\", state)\n",
    "    arrange_dates(_path)\n",
    "    print(\"{} complete\".format(state.split()[-1]))\n",
    "    print(\"time taken: {} mins\".format(round((time() - t_1)/60, 2)))\n",
    "print(\"time taken: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77d24315",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open(\"Cleaned Data\\\\cleaned Andhra_Pradesh\\\\cleaned 2020-01-22.csv.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bb6b576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6457814c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>retweet</th>\n",
       "      <th>caps_share</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>emojis</th>\n",
       "      <th>polarity_new</th>\n",
       "      <th>muslim</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.220025e+18</td>\n",
       "      <td>1.220025e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>17:47:44</td>\n",
       "      <td>4.854638e+08</td>\n",
       "      <td>Even after watching 5th time, felt emotional i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>even after watching time felt emotional in pre...</td>\n",
       "      <td>anilravipudi anilsunkara1 urstrulymahesh</td>\n",
       "      <td>sarileruneekevvaru blockbustersarileruneekevva...</td>\n",
       "      <td>folded_hands thumbs_up</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-22 23:17:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.220021e+18</td>\n",
       "      <td>1.219905e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>17:32:09</td>\n",
       "      <td>8.768553e+17</td>\n",
       "      <td>@DishPatani just amazing  https://t.co/dq2t76yhRr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>just amazing</td>\n",
       "      <td>dishpatani</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-22 23:02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.220021e+18</td>\n",
       "      <td>1.220021e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>17:31:06</td>\n",
       "      <td>1.059703e+18</td>\n",
       "      <td>@sexeducation why this girl have little screen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.068376</td>\n",
       "      <td>why this girl have little screen time in both ...</td>\n",
       "      <td>sexeducation</td>\n",
       "      <td>sexeducations2</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-22 23:01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.220021e+18</td>\n",
       "      <td>1.220021e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>17:29:31</td>\n",
       "      <td>1.014312e+08</td>\n",
       "      <td>@airindiain @HardeepSPuri Air India loses my l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>air india loses my luggage and is clueless abo...</td>\n",
       "      <td>airindiain hardeepspuri</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-22 22:59:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.220020e+18</td>\n",
       "      <td>1.220020e+18</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>17:26:30</td>\n",
       "      <td>1.538231e+08</td>\n",
       "      <td>Lit the fire 🔥 to drive away the chill. @ Hari...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>lit the fire to drive away the chill haritha h...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>fire</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-22 22:56:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  conversation_id        date      time       user_id  \\\n",
       "0  1.220025e+18     1.220025e+18  2020-01-22  17:47:44  4.854638e+08   \n",
       "1  1.220021e+18     1.219905e+18  2020-01-22  17:32:09  8.768553e+17   \n",
       "2  1.220021e+18     1.220021e+18  2020-01-22  17:31:06  1.059703e+18   \n",
       "3  1.220021e+18     1.220021e+18  2020-01-22  17:29:31  1.014312e+08   \n",
       "4  1.220020e+18     1.220020e+18  2020-01-22  17:26:30  1.538231e+08   \n",
       "\n",
       "                                               tweet  retweets_count  \\\n",
       "0  Even after watching 5th time, felt emotional i...             0.0   \n",
       "1  @DishPatani just amazing  https://t.co/dq2t76yhRr             0.0   \n",
       "2  @sexeducation why this girl have little screen...             0.0   \n",
       "3  @airindiain @HardeepSPuri Air India loses my l...             0.0   \n",
       "4  Lit the fire 🔥 to drive away the chill. @ Hari...             0.0   \n",
       "\n",
       "   replies_count  likes_count retweet  caps_share  \\\n",
       "0            0.0          2.0   False    0.116279   \n",
       "1            0.0          0.0   False    0.065217   \n",
       "2            0.0          0.0   False    0.068376   \n",
       "3            1.0          0.0   False    0.081301   \n",
       "4            0.0          0.0   False    0.095238   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  even after watching time felt emotional in pre...   \n",
       "1                                       just amazing   \n",
       "2  why this girl have little screen time in both ...   \n",
       "3  air india loses my luggage and is clueless abo...   \n",
       "4  lit the fire to drive away the chill haritha h...   \n",
       "\n",
       "                                     target  \\\n",
       "0  anilravipudi anilsunkara1 urstrulymahesh   \n",
       "1                                dishpatani   \n",
       "2                              sexeducation   \n",
       "3                   airindiain hardeepspuri   \n",
       "4                                             \n",
       "\n",
       "                                             hashtag                  emojis  \\\n",
       "0  sarileruneekevvaru blockbustersarileruneekevva...  folded_hands thumbs_up   \n",
       "1                                                                              \n",
       "2                                     sexeducations2                           \n",
       "3                                                                              \n",
       "4                                                                       fire   \n",
       "\n",
       "   polarity_new  muslim           date_time  \n",
       "0           1.0       0 2020-01-22 23:17:44  \n",
       "1           1.0       0 2020-01-22 23:02:09  \n",
       "2           1.0       0 2020-01-22 23:01:06  \n",
       "3           1.0       0 2020-01-22 22:59:31  \n",
       "4           2.0       0 2020-01-22 22:56:30  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d79500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
