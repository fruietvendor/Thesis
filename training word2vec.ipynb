{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebfe15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423d2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining an iterator that reads the txt file line by line, converts all the letters to lowercase and \n",
    "#splits them on whitespace, creating a list of tokens for each tweet. The final corpus is thus a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3486b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.file, encoding = \"utf-8\") as f:\n",
    "            for row in f:\n",
    "                _row = row.lower().split()\n",
    "                yield _row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0f8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the corpus as an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c9b9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus(\"word2vec_tweets.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b923f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the word2vec model with context_window = 1, vector_size = 300, CBOW architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e8633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:22:36: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2023-04-01T18:22:36.742560', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(\n",
    "    min_count = 20,\n",
    "    window = 1,\n",
    "    vector_size = 300,\n",
    "    sample = 6e-05,\n",
    "    alpha = 0.03,\n",
    "    min_alpha = 0.0007,\n",
    "    negative = 20,\n",
    "    workers = cores - 2,\n",
    "    ns_exponent = 0.75,\n",
    "    sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1f4b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the vocabulary for the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37ed9316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:22:38: collecting all words and their counts\n",
      "INFO - 18:22:38: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:22:38: PROGRESS: at sentence #10000, processed 133543 words, keeping 16127 word types\n",
      "INFO - 18:22:38: PROGRESS: at sentence #20000, processed 268382 words, keeping 24156 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #30000, processed 406570 words, keeping 30617 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #40000, processed 546710 words, keeping 36117 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #50000, processed 689132 words, keeping 41262 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #60000, processed 837726 words, keeping 45253 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #70000, processed 984773 words, keeping 49023 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #80000, processed 1123420 words, keeping 52680 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #90000, processed 1269111 words, keeping 56296 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #100000, processed 1415993 words, keeping 59872 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #110000, processed 1558010 words, keeping 63079 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #120000, processed 1691482 words, keeping 66159 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #130000, processed 1845820 words, keeping 69175 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #140000, processed 1991170 words, keeping 72032 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #150000, processed 2133556 words, keeping 75108 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #160000, processed 2266399 words, keeping 77791 word types\n",
      "INFO - 18:22:39: PROGRESS: at sentence #170000, processed 2415021 words, keeping 80649 word types\n",
      "INFO - 18:22:39: collected 82037 word types from a corpus of 2491052 raw words and 175192 sentences\n",
      "INFO - 18:22:39: Creating a fresh vocabulary\n",
      "INFO - 18:22:39: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 7853 unique words (9.57% of original 82037, drops 74184)', 'datetime': '2023-04-01T18:22:39.921473', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:39: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 2294154 word corpus (92.10% of original 2491052, drops 196898)', 'datetime': '2023-04-01T18:22:39.921473', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:40: deleting the raw counts dictionary of 82037 items\n",
      "INFO - 18:22:40: sample=6e-05 downsamples 863 most-common words\n",
      "INFO - 18:22:40: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1030079.5607192045 word corpus (44.9%% of prior 2294154)', 'datetime': '2023-04-01T18:22:40.030249', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:40: estimated required memory for 7853 words and 300 dimensions: 22773700 bytes\n",
      "INFO - 18:22:40: resetting layer weights\n",
      "INFO - 18:22:40: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T18:22:40.184011', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to build vocab: 0.02 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2v_model.build_vocab(sentences, progress_per = 10000)\n",
    "\n",
    "print(\"time to build vocab: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b09cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0ef352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:22:44: Word2Vec lifecycle event {'msg': 'training model with 6 workers on 7853 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=1 shrink_windows=True', 'datetime': '2023-04-01T18:22:44.912229', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "INFO - 18:22:45: EPOCH 0 - PROGRESS: at 38.58% examples, 392642 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:46: EPOCH 0 - PROGRESS: at 85.02% examples, 435351 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 18:22:47: EPOCH 0: training on 2491052 raw words (1030410 effective words) took 2.3s, 440712 effective words/s\n",
      "INFO - 18:22:48: EPOCH 1 - PROGRESS: at 45.44% examples, 459411 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:49: EPOCH 1 - PROGRESS: at 91.38% examples, 464689 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:49: EPOCH 1: training on 2491052 raw words (1029238 effective words) took 2.2s, 465249 effective words/s\n",
      "INFO - 18:22:50: EPOCH 2 - PROGRESS: at 46.73% examples, 472611 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:51: EPOCH 2 - PROGRESS: at 93.35% examples, 475762 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:51: EPOCH 2: training on 2491052 raw words (1029067 effective words) took 2.2s, 474386 effective words/s\n",
      "INFO - 18:22:52: EPOCH 3 - PROGRESS: at 44.73% examples, 452623 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:53: EPOCH 3 - PROGRESS: at 87.28% examples, 449409 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:53: EPOCH 3: training on 2491052 raw words (1030275 effective words) took 2.3s, 445511 effective words/s\n",
      "INFO - 18:22:54: EPOCH 4 - PROGRESS: at 45.94% examples, 462370 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:55: EPOCH 4 - PROGRESS: at 92.93% examples, 472763 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:56: EPOCH 4: training on 2491052 raw words (1029983 effective words) took 2.2s, 472134 effective words/s\n",
      "INFO - 18:22:57: EPOCH 5 - PROGRESS: at 45.44% examples, 461127 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:58: EPOCH 5 - PROGRESS: at 92.93% examples, 474143 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:58: EPOCH 5: training on 2491052 raw words (1030328 effective words) took 2.2s, 472633 effective words/s\n",
      "INFO - 18:22:59: EPOCH 6 - PROGRESS: at 45.94% examples, 463215 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:00: EPOCH 6 - PROGRESS: at 92.55% examples, 471163 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:00: EPOCH 6: training on 2491052 raw words (1030434 effective words) took 2.2s, 472830 effective words/s\n",
      "INFO - 18:23:01: EPOCH 7 - PROGRESS: at 45.44% examples, 458722 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:02: EPOCH 7 - PROGRESS: at 86.88% examples, 444620 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:02: EPOCH 7: training on 2491052 raw words (1029539 effective words) took 2.3s, 447411 effective words/s\n",
      "INFO - 18:23:03: EPOCH 8 - PROGRESS: at 46.35% examples, 468686 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:04: EPOCH 8 - PROGRESS: at 91.82% examples, 467657 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:05: EPOCH 8: training on 2491052 raw words (1029451 effective words) took 2.2s, 467831 effective words/s\n",
      "INFO - 18:23:06: EPOCH 9 - PROGRESS: at 44.73% examples, 453101 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:07: EPOCH 9 - PROGRESS: at 92.23% examples, 469364 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:07: EPOCH 9: training on 2491052 raw words (1030500 effective words) took 2.2s, 471127 effective words/s\n",
      "INFO - 18:23:08: EPOCH 10 - PROGRESS: at 47.47% examples, 477855 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:09: EPOCH 10 - PROGRESS: at 91.82% examples, 467156 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:09: EPOCH 10: training on 2491052 raw words (1030075 effective words) took 2.2s, 467839 effective words/s\n",
      "INFO - 18:23:10: EPOCH 11 - PROGRESS: at 46.73% examples, 470334 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:23:11: EPOCH 11 - PROGRESS: at 94.11% examples, 474798 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 18:23:11: EPOCH 11: training on 2491052 raw words (1030832 effective words) took 2.1s, 480721 effective words/s\n",
      "INFO - 18:23:12: EPOCH 12 - PROGRESS: at 48.65% examples, 491720 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:13: EPOCH 12 - PROGRESS: at 93.35% examples, 471756 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:13: EPOCH 12: training on 2491052 raw words (1029413 effective words) took 2.2s, 475082 effective words/s\n",
      "INFO - 18:23:14: EPOCH 13 - PROGRESS: at 44.73% examples, 453787 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:15: EPOCH 13 - PROGRESS: at 87.28% examples, 448158 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:16: EPOCH 13: training on 2491052 raw words (1029937 effective words) took 2.3s, 447061 effective words/s\n",
      "INFO - 18:23:17: EPOCH 14 - PROGRESS: at 40.48% examples, 414225 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 18:23:18: EPOCH 14 - PROGRESS: at 84.26% examples, 434112 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:18: EPOCH 14: training on 2491052 raw words (1030782 effective words) took 2.4s, 437314 effective words/s\n",
      "INFO - 18:23:19: EPOCH 15 - PROGRESS: at 42.09% examples, 425828 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 18:23:20: EPOCH 15 - PROGRESS: at 83.87% examples, 427666 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 18:23:20: EPOCH 15: training on 2491052 raw words (1029482 effective words) took 2.4s, 434822 effective words/s\n",
      "INFO - 18:23:21: EPOCH 16 - PROGRESS: at 43.35% examples, 441104 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:22: EPOCH 16 - PROGRESS: at 87.28% examples, 447798 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:23: EPOCH 16: training on 2491052 raw words (1029502 effective words) took 2.3s, 449036 effective words/s\n",
      "INFO - 18:23:24: EPOCH 17 - PROGRESS: at 41.28% examples, 419447 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:25: EPOCH 17 - PROGRESS: at 83.50% examples, 427616 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:25: EPOCH 17: training on 2491052 raw words (1029823 effective words) took 2.4s, 428713 effective words/s\n",
      "INFO - 18:23:26: EPOCH 18 - PROGRESS: at 43.78% examples, 446191 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:27: EPOCH 18 - PROGRESS: at 87.28% examples, 448347 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:27: EPOCH 18: training on 2491052 raw words (1029650 effective words) took 2.3s, 448936 effective words/s\n",
      "INFO - 18:23:28: EPOCH 19 - PROGRESS: at 40.48% examples, 414306 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:29: EPOCH 19 - PROGRESS: at 80.63% examples, 414838 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:23:30: EPOCH 19: training on 2491052 raw words (1030504 effective words) took 2.4s, 422122 effective words/s\n",
      "INFO - 18:23:31: EPOCH 20 - PROGRESS: at 43.78% examples, 443048 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:32: EPOCH 20 - PROGRESS: at 87.69% examples, 449567 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:32: EPOCH 20: training on 2491052 raw words (1030254 effective words) took 2.3s, 450791 effective words/s\n",
      "INFO - 18:23:33: EPOCH 21 - PROGRESS: at 41.28% examples, 420504 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:34: EPOCH 21 - PROGRESS: at 84.67% examples, 434343 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:34: EPOCH 21: training on 2491052 raw words (1030409 effective words) took 2.4s, 434272 effective words/s\n",
      "INFO - 18:23:35: EPOCH 22 - PROGRESS: at 41.68% examples, 426126 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:36: EPOCH 22 - PROGRESS: at 85.02% examples, 437329 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:37: EPOCH 22: training on 2491052 raw words (1029567 effective words) took 2.4s, 430252 effective words/s\n",
      "INFO - 18:23:38: EPOCH 23 - PROGRESS: at 44.21% examples, 449708 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:23:39: EPOCH 23 - PROGRESS: at 87.28% examples, 445091 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 18:23:39: EPOCH 23: training on 2491052 raw words (1030398 effective words) took 2.3s, 448626 effective words/s\n",
      "INFO - 18:23:40: EPOCH 24 - PROGRESS: at 41.68% examples, 426221 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 18:23:41: EPOCH 24 - PROGRESS: at 86.53% examples, 444004 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:41: EPOCH 24: training on 2491052 raw words (1030115 effective words) took 2.3s, 446308 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:23:42: EPOCH 25 - PROGRESS: at 44.21% examples, 449493 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:43: EPOCH 25 - PROGRESS: at 86.14% examples, 441531 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 18:23:44: EPOCH 25: training on 2491052 raw words (1030573 effective words) took 2.4s, 436371 effective words/s\n",
      "INFO - 18:23:45: EPOCH 26 - PROGRESS: at 43.78% examples, 440231 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:46: EPOCH 26 - PROGRESS: at 88.14% examples, 449477 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:46: EPOCH 26: training on 2491052 raw words (1029604 effective words) took 2.3s, 451690 effective words/s\n",
      "INFO - 18:23:47: EPOCH 27 - PROGRESS: at 43.78% examples, 441550 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:48: EPOCH 27 - PROGRESS: at 88.88% examples, 454447 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:48: EPOCH 27: training on 2491052 raw words (1029831 effective words) took 2.3s, 447537 effective words/s\n",
      "INFO - 18:23:49: EPOCH 28 - PROGRESS: at 42.55% examples, 434239 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:50: EPOCH 28 - PROGRESS: at 87.28% examples, 450304 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:51: EPOCH 28: training on 2491052 raw words (1030811 effective words) took 2.3s, 450833 effective words/s\n",
      "INFO - 18:23:52: EPOCH 29 - PROGRESS: at 44.73% examples, 453054 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:53: EPOCH 29 - PROGRESS: at 88.03% examples, 450485 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 18:23:53: EPOCH 29: training on 2491052 raw words (1029956 effective words) took 2.3s, 450759 effective words/s\n",
      "INFO - 18:23:53: Word2Vec lifecycle event {'msg': 'training on 74731560 raw words (30900743 effective words) took 68.5s, 451047 effective words/s', 'datetime': '2023-04-01T18:23:53.418863', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to train model: 1.14 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 30, report_delay = 1)\n",
    "\n",
    "print(\"time taken to train model: {} mins\".format(round((time() - t)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1354ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the word2vec model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d1faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(w2v_model, open(\"wv_model.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
